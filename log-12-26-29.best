Sample size:  256
State size:  24
Action size:  9
bleu_seq
You select the reward based on the sequence accuaracy bleu_seq
EPOCH %d 1
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.9 0.9

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

0 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

2 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
3 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 4 5 3]]
Reward:  [1.  0.8 0.6] 

4 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
5 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
6 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 3]
Eval  :  [[2 4 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 

EPOCH %d 2
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.81 0.81
2493 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
2494 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
2495 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
2496 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
2497 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
2498 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
2499 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
EPOCH %d 3
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.7290000000000001 0.7290000000000001
4986 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4987 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4988 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4989 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4990 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4991 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4992 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
As referece this first test on dev data. Is maded with the Q networks, initialized randomly : 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.9378,  0.6153, -0.6642])
So far:  [array([2]), array([4]), array([4])]  the state[:3] is:  tensor([-0.9584,  0.7692, -0.9055])
So far:  [array([2]), array([4]), array([4]), array([4])]  the state[:3] is:  tensor([-0.9540,  0.8120, -0.9279])
So far:  [array([2]), array([4]), array([4]), array([4]), array([4])]  the state[:3] is:  tensor([-0.9512,  0.8205, -0.9322])
So far:  [array([2]), array([4]), array([4]), array([4]), array([4]), array([4])]  the state[:3] is:  tensor([-0.9491,  0.8240, -0.9336])
So far:  [array([2]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4])]  the state[:3] is:  tensor([-0.9477,  0.8256, -0.9341])
So far:  [array([2]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4])]  the state[:3] is:  tensor([-0.9469,  0.8265, -0.9344])
So far:  [array([2]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4])]  the state[:3] is:  tensor([-0.9464,  0.8269, -0.9346])
So far:  [array([2]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4])]  the state[:3] is:  tensor([-0.9461,  0.8272, -0.9346])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  3 10 10 10 10 10 10 10 10]
Eval  :  [[2 4 4 4 4 4 4 4 4 4 4]]
Reward:  [-1.  -0.2 -0.4 -0.6 -0.8 -1.  -1.2 -1.4 -1.6 -1.8] 

roptimal:  tensor([[2, 7]]) [[2 7 3]]
[1.  0.8]
So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.9850,  0.9089, -0.9363])
So far:  [array([2]), array([4]), array([4])]  the state[:3] is:  tensor([-0.9922,  0.9398, -0.9843])
So far:  [array([2]), array([4]), array([4]), array([4])]  the state[:3] is:  tensor([-0.9903,  0.9516, -0.9886])
So far:  [array([2]), array([4]), array([4]), array([4]), array([4])]  the state[:3] is:  tensor([-0.9894,  0.9543, -0.9892])
So far:  [array([2]), array([4]), array([4]), array([4]), array([4]), array([4])]  the state[:3] is:  tensor([-0.9887,  0.9556, -0.9894])
So far:  [array([2]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4])]  the state[:3] is:  tensor([-0.9882,  0.9562, -0.9895])
So far:  [array([2]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4])]  the state[:3] is:  tensor([-0.9880,  0.9565, -0.9895])
So far:  [array([2]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4])]  the state[:3] is:  tensor([-0.9879,  0.9567, -0.9895])
So far:  [array([2]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4])]  the state[:3] is:  tensor([-0.9878,  0.9567, -0.9895])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  8  3 10 10 10 10 10 10 10 10]
Eval  :  [[2 4 4 4 4 4 4 4 4 4 4]]
Reward:  [-1.  -0.2 -0.4 -0.6 -0.8 -1.  -1.2 -1.4 -1.6 -1.8] 

roptimal:  tensor([[2, 8]]) [[2 8 3]]
[1.  0.8]
So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])
So far:  [array([2]), array([4]), array([4])]  the state[:3] is:  tensor([-0.9673,  0.9915, -0.7579])
So far:  [array([2]), array([4]), array([4]), array([4])]  the state[:3] is:  tensor([-0.9833,  0.9930, -0.9192])
So far:  [array([2]), array([4]), array([4]), array([4]), array([4])]  the state[:3] is:  tensor([-0.9852,  0.9933, -0.9380])
So far:  [array([2]), array([4]), array([4]), array([4]), array([4]), array([4])]  the state[:3] is:  tensor([-0.9853,  0.9933, -0.9427])
So far:  [array([2]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4])]  the state[:3] is:  tensor([-0.9850,  0.9934, -0.9443])
So far:  [array([2]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4])]  the state[:3] is:  tensor([-0.9848,  0.9934, -0.9449])
So far:  [array([2]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4])]  the state[:3] is:  tensor([-0.9846,  0.9934, -0.9452])
So far:  [array([2]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4])]  the state[:3] is:  tensor([-0.9845,  0.9934, -0.9453])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  4  5  3 10 10 10 10 10 10 10]
Eval  :  [[2 4 4 4 4 4 4 4 4 4 4]]
Reward:  [ 1.  -2.2 -0.4 -0.6 -0.8 -1.  -1.2 -1.4 -1.6 -1.8] 

roptimal:  tensor([[2, 4, 5]]) [[2 4 5 3]]
[1.  0.8 0.6]

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10 10 10 10 10 10 10]
Eval  :  [[2 4 4 4 4 4 4 4 4 4 4]]
Reward:  [-1.  -0.2 -0.4 -0.6 -0.8 -1.  -1.2 -1.4 -1.6 -1.8] 

roptimal:  tensor([[2, 7, 6]]) [[2 7 6 3]]
[1.  0.8 0.6]

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  8  3 10 10 10 10 10 10 10 10]
Eval  :  [[2 4 4 4 4 4 4 4 4 4 4]]
Reward:  [-1.  -0.2 -0.4 -0.6 -0.8 -1.  -1.2 -1.4 -1.6 -1.8] 

roptimal:  tensor([[2, 8]]) [[2 8 3]]
[1.  0.8]

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  5  3 10 10 10 10 10 10 10 10]
Eval  :  [[2 4 4 4 4 4 4 4 4 4 4]]
Reward:  [-1.  -0.2 -0.4 -0.6 -0.8 -1.  -1.2 -1.4 -1.6 -1.8] 

roptimal:  tensor([[2, 5]]) [[2 5 3]]
[1.  0.8]

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  6  3 10 10 10 10 10 10 10 10]
Eval  :  [[2 4 4 4 4 4 4 4 4 4 4]]
Reward:  [-1.  -0.2 -0.4 -0.6 -0.8 -1.  -1.2 -1.4 -1.6 -1.8] 

roptimal:  tensor([[2, 6]]) [[2 6 3]]
[1.  0.8]

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  4  6  3 10 10 10 10 10 10 10]
Eval  :  [[2 4 4 4 4 4 4 4 4 4 4]]
Reward:  [ 1.  -2.2 -0.4 -0.6 -0.8 -1.  -1.2 -1.4 -1.6 -1.8] 

roptimal:  tensor([[2, 4, 6]]) [[2 4 6 3]]
[1.  0.8 0.6]

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  5  3 10 10 10 10 10 10 10]
Eval  :  [[2 4 4 4 4 4 4 4 4 4 4]]
Reward:  [-1.  -0.2 -0.4 -0.6 -0.8 -1.  -1.2 -1.4 -1.6 -1.8] 

roptimal:  tensor([[2, 7, 5]]) [[2 7 5 3]]
[1.  0.8 0.6]

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  3 10 10 10 10 10 10 10 10]
Eval  :  [[2 4 4 4 4 4 4 4 4 4 4]]
Reward:  [-1.  -0.2 -0.4 -0.6 -0.8 -1.  -1.2 -1.4 -1.6 -1.8] 

roptimal:  tensor([[2, 7]]) [[2 7 3]]
[1.  0.8]
roptimal:  tensor([[2, 6, 7]]) [[2 6 7 3]]
[1.  0.8 0.6]
roptimal:  tensor([[2, 7]]) [[2 7 3]]
[1.  0.8]
roptimal:  tensor([[2, 7, 6]]) [[2 7 6 3]]
[1.  0.8 0.6]
roptimal:  tensor([[2, 6]]) [[2 6 3]]
[1.  0.8]
roptimal:  tensor([[2, 4]]) [[2 4 3]]
[1.  0.8]
roptimal:  tensor([[2, 8, 8]]) [[2 8 8 3]]
[1.  0.8 0.6]
roptimal:  tensor([[2, 8]]) [[2 8 3]]
[1.  0.8]
roptimal:  tensor([[2, 5, 4]]) [[2 5 4 3]]
[1.  0.8 0.6]
roptimal:  tensor([[2, 4, 8]]) [[2 4 8 3]]
[1.  0.8 0.6]
roptimal:  tensor([[2, 5]]) [[2 5 3]]
[1.  0.8]
roptimal:  tensor([[2, 8]]) [[2 8 3]]
[1.  0.8]
roptimal:  tensor([[2, 8]]) [[2 8 3]]
[1.  0.8]
roptimal:  tensor([[2, 5]]) [[2 5 3]]
[1.  0.8]
roptimal:  tensor([[2, 7, 8]]) [[2 7 8 3]]
[1.  0.8 0.6]
roptimal:  tensor([[2, 7]]) [[2 7 3]]
[1.  0.8]
roptimal:  tensor([[2, 6, 5]]) [[2 6 5 3]]
[1.  0.8 0.6]
roptimal:  tensor([[2, 7]]) [[2 7 3]]
[1.  0.8]
roptimal:  tensor([[2, 7, 6]]) [[2 7 6 3]]
[1.  0.8 0.6]
roptimal:  tensor([[2, 4, 7]]) [[2 4 7 3]]
[1.  0.8 0.6]
roptimal:  tensor([[2, 6]]) [[2 6 3]]
[1.  0.8]
roptimal:  tensor([[2, 8, 6]]) [[2 8 6 3]]
[1.  0.8 0.6]
roptimal:  tensor([[2, 8]]) [[2 8 3]]
[1.  0.8]
roptimal:  tensor([[2, 6, 6]]) [[2 6 6 3]]
[1.  0.8 0.6]
roptimal:  tensor([[2, 4, 6]]) [[2 4 6 3]]
[1.  0.8 0.6]
roptimal:  tensor([[2, 6, 8]]) [[2 6 8 3]]
[1.  0.8 0.6]
roptimal:  tensor([[2, 4, 6]]) [[2 4 6 3]]
[1.  0.8 0.6]
roptimal:  tensor([[2, 4]]) [[2 4 3]]
[1.  0.8]
roptimal:  tensor([[2, 5, 6]]) [[2 5 6 3]]
[1.  0.8 0.6]
roptimal:  tensor([[2, 6]]) [[2 6 3]]
[1.  0.8]
roptimal:  tensor([[2, 6, 4]]) [[2 6 4 3]]
[1.  0.8 0.6]
roptimal:  tensor([[2, 6]]) [[2 6 3]]
[1.  0.8]
roptimal:  tensor([[2, 6, 6]]) [[2 6 6 3]]
[1.  0.8 0.6]
roptimal:  tensor([[2, 5, 7]]) [[2 5 7 3]]
[1.  0.8 0.6]
roptimal:  tensor([[2, 5]]) [[2 5 3]]
[1.  0.8]
roptimal:  tensor([[2, 6, 8]]) [[2 6 8 3]]
[1.  0.8 0.6]
roptimal:  tensor([[2, 4, 6]]) [[2 4 6 3]]
[1.  0.8 0.6]
roptimal:  tensor([[2, 4, 7]]) [[2 4 7 3]]
[1.  0.8 0.6]
roptimal:  tensor([[2, 4, 7]]) [[2 4 7 3]]
[1.  0.8 0.6]
roptimal:  tensor([[2, 5, 8]]) [[2 5 8 3]]
[1.  0.8 0.6]
roptimal:  tensor([[2, 7]]) [[2 7 3]]
[1.  0.8]
roptimal:  tensor([[2, 4]]) [[2 4 3]]
[1.  0.8]
roptimal:  tensor([[2, 4]]) [[2 4 3]]
[1.  0.8]
roptimal:  tensor([[2, 5]]) [[2 5 3]]
[1.  0.8]
roptimal:  tensor([[2, 5]]) [[2 5 3]]
[1.  0.8]
roptimal:  tensor([[2, 5, 5]]) [[2 5 5 3]]
[1.  0.8 0.6]
roptimal:  tensor([[2, 4]]) [[2 4 3]]
[1.  0.8]
roptimal:  tensor([[2, 5, 6]]) [[2 5 6 3]]
[1.  0.8 0.6]
roptimal:  tensor([[2, 4, 4]]) [[2 4 4 3]]
[1.  0.8 0.6]
roptimal:  tensor([[2, 6, 7]]) [[2 6 7 3]]
[1.  0.8 0.6]
roptimal:  tensor([[2, 4, 8]]) [[2 4 8 3]]
[1.  0.8 0.6]
roptimal:  tensor([[2, 5, 8]]) [[2 5 8 3]]
[1.  0.8 0.6]
roptimal:  tensor([[2, 5]]) [[2 5 3]]
[1.  0.8]
roptimal:  tensor([[2, 8, 6]]) [[2 8 6 3]]
[1.  0.8 0.6]
roptimal:  tensor([[2, 7]]) [[2 7 3]]
[1.  0.8]
roptimal:  tensor([[2, 6]]) [[2 6 3]]
[1.  0.8]
roptimal:  tensor([[2, 5, 6]]) [[2 5 6 3]]
[1.  0.8 0.6]
roptimal:  tensor([[2, 8]]) [[2 8 3]]
[1.  0.8]
roptimal:  tensor([[2, 6]]) [[2 6 3]]
[1.  0.8]
roptimal:  tensor([[2, 8]]) [[2 8 3]]
[1.  0.8]
roptimal:  tensor([[2, 6, 5]]) [[2 6 5 3]]
[1.  0.8 0.6]
roptimal:  tensor([[2, 4, 6]]) [[2 4 6 3]]
[1.  0.8 0.6]
roptimal:  tensor([[2, 7]]) [[2 7 3]]
[1.  0.8]
roptimal:  tensor([[2, 7]]) [[2 7 3]]
[1.  0.8]
roptimal:  tensor([[2, 8]]) [[2 8 3]]
[1.  0.8]
roptimal:  tensor([[2, 8]]) [[2 8 3]]
[1.  0.8]
roptimal:  tensor([[2, 8, 8]]) [[2 8 8 3]]
[1.  0.8 0.6]
roptimal:  tensor([[2, 6, 6]]) [[2 6 6 3]]
[1.  0.8 0.6]
roptimal:  tensor([[2, 7, 4]]) [[2 7 4 3]]
[1.  0.8 0.6]
roptimal:  tensor([[2, 5]]) [[2 5 3]]
[1.  0.8]
roptimal:  tensor([[2, 4]]) [[2 4 3]]
[1.  0.8]
roptimal:  tensor([[2, 4]]) [[2 4 3]]
[1.  0.8]
roptimal:  tensor([[2, 5]]) [[2 5 3]]
[1.  0.8]
roptimal:  tensor([[2, 5, 7]]) [[2 5 7 3]]
[1.  0.8 0.6]
roptimal:  tensor([[2, 5]]) [[2 5 3]]
[1.  0.8]
roptimal:  tensor([[2, 4, 4]]) [[2 4 4 3]]
[1.  0.8 0.6]
roptimal:  tensor([[2, 7]]) [[2 7 3]]
[1.  0.8]
roptimal:  tensor([[2, 4, 8]]) [[2 4 8 3]]
[1.  0.8 0.6]
roptimal:  tensor([[2, 6, 6]]) [[2 6 6 3]]
[1.  0.8 0.6]
roptimal:  tensor([[2, 4, 7]]) [[2 4 7 3]]
[1.  0.8 0.6]
roptimal:  tensor([[2, 4]]) [[2 4 3]]
[1.  0.8]
roptimal:  tensor([[2, 5]]) [[2 5 3]]
[1.  0.8]
roptimal:  tensor([[2, 5, 4]]) [[2 5 4 3]]
[1.  0.8 0.6]
roptimal:  tensor([[2, 8]]) [[2 8 3]]
[1.  0.8]
roptimal:  tensor([[2, 4, 7]]) [[2 4 7 3]]
[1.  0.8 0.6]
roptimal:  tensor([[2, 4]]) [[2 4 3]]
[1.  0.8]
roptimal:  tensor([[2, 7, 7]]) [[2 7 7 3]]
[1.  0.8 0.6]
roptimal:  tensor([[2, 7]]) [[2 7 3]]
[1.  0.8]
roptimal:  tensor([[2, 5, 8]]) [[2 5 8 3]]
[1.  0.8 0.6]
roptimal:  tensor([[2, 8, 6]]) [[2 8 6 3]]
[1.  0.8 0.6]
roptimal:  tensor([[2, 7, 5]]) [[2 7 5 3]]
[1.  0.8 0.6]
roptimal:  tensor([[2, 4]]) [[2 4 3]]
[1.  0.8]
roptimal:  tensor([[2, 6]]) [[2 6 3]]
[1.  0.8]
roptimal:  tensor([[2, 7, 6]]) [[2 7 6 3]]
[1.  0.8 0.6]
The optimal reward is:  216.60000000000042
valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['1 1 1 1 1 1 1 1 1 1', '1 1 1 1 1 1 1 1 1 1', '1 1 1 1 1 1 1 1 1 1', '1 1 1 1 1 1 1 1 1 1', '1 1 1 1 1 1 1 1 1 1', '1 1 1 1 1 1 1 1 1 1', '1 1 1 1 1 1 1 1 1 1', '1 1 1 1 1 1 1 1 1 1', '1 1 1 1 1 1 1 1 1 1', '1 1 1 1 1 1 1 1 1 1']
1  r_total and score:  38.8 0.0
Current Bleu score is:  0.0
learn step counter:  1
dev_network_count:  1
Using pretraining...
learn step counter:  51
dev_network_count:  1
learn step counter:  101
dev_network_count:  1
learn step counter:  151
dev_network_count:  1
learn step counter:  201
dev_network_count:  1
learn step counter:  251
dev_network_count:  1

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])
So far:  [array([2]), array([4]), array([7])]  the state[:3] is:  tensor([-0.9601,  0.9921, -0.7420])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 4 7 3]]
Reward:  [ 1.  -2.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 5 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 8 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 7 7 3]]
Reward:  [ 1.  -2.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '1 3', '4 4', '2', '0', '4', '2 4', '3 3', '4']
2  r_total and score:  276.0000000000004 49.56385459568427
Current Bleu score is:  49.56385459568427
learn step counter:  301
dev_network_count:  2
learn step counter:  351
dev_network_count:  2
learn step counter:  401
dev_network_count:  2
learn step counter:  451
dev_network_count:  2
learn step counter:  501
dev_network_count:  2
learn step counter:  551
dev_network_count:  2

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([5])]  the state[:3] is:  tensor([-0.5256,  0.9823,  0.5156])
So far:  [array([2]), array([5]), array([4])]  the state[:3] is:  tensor([-0.9656,  0.9911, -0.7185])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 5 4 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 5 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 8 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 5 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '0 1', '4 4', '2', '0', '4', '2 4', '0 3', '4']
3  r_total and score:  270.8000000000004 51.83960757134804
Current Bleu score is:  51.83960757134804
learn step counter:  601
dev_network_count:  3
learn step counter:  651
dev_network_count:  3
learn step counter:  701
dev_network_count:  3
learn step counter:  751
dev_network_count:  3
learn step counter:  801
dev_network_count:  3
learn step counter:  851
dev_network_count:  3

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([5])]  the state[:3] is:  tensor([-0.5256,  0.9823,  0.5156])
So far:  [array([2]), array([5]), array([4])]  the state[:3] is:  tensor([-0.9656,  0.9911, -0.7185])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 5 4 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 5 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 8 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 5 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '0 1', '4 4', '2', '0', '4', '2 4', '0 3', '4']
4  r_total and score:  264.20000000000044 56.511488983186176
Current Bleu score is:  56.511488983186176
learn step counter:  901
dev_network_count:  4
learn step counter:  951
dev_network_count:  4
EPOCH %d 4
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.6561 0.6561
2479 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
2480 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
2481 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
2482 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
Starting using Q target net....
2483 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
2484 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
2485 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  1001
dev_network_count:  4
learn step counter:  1051
dev_network_count:  4
learn step counter:  1101
dev_network_count:  4
learn step counter:  1151
dev_network_count:  4

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([5])]  the state[:3] is:  tensor([-0.5256,  0.9823,  0.5156])
So far:  [array([2]), array([5]), array([4])]  the state[:3] is:  tensor([-0.9656,  0.9911, -0.7185])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 5 4 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 5 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 8 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 5 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '0 1', '4 4', '2', '0', '4', '2 4', '0 3', '4']
5  r_total and score:  260.8000000000004 56.633681297993704
Current Bleu score is:  56.633681297993704
learn step counter:  1201
dev_network_count:  5
learn step counter:  1251
dev_network_count:  5
learn step counter:  1301
dev_network_count:  5
learn step counter:  1351
dev_network_count:  5
learn step counter:  1401
dev_network_count:  5
learn step counter:  1451
dev_network_count:  5

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])
So far:  [array([2]), array([4]), array([7])]  the state[:3] is:  tensor([-0.9601,  0.9921, -0.7420])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 4 7 3]]
Reward:  [ 1.  -2.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 5 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 8 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 5 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '1 3', '4 4', '2', '0', '4', '2 4', '0 3', '4']
6  r_total and score:  266.6000000000004 57.781568665270356
Current Bleu score is:  57.781568665270356
learn step counter:  1501
dev_network_count:  6
learn step counter:  1551
dev_network_count:  6
learn step counter:  1601
dev_network_count:  6
learn step counter:  1651
dev_network_count:  6
learn step counter:  1701
dev_network_count:  6
learn step counter:  1751
dev_network_count:  6

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])
So far:  [array([2]), array([4]), array([4])]  the state[:3] is:  tensor([-0.9673,  0.9915, -0.7579])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 4 4 3]]
Reward:  [ 1.  -2.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 5 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 8 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 5 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '1 1', '4 4', '2', '0', '4', '2 4', '0 3', '4']
7  r_total and score:  263.40000000000043 59.02432522055574
Current Bleu score is:  59.02432522055574
learn step counter:  1801
dev_network_count:  7
learn step counter:  1851
dev_network_count:  7
learn step counter:  1901
dev_network_count:  7
learn step counter:  1951
dev_network_count:  7
EPOCH %d 5
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.5904900000000001 0.5904900000000001
4972 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4973 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4974 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4975 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4976 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4977 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4978 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  2001
dev_network_count:  7
learn step counter:  2051
dev_network_count:  7

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])
So far:  [array([2]), array([4]), array([7])]  the state[:3] is:  tensor([-0.9601,  0.9921, -0.7420])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 4 7 3]]
Reward:  [ 1.  -2.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 5 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 4 6 3]]
Reward:  [1.  0.8 0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '1 3', '4 4', '2', '0', '4', '1 4', '4 3', '4']
8  r_total and score:  257.8000000000004 69.88114688402233
Current Bleu score is:  69.88114688402233
learn step counter:  2101
dev_network_count:  8
learn step counter:  2151
dev_network_count:  8
learn step counter:  2201
dev_network_count:  8
learn step counter:  2251
dev_network_count:  8
learn step counter:  2301
dev_network_count:  8
learn step counter:  2351
dev_network_count:  8

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])
So far:  [array([2]), array([4]), array([4])]  the state[:3] is:  tensor([-0.9673,  0.9915, -0.7579])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 4 4 3]]
Reward:  [ 1.  -2.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 5 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 4 6 3]]
Reward:  [1.  0.8 0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '1 1', '4 4', '2', '0', '4', '1 4', '4 3', '4']
9  r_total and score:  253.4000000000004 72.61510072703895
Current Bleu score is:  72.61510072703895
learn step counter:  2401
dev_network_count:  9
learn step counter:  2451
dev_network_count:  9
learn step counter:  2501
dev_network_count:  9
learn step counter:  2551
dev_network_count:  9
learn step counter:  2601
dev_network_count:  9
learn step counter:  2651
dev_network_count:  9

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])
So far:  [array([2]), array([4]), array([7])]  the state[:3] is:  tensor([-0.9601,  0.9921, -0.7420])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 4 7 3]]
Reward:  [ 1.  -2.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 5 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 4 6 3]]
Reward:  [1.  0.8 0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '1 3', '4 4', '2', '0', '4', '1 4', '4 3', '4']
10  r_total and score:  253.20000000000041 72.90498252127077
Current Bleu score is:  72.90498252127077
learn step counter:  2701
dev_network_count:  10
learn step counter:  2751
dev_network_count:  10
learn step counter:  2801
dev_network_count:  10
learn step counter:  2851
dev_network_count:  10
learn step counter:  2901
dev_network_count:  10
learn step counter:  2951
dev_network_count:  10
EPOCH %d 6
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.531441 0.531441
2465 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
2466 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
2467 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
2468 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
2469 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
2470 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
2471 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])
So far:  [array([2]), array([4]), array([7])]  the state[:3] is:  tensor([-0.9601,  0.9921, -0.7420])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 4 7 3]]
Reward:  [ 1.  -2.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 7 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 4 6 3]]
Reward:  [1.  0.8 0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 5 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1 3', '4 4', '2', '3', '4', '1 4', '0 3', '3']
11  r_total and score:  253.60000000000045 68.37635211648524
Current Bleu score is:  68.37635211648524
learn step counter:  3001
dev_network_count:  11
learn step counter:  3051
dev_network_count:  11
learn step counter:  3101
dev_network_count:  11
learn step counter:  3151
dev_network_count:  11
learn step counter:  3201
dev_network_count:  11
learn step counter:  3251
dev_network_count:  11

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])
So far:  [array([2]), array([4]), array([7])]  the state[:3] is:  tensor([-0.9601,  0.9921, -0.7420])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 4 7 3]]
Reward:  [ 1.  -2.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 7 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 4 6 3]]
Reward:  [1.  0.8 0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '1 3', '4 4', '2', '3', '4', '1 4', '4 3', '4']
12  r_total and score:  264.40000000000043 69.34647072837512
Current Bleu score is:  69.34647072837512
learn step counter:  3301
dev_network_count:  12
learn step counter:  3351
dev_network_count:  12
learn step counter:  3401
dev_network_count:  12
learn step counter:  3451
dev_network_count:  12
learn step counter:  3501
dev_network_count:  12
learn step counter:  3551
dev_network_count:  12

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])
So far:  [array([2]), array([4]), array([7])]  the state[:3] is:  tensor([-0.9601,  0.9921, -0.7420])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 4 7 3]]
Reward:  [ 1.  -2.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 5 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 4 6 3]]
Reward:  [1.  0.8 0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '1 3', '4 4', '2', '0', '4', '1 4', '4 3', '4']
13  r_total and score:  257.8000000000004 69.88114688402233
Current Bleu score is:  69.88114688402233
learn step counter:  3601
dev_network_count:  13
learn step counter:  3651
dev_network_count:  13
learn step counter:  3701
dev_network_count:  13
learn step counter:  3751
dev_network_count:  13
learn step counter:  3801
dev_network_count:  13
learn step counter:  3851
dev_network_count:  13

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])
So far:  [array([2]), array([4]), array([7])]  the state[:3] is:  tensor([-0.9601,  0.9921, -0.7420])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 4 7 3]]
Reward:  [ 1.  -2.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 7 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 4 6 3]]
Reward:  [1.  0.8 0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '1 3', '4 4', '2', '3', '4', '1 4', '4 3', '4']
14  r_total and score:  268.8000000000004 66.68320721193214
Current Bleu score is:  66.68320721193214
learn step counter:  3901
dev_network_count:  14
learn step counter:  3951
dev_network_count:  14
EPOCH %d 7
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.4782969000000001 0.4782969000000001
4958 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4959 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4960 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4961 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4962 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4963 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4964 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  4001
dev_network_count:  14
learn step counter:  4051
dev_network_count:  14
learn step counter:  4101
dev_network_count:  14
learn step counter:  4151
dev_network_count:  14

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])
So far:  [array([2]), array([4]), array([7])]  the state[:3] is:  tensor([-0.9601,  0.9921, -0.7420])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 4 7 3]]
Reward:  [ 1.  -2.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '1 3', '4 4', '2', '4', '4', '4 4', '4 3', '4']
15  r_total and score:  279.20000000000033 52.24489004658875
Current Bleu score is:  52.24489004658875
learn step counter:  4201
dev_network_count:  15
learn step counter:  4251
dev_network_count:  15
learn step counter:  4301
dev_network_count:  15
learn step counter:  4351
dev_network_count:  15
learn step counter:  4401
dev_network_count:  15
learn step counter:  4451
dev_network_count:  15

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])
So far:  [array([2]), array([4]), array([7])]  the state[:3] is:  tensor([-0.9601,  0.9921, -0.7420])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 4 7 3]]
Reward:  [ 1.  -2.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 4 6 3]]
Reward:  [1.  0.8 0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '1 3', '4 4', '2', '4', '4', '1 4', '4 3', '4']
16  r_total and score:  274.20000000000033 59.30741713350149
Current Bleu score is:  59.30741713350149
learn step counter:  4501
dev_network_count:  16
learn step counter:  4551
dev_network_count:  16
learn step counter:  4601
dev_network_count:  16
learn step counter:  4651
dev_network_count:  16
learn step counter:  4701
dev_network_count:  16
learn step counter:  4751
dev_network_count:  16

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])
So far:  [array([2]), array([4]), array([7])]  the state[:3] is:  tensor([-0.9601,  0.9921, -0.7420])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 4 7 3]]
Reward:  [ 1.  -2.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 4 6 3]]
Reward:  [1.  0.8 0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '1 3', '4 4', '2', '4', '4', '1 4', '4 3', '4']
17  r_total and score:  274.20000000000033 59.30741713350149
Current Bleu score is:  59.30741713350149
learn step counter:  4801
dev_network_count:  17
learn step counter:  4851
dev_network_count:  17
learn step counter:  4901
dev_network_count:  17
learn step counter:  4951
dev_network_count:  17
EPOCH %d 8
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.4304672100000001 0.4304672100000001
2451 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
2452 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
2453 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
2454 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
2455 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
2456 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
2457 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  5001
dev_network_count:  17
learn step counter:  5051
dev_network_count:  17

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])
So far:  [array([2]), array([4]), array([7])]  the state[:3] is:  tensor([-0.9601,  0.9921, -0.7420])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 4 7 3]]
Reward:  [ 1.  -2.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '1 3', '4 4', '2', '4', '4', '4 4', '4 3', '4']
18  r_total and score:  272.2000000000004 52.31947714390207
Current Bleu score is:  52.31947714390207
learn step counter:  5101
dev_network_count:  18
learn step counter:  5151
dev_network_count:  18
learn step counter:  5201
dev_network_count:  18
learn step counter:  5251
dev_network_count:  18
learn step counter:  5301
dev_network_count:  18
learn step counter:  5351
dev_network_count:  18

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])
So far:  [array([2]), array([4]), array([7])]  the state[:3] is:  tensor([-0.9601,  0.9921, -0.7420])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 4 7 3]]
Reward:  [ 1.  -2.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '1 3', '4 4', '2', '4', '4', '4 4', '4 3', '4']
19  r_total and score:  275.80000000000035 51.98300924937251
Current Bleu score is:  51.98300924937251
learn step counter:  5401
dev_network_count:  19
learn step counter:  5451
dev_network_count:  19
learn step counter:  5501
dev_network_count:  19
learn step counter:  5551
dev_network_count:  19
learn step counter:  5601
dev_network_count:  19
learn step counter:  5651
dev_network_count:  19

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])
So far:  [array([2]), array([4]), array([7])]  the state[:3] is:  tensor([-0.9601,  0.9921, -0.7420])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 4 7 3]]
Reward:  [ 1.  -2.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1 3', '4 4', '2', '4', '4', '4 4', '4 3', '3']
20  r_total and score:  264.8000000000004 54.79490295253894
Current Bleu score is:  54.79490295253894
learn step counter:  5701
dev_network_count:  20
learn step counter:  5751
dev_network_count:  20
learn step counter:  5801
dev_network_count:  20
learn step counter:  5851
dev_network_count:  20
learn step counter:  5901
dev_network_count:  20
learn step counter:  5951
dev_network_count:  20
EPOCH %d 9
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.3874204890000001 0.3874204890000001
4944 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4945 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4946 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4947 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4948 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4949 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4950 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])
So far:  [array([2]), array([4]), array([7])]  the state[:3] is:  tensor([-0.9601,  0.9921, -0.7420])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 4 7 3]]
Reward:  [ 1.  -2.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 4 7 3]]
Reward:  [ 1.  -2.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1 3', '4 4', '2', '', '4', '1 3', '4', '3']
21  r_total and score:  229.00000000000026 50.18142486417601
Current Bleu score is:  50.18142486417601
learn step counter:  6001
dev_network_count:  21
learn step counter:  6051
dev_network_count:  21
learn step counter:  6101
dev_network_count:  21
learn step counter:  6151
dev_network_count:  21
learn step counter:  6201
dev_network_count:  21
learn step counter:  6251
dev_network_count:  21

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])
So far:  [array([2]), array([4]), array([7])]  the state[:3] is:  tensor([-0.9601,  0.9921, -0.7420])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 4 7 3]]
Reward:  [ 1.  -2.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1 3', '4 4', '2', '4', '4', '3 4', '4 3', '3']
22  r_total and score:  261.0000000000004 55.399683972027674
Current Bleu score is:  55.399683972027674
learn step counter:  6301
dev_network_count:  22
learn step counter:  6351
dev_network_count:  22
learn step counter:  6401
dev_network_count:  22
learn step counter:  6451
dev_network_count:  22
learn step counter:  6501
dev_network_count:  22
learn step counter:  6551
dev_network_count:  22

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])
So far:  [array([2]), array([4]), array([7])]  the state[:3] is:  tensor([-0.9601,  0.9921, -0.7420])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 4 7 3]]
Reward:  [ 1.  -2.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1 3', '4 4', '2', '4', '4', '4 4', '4 3', '3']
23  r_total and score:  261.0000000000004 55.399683972027674
Current Bleu score is:  55.399683972027674
learn step counter:  6601
dev_network_count:  23
learn step counter:  6651
dev_network_count:  23
learn step counter:  6701
dev_network_count:  23
learn step counter:  6751
dev_network_count:  23
learn step counter:  6801
dev_network_count:  23
learn step counter:  6851
dev_network_count:  23

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])
So far:  [array([2]), array([4]), array([7])]  the state[:3] is:  tensor([-0.9601,  0.9921, -0.7420])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 4 7 3]]
Reward:  [ 1.  -2.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1 3', '4 4', '2', '4', '4', '4 4', '4 3', '3']
24  r_total and score:  261.0000000000004 55.399683972027674
Current Bleu score is:  55.399683972027674
learn step counter:  6901
dev_network_count:  24
learn step counter:  6951
dev_network_count:  24
EPOCH %d 10
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.3486784401000001 0.3486784401000001
2437 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
2438 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
2439 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
2440 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
2441 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
2442 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
2443 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  7001
dev_network_count:  24
learn step counter:  7051
dev_network_count:  24
learn step counter:  7101
dev_network_count:  24
learn step counter:  7151
dev_network_count:  24

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])
So far:  [array([2]), array([4]), array([7])]  the state[:3] is:  tensor([-0.9601,  0.9921, -0.7420])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 4 7 3]]
Reward:  [ 1.  -2.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1 3', '4 4', '2', '', '4', '3 4', '4', '3']
25  r_total and score:  222.0000000000003 49.950704447290875
Current Bleu score is:  49.950704447290875
learn step counter:  7201
dev_network_count:  25
learn step counter:  7251
dev_network_count:  25
learn step counter:  7301
dev_network_count:  25
learn step counter:  7351
dev_network_count:  25
learn step counter:  7401
dev_network_count:  25
learn step counter:  7451
dev_network_count:  25

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])
So far:  [array([2]), array([4]), array([7])]  the state[:3] is:  tensor([-0.9601,  0.9921, -0.7420])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 4 7 3]]
Reward:  [ 1.  -2.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1 3', '4 4', '2', '', '4', '3 4', '4', '3']
26  r_total and score:  222.0000000000003 49.950704447290875
Current Bleu score is:  49.950704447290875
learn step counter:  7501
dev_network_count:  26
learn step counter:  7551
dev_network_count:  26
learn step counter:  7601
dev_network_count:  26
learn step counter:  7651
dev_network_count:  26
learn step counter:  7701
dev_network_count:  26
learn step counter:  7751
dev_network_count:  26

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])
So far:  [array([2]), array([4]), array([7])]  the state[:3] is:  tensor([-0.9601,  0.9921, -0.7420])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 4 7 3]]
Reward:  [ 1.  -2.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1 3', '4 4', '2', '', '4', '4 4', '4', '3']
27  r_total and score:  222.0000000000003 49.950704447290875
Current Bleu score is:  49.950704447290875
learn step counter:  7801
dev_network_count:  27
learn step counter:  7851
dev_network_count:  27
learn step counter:  7901
dev_network_count:  27
learn step counter:  7951
dev_network_count:  27
EPOCH %d 11
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.31381059609000006 0.31381059609000006
4930 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4931 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4932 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4933 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4934 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4935 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4936 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  8001
dev_network_count:  27
learn step counter:  8051
dev_network_count:  27

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '4 4', '4', '3']
28  r_total and score:  216.8000000000003 37.79707763349747
Current Bleu score is:  37.79707763349747
learn step counter:  8101
dev_network_count:  28
learn step counter:  8151
dev_network_count:  28
learn step counter:  8201
dev_network_count:  28
learn step counter:  8251
dev_network_count:  28
learn step counter:  8301
dev_network_count:  28
learn step counter:  8351
dev_network_count:  28

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
29  r_total and score:  217.80000000000024 36.840034252847836
Current Bleu score is:  36.840034252847836
learn step counter:  8401
dev_network_count:  29
learn step counter:  8451
dev_network_count:  29
learn step counter:  8501
dev_network_count:  29
learn step counter:  8551
dev_network_count:  29
learn step counter:  8601
dev_network_count:  29
learn step counter:  8651
dev_network_count:  29

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
30  r_total and score:  214.80000000000024 36.840034252847836
Current Bleu score is:  36.840034252847836
learn step counter:  8701
dev_network_count:  30
learn step counter:  8751
dev_network_count:  30
learn step counter:  8801
dev_network_count:  30
learn step counter:  8851
dev_network_count:  30
learn step counter:  8901
dev_network_count:  30
learn step counter:  8951
dev_network_count:  30
EPOCH %d 12
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.2824295364810001 0.3
2423 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
2424 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
2425 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
2426 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
2427 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
2428 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
2429 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
31  r_total and score:  217.80000000000024 36.840034252847836
Current Bleu score is:  36.840034252847836
learn step counter:  9001
dev_network_count:  31
learn step counter:  9051
dev_network_count:  31
learn step counter:  9101
dev_network_count:  31
learn step counter:  9151
dev_network_count:  31
learn step counter:  9201
dev_network_count:  31
learn step counter:  9251
dev_network_count:  31

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
32  r_total and score:  217.80000000000024 36.840034252847836
Current Bleu score is:  36.840034252847836
learn step counter:  9301
dev_network_count:  32
learn step counter:  9351
dev_network_count:  32
learn step counter:  9401
dev_network_count:  32
learn step counter:  9451
dev_network_count:  32
learn step counter:  9501
dev_network_count:  32
learn step counter:  9551
dev_network_count:  32

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
33  r_total and score:  214.80000000000024 36.840034252847836
Current Bleu score is:  36.840034252847836
learn step counter:  9601
dev_network_count:  33
learn step counter:  9651
dev_network_count:  33
learn step counter:  9701
dev_network_count:  33
learn step counter:  9751
dev_network_count:  33
learn step counter:  9801
dev_network_count:  33
learn step counter:  9851
dev_network_count:  33

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
34  r_total and score:  214.80000000000024 36.840034252847836
Current Bleu score is:  36.840034252847836
learn step counter:  9901
dev_network_count:  34
learn step counter:  9951
dev_network_count:  34
EPOCH %d 13
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.2541865828329001 0.3
4916 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4917 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4918 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4919 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4920 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4921 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4922 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  10001
dev_network_count:  34
learn step counter:  10051
dev_network_count:  34
learn step counter:  10101
dev_network_count:  34
learn step counter:  10151
dev_network_count:  34

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
35  r_total and score:  214.80000000000024 36.840034252847836
Current Bleu score is:  36.840034252847836
learn step counter:  10201
dev_network_count:  35
learn step counter:  10251
dev_network_count:  35
learn step counter:  10301
dev_network_count:  35
learn step counter:  10351
dev_network_count:  35
learn step counter:  10401
dev_network_count:  35
learn step counter:  10451
dev_network_count:  35

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
36  r_total and score:  214.80000000000024 36.840034252847836
Current Bleu score is:  36.840034252847836
learn step counter:  10501
dev_network_count:  36
learn step counter:  10551
dev_network_count:  36
learn step counter:  10601
dev_network_count:  36
learn step counter:  10651
dev_network_count:  36
learn step counter:  10701
dev_network_count:  36
learn step counter:  10751
dev_network_count:  36

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
37  r_total and score:  214.80000000000024 36.840034252847836
Current Bleu score is:  36.840034252847836
learn step counter:  10801
dev_network_count:  37
learn step counter:  10851
dev_network_count:  37
learn step counter:  10901
dev_network_count:  37
learn step counter:  10951
dev_network_count:  37
EPOCH %d 14
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.2287679245496101 0.3
2409 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
2410 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
2411 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
2412 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
2413 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
2414 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
2415 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  11001
dev_network_count:  37
learn step counter:  11051
dev_network_count:  37

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
38  r_total and score:  214.80000000000024 36.840034252847836
Current Bleu score is:  36.840034252847836
learn step counter:  11101
dev_network_count:  38
learn step counter:  11151
dev_network_count:  38
learn step counter:  11201
dev_network_count:  38
learn step counter:  11251
dev_network_count:  38
learn step counter:  11301
dev_network_count:  38
learn step counter:  11351
dev_network_count:  38

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
39  r_total and score:  214.80000000000024 36.840034252847836
Current Bleu score is:  36.840034252847836
learn step counter:  11401
dev_network_count:  39
learn step counter:  11451
dev_network_count:  39
learn step counter:  11501
dev_network_count:  39
learn step counter:  11551
dev_network_count:  39
learn step counter:  11601
dev_network_count:  39
learn step counter:  11651
dev_network_count:  39

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
40  r_total and score:  214.80000000000024 36.840034252847836
Current Bleu score is:  36.840034252847836
learn step counter:  11701
dev_network_count:  40
learn step counter:  11751
dev_network_count:  40
learn step counter:  11801
dev_network_count:  40
learn step counter:  11851
dev_network_count:  40
learn step counter:  11901
dev_network_count:  40
learn step counter:  11951
dev_network_count:  40
EPOCH %d 15
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.20589113209464907 0.3
4902 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4903 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4904 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4905 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4906 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4907 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4908 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
41  r_total and score:  214.80000000000024 36.840034252847836
Current Bleu score is:  36.840034252847836
learn step counter:  12001
dev_network_count:  41
learn step counter:  12051
dev_network_count:  41
learn step counter:  12101
dev_network_count:  41
learn step counter:  12151
dev_network_count:  41
learn step counter:  12201
dev_network_count:  41
learn step counter:  12251
dev_network_count:  41

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
42  r_total and score:  214.80000000000024 36.840034252847836
Current Bleu score is:  36.840034252847836
learn step counter:  12301
dev_network_count:  42
learn step counter:  12351
dev_network_count:  42
learn step counter:  12401
dev_network_count:  42
learn step counter:  12451
dev_network_count:  42
learn step counter:  12501
dev_network_count:  42
learn step counter:  12551
dev_network_count:  42

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '4 3', '4', '3']
43  r_total and score:  217.80000000000024 31.746272569217034
Current Bleu score is:  31.746272569217034
learn step counter:  12601
dev_network_count:  43
learn step counter:  12651
dev_network_count:  43
learn step counter:  12701
dev_network_count:  43
learn step counter:  12751
dev_network_count:  43
learn step counter:  12801
dev_network_count:  43
learn step counter:  12851
dev_network_count:  43

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
44  r_total and score:  217.80000000000024 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  12901
dev_network_count:  44
learn step counter:  12951
dev_network_count:  44
EPOCH %d 16
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.18530201888518416 0.3
2395 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
2396 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
2397 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
2398 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
2399 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
2400 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
2401 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  13001
dev_network_count:  44
learn step counter:  13051
dev_network_count:  44
learn step counter:  13101
dev_network_count:  44
learn step counter:  13151
dev_network_count:  44

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
45  r_total and score:  217.80000000000024 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  13201
dev_network_count:  45
learn step counter:  13251
dev_network_count:  45
learn step counter:  13301
dev_network_count:  45
learn step counter:  13351
dev_network_count:  45
learn step counter:  13401
dev_network_count:  45
learn step counter:  13451
dev_network_count:  45

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
46  r_total and score:  217.80000000000024 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  13501
dev_network_count:  46
learn step counter:  13551
dev_network_count:  46
learn step counter:  13601
dev_network_count:  46
learn step counter:  13651
dev_network_count:  46
learn step counter:  13701
dev_network_count:  46
learn step counter:  13751
dev_network_count:  46

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
47  r_total and score:  217.80000000000024 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  13801
dev_network_count:  47
learn step counter:  13851
dev_network_count:  47
learn step counter:  13901
dev_network_count:  47
learn step counter:  13951
dev_network_count:  47
EPOCH %d 17
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.16677181699666577 0.3
4888 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4889 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4890 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4891 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4892 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4893 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4894 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  14001
dev_network_count:  47
learn step counter:  14051
dev_network_count:  47

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
48  r_total and score:  217.80000000000024 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  14101
dev_network_count:  48
learn step counter:  14151
dev_network_count:  48
learn step counter:  14201
dev_network_count:  48
learn step counter:  14251
dev_network_count:  48
learn step counter:  14301
dev_network_count:  48
learn step counter:  14351
dev_network_count:  48

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
49  r_total and score:  217.80000000000024 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  14401
dev_network_count:  49
learn step counter:  14451
dev_network_count:  49
learn step counter:  14501
dev_network_count:  49
learn step counter:  14551
dev_network_count:  49
learn step counter:  14601
dev_network_count:  49
learn step counter:  14651
dev_network_count:  49

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
50  r_total and score:  217.80000000000024 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  14701
dev_network_count:  50
learn step counter:  14751
dev_network_count:  50
learn step counter:  14801
dev_network_count:  50
learn step counter:  14851
dev_network_count:  50
learn step counter:  14901
dev_network_count:  50
learn step counter:  14951
dev_network_count:  50
EPOCH %d 18
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.15009463529699918 0.3
2381 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
2382 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
2383 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
2384 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
2385 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
2386 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
2387 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
51  r_total and score:  217.80000000000024 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  15001
dev_network_count:  51
learn step counter:  15051
dev_network_count:  51
learn step counter:  15101
dev_network_count:  51
learn step counter:  15151
dev_network_count:  51
learn step counter:  15201
dev_network_count:  51
learn step counter:  15251
dev_network_count:  51

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
52  r_total and score:  217.80000000000024 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  15301
dev_network_count:  52
learn step counter:  15351
dev_network_count:  52
learn step counter:  15401
dev_network_count:  52
learn step counter:  15451
dev_network_count:  52
learn step counter:  15501
dev_network_count:  52
learn step counter:  15551
dev_network_count:  52

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
53  r_total and score:  217.80000000000024 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  15601
dev_network_count:  53
learn step counter:  15651
dev_network_count:  53
learn step counter:  15701
dev_network_count:  53
learn step counter:  15751
dev_network_count:  53
learn step counter:  15801
dev_network_count:  53
learn step counter:  15851
dev_network_count:  53

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
54  r_total and score:  217.80000000000024 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  15901
dev_network_count:  54
learn step counter:  15951
dev_network_count:  54
EPOCH %d 19
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.13508517176729928 0.3
4874 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4875 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4876 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4877 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4878 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4879 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4880 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  16001
dev_network_count:  54
learn step counter:  16051
dev_network_count:  54
learn step counter:  16101
dev_network_count:  54
learn step counter:  16151
dev_network_count:  54

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
55  r_total and score:  217.80000000000024 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  16201
dev_network_count:  55
learn step counter:  16251
dev_network_count:  55
learn step counter:  16301
dev_network_count:  55
learn step counter:  16351
dev_network_count:  55
learn step counter:  16401
dev_network_count:  55
learn step counter:  16451
dev_network_count:  55

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
56  r_total and score:  217.80000000000024 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  16501
dev_network_count:  56
learn step counter:  16551
dev_network_count:  56
learn step counter:  16601
dev_network_count:  56
learn step counter:  16651
dev_network_count:  56
learn step counter:  16701
dev_network_count:  56
learn step counter:  16751
dev_network_count:  56

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
57  r_total and score:  217.80000000000024 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  16801
dev_network_count:  57
learn step counter:  16851
dev_network_count:  57
learn step counter:  16901
dev_network_count:  57
learn step counter:  16951
dev_network_count:  57
EPOCH %d 20
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.12157665459056935 0.3
2367 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
2368 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
2369 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
2370 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
2371 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
2372 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
2373 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  17001
dev_network_count:  57
learn step counter:  17051
dev_network_count:  57

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
58  r_total and score:  217.80000000000024 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  17101
dev_network_count:  58
learn step counter:  17151
dev_network_count:  58
learn step counter:  17201
dev_network_count:  58
learn step counter:  17251
dev_network_count:  58
learn step counter:  17301
dev_network_count:  58
learn step counter:  17351
dev_network_count:  58

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
59  r_total and score:  217.80000000000024 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  17401
dev_network_count:  59
learn step counter:  17451
dev_network_count:  59
learn step counter:  17501
dev_network_count:  59
learn step counter:  17551
dev_network_count:  59
learn step counter:  17601
dev_network_count:  59
learn step counter:  17651
dev_network_count:  59

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
60  r_total and score:  217.80000000000024 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  17701
dev_network_count:  60
learn step counter:  17751
dev_network_count:  60
learn step counter:  17801
dev_network_count:  60
learn step counter:  17851
dev_network_count:  60
learn step counter:  17901
dev_network_count:  60
learn step counter:  17951
dev_network_count:  60
EPOCH %d 21
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.10941898913151242 0.3
4860 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4861 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4862 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4863 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4864 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4865 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4866 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
61  r_total and score:  217.80000000000024 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  18001
dev_network_count:  61
learn step counter:  18051
dev_network_count:  61
learn step counter:  18101
dev_network_count:  61
learn step counter:  18151
dev_network_count:  61
learn step counter:  18201
dev_network_count:  61
learn step counter:  18251
dev_network_count:  61

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
62  r_total and score:  217.80000000000024 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  18301
dev_network_count:  62
learn step counter:  18351
dev_network_count:  62
learn step counter:  18401
dev_network_count:  62
learn step counter:  18451
dev_network_count:  62
learn step counter:  18501
dev_network_count:  62
learn step counter:  18551
dev_network_count:  62

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
63  r_total and score:  217.80000000000024 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  18601
dev_network_count:  63
learn step counter:  18651
dev_network_count:  63
learn step counter:  18701
dev_network_count:  63
learn step counter:  18751
dev_network_count:  63
learn step counter:  18801
dev_network_count:  63
learn step counter:  18851
dev_network_count:  63

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
64  r_total and score:  217.80000000000024 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  18901
dev_network_count:  64
learn step counter:  18951
dev_network_count:  64
EPOCH %d 22
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.09847709021836118 0.3
2353 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
2354 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
2355 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
2356 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
2357 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
2358 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
2359 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  19001
dev_network_count:  64
learn step counter:  19051
dev_network_count:  64
learn step counter:  19101
dev_network_count:  64
learn step counter:  19151
dev_network_count:  64

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
65  r_total and score:  217.80000000000024 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  19201
dev_network_count:  65
learn step counter:  19251
dev_network_count:  65
learn step counter:  19301
dev_network_count:  65
learn step counter:  19351
dev_network_count:  65
learn step counter:  19401
dev_network_count:  65
learn step counter:  19451
dev_network_count:  65

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
66  r_total and score:  217.80000000000024 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  19501
dev_network_count:  66
learn step counter:  19551
dev_network_count:  66
learn step counter:  19601
dev_network_count:  66
learn step counter:  19651
dev_network_count:  66
learn step counter:  19701
dev_network_count:  66
learn step counter:  19751
dev_network_count:  66

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
67  r_total and score:  217.80000000000024 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  19801
dev_network_count:  67
learn step counter:  19851
dev_network_count:  67
learn step counter:  19901
dev_network_count:  67
learn step counter:  19951
dev_network_count:  67
EPOCH %d 23
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.08862938119652507 0.3
4846 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4847 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4848 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4849 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4850 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4851 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4852 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  20001
dev_network_count:  67
learn step counter:  20051
dev_network_count:  67

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
68  r_total and score:  221.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  20101
dev_network_count:  68
learn step counter:  20151
dev_network_count:  68
learn step counter:  20201
dev_network_count:  68
learn step counter:  20251
dev_network_count:  68
learn step counter:  20301
dev_network_count:  68
learn step counter:  20351
dev_network_count:  68

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
69  r_total and score:  217.80000000000024 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  20401
dev_network_count:  69
learn step counter:  20451
dev_network_count:  69
learn step counter:  20501
dev_network_count:  69
learn step counter:  20551
dev_network_count:  69
learn step counter:  20601
dev_network_count:  69
learn step counter:  20651
dev_network_count:  69

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
70  r_total and score:  217.80000000000024 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  20701
dev_network_count:  70
learn step counter:  20751
dev_network_count:  70
learn step counter:  20801
dev_network_count:  70
learn step counter:  20851
dev_network_count:  70
learn step counter:  20901
dev_network_count:  70
learn step counter:  20951
dev_network_count:  70
EPOCH %d 24
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.07976644307687256 0.3
2339 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
2340 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
2341 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
2342 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
2343 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
2344 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
2345 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
71  r_total and score:  217.80000000000024 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  21001
dev_network_count:  71
learn step counter:  21051
dev_network_count:  71
learn step counter:  21101
dev_network_count:  71
learn step counter:  21151
dev_network_count:  71
learn step counter:  21201
dev_network_count:  71
learn step counter:  21251
dev_network_count:  71

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
72  r_total and score:  217.80000000000024 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  21301
dev_network_count:  72
learn step counter:  21351
dev_network_count:  72
learn step counter:  21401
dev_network_count:  72
learn step counter:  21451
dev_network_count:  72
learn step counter:  21501
dev_network_count:  72
learn step counter:  21551
dev_network_count:  72

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
73  r_total and score:  217.80000000000024 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  21601
dev_network_count:  73
learn step counter:  21651
dev_network_count:  73
learn step counter:  21701
dev_network_count:  73
learn step counter:  21751
dev_network_count:  73
learn step counter:  21801
dev_network_count:  73
learn step counter:  21851
dev_network_count:  73

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
74  r_total and score:  217.80000000000024 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  21901
dev_network_count:  74
learn step counter:  21951
dev_network_count:  74
EPOCH %d 25
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.0717897987691853 0.3
4832 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4833 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4834 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4835 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4836 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4837 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4838 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  22001
dev_network_count:  74
learn step counter:  22051
dev_network_count:  74
learn step counter:  22101
dev_network_count:  74
learn step counter:  22151
dev_network_count:  74

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
75  r_total and score:  217.80000000000024 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  22201
dev_network_count:  75
learn step counter:  22251
dev_network_count:  75
learn step counter:  22301
dev_network_count:  75
learn step counter:  22351
dev_network_count:  75
learn step counter:  22401
dev_network_count:  75
learn step counter:  22451
dev_network_count:  75

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
76  r_total and score:  217.80000000000024 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  22501
dev_network_count:  76
learn step counter:  22551
dev_network_count:  76
learn step counter:  22601
dev_network_count:  76
learn step counter:  22651
dev_network_count:  76
learn step counter:  22701
dev_network_count:  76
learn step counter:  22751
dev_network_count:  76

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
77  r_total and score:  218.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  22801
dev_network_count:  77
learn step counter:  22851
dev_network_count:  77
learn step counter:  22901
dev_network_count:  77
learn step counter:  22951
dev_network_count:  77
EPOCH %d 26
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.06461081889226677 0.3
2325 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
2326 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
2327 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
2328 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
2329 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
2330 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
2331 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  23001
dev_network_count:  77
learn step counter:  23051
dev_network_count:  77

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
78  r_total and score:  218.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  23101
dev_network_count:  78
learn step counter:  23151
dev_network_count:  78
learn step counter:  23201
dev_network_count:  78
learn step counter:  23251
dev_network_count:  78
learn step counter:  23301
dev_network_count:  78
learn step counter:  23351
dev_network_count:  78

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
79  r_total and score:  218.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  23401
dev_network_count:  79
learn step counter:  23451
dev_network_count:  79
learn step counter:  23501
dev_network_count:  79
learn step counter:  23551
dev_network_count:  79
learn step counter:  23601
dev_network_count:  79
learn step counter:  23651
dev_network_count:  79

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
80  r_total and score:  218.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  23701
dev_network_count:  80
learn step counter:  23751
dev_network_count:  80
learn step counter:  23801
dev_network_count:  80
learn step counter:  23851
dev_network_count:  80
learn step counter:  23901
dev_network_count:  80
learn step counter:  23951
dev_network_count:  80
EPOCH %d 27
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.058149737003040096 0.3
4818 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4819 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4820 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4821 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4822 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4823 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4824 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
81  r_total and score:  218.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  24001
dev_network_count:  81
learn step counter:  24051
dev_network_count:  81
learn step counter:  24101
dev_network_count:  81
learn step counter:  24151
dev_network_count:  81
learn step counter:  24201
dev_network_count:  81
learn step counter:  24251
dev_network_count:  81

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
82  r_total and score:  218.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  24301
dev_network_count:  82
learn step counter:  24351
dev_network_count:  82
learn step counter:  24401
dev_network_count:  82
learn step counter:  24451
dev_network_count:  82
learn step counter:  24501
dev_network_count:  82
learn step counter:  24551
dev_network_count:  82

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
83  r_total and score:  217.80000000000024 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  24601
dev_network_count:  83
learn step counter:  24651
dev_network_count:  83
learn step counter:  24701
dev_network_count:  83
learn step counter:  24751
dev_network_count:  83
learn step counter:  24801
dev_network_count:  83
learn step counter:  24851
dev_network_count:  83

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
84  r_total and score:  218.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  24901
dev_network_count:  84
learn step counter:  24951
dev_network_count:  84
EPOCH %d 28
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.05233476330273609 0.3
2311 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
2312 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
2313 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
2314 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
2315 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
2316 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
2317 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  25001
dev_network_count:  84
learn step counter:  25051
dev_network_count:  84
learn step counter:  25101
dev_network_count:  84
learn step counter:  25151
dev_network_count:  84

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
85  r_total and score:  217.80000000000024 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  25201
dev_network_count:  85
learn step counter:  25251
dev_network_count:  85
learn step counter:  25301
dev_network_count:  85
learn step counter:  25351
dev_network_count:  85
learn step counter:  25401
dev_network_count:  85
learn step counter:  25451
dev_network_count:  85

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
86  r_total and score:  218.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  25501
dev_network_count:  86
learn step counter:  25551
dev_network_count:  86
learn step counter:  25601
dev_network_count:  86
learn step counter:  25651
dev_network_count:  86
learn step counter:  25701
dev_network_count:  86
learn step counter:  25751
dev_network_count:  86

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
87  r_total and score:  218.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  25801
dev_network_count:  87
learn step counter:  25851
dev_network_count:  87
learn step counter:  25901
dev_network_count:  87
learn step counter:  25951
dev_network_count:  87
EPOCH %d 29
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.047101286972462485 0.3
4804 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4805 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4806 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4807 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4808 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4809 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4810 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  26001
dev_network_count:  87
learn step counter:  26051
dev_network_count:  87

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
88  r_total and score:  218.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  26101
dev_network_count:  88
learn step counter:  26151
dev_network_count:  88
learn step counter:  26201
dev_network_count:  88
learn step counter:  26251
dev_network_count:  88
learn step counter:  26301
dev_network_count:  88
learn step counter:  26351
dev_network_count:  88

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
89  r_total and score:  218.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  26401
dev_network_count:  89
learn step counter:  26451
dev_network_count:  89
learn step counter:  26501
dev_network_count:  89
learn step counter:  26551
dev_network_count:  89
learn step counter:  26601
dev_network_count:  89
learn step counter:  26651
dev_network_count:  89

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
90  r_total and score:  218.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  26701
dev_network_count:  90
learn step counter:  26751
dev_network_count:  90
learn step counter:  26801
dev_network_count:  90
learn step counter:  26851
dev_network_count:  90
learn step counter:  26901
dev_network_count:  90
learn step counter:  26951
dev_network_count:  90
EPOCH %d 30
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.04239115827521624 0.3
2297 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
2298 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
2299 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
2300 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
2301 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
2302 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
2303 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
91  r_total and score:  218.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  27001
dev_network_count:  91
learn step counter:  27051
dev_network_count:  91
learn step counter:  27101
dev_network_count:  91
learn step counter:  27151
dev_network_count:  91
learn step counter:  27201
dev_network_count:  91
learn step counter:  27251
dev_network_count:  91

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
92  r_total and score:  217.80000000000024 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  27301
dev_network_count:  92
learn step counter:  27351
dev_network_count:  92
learn step counter:  27401
dev_network_count:  92
learn step counter:  27451
dev_network_count:  92
learn step counter:  27501
dev_network_count:  92
learn step counter:  27551
dev_network_count:  92

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
93  r_total and score:  221.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  27601
dev_network_count:  93
learn step counter:  27651
dev_network_count:  93
learn step counter:  27701
dev_network_count:  93
learn step counter:  27751
dev_network_count:  93
learn step counter:  27801
dev_network_count:  93
learn step counter:  27851
dev_network_count:  93

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
94  r_total and score:  218.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  27901
dev_network_count:  94
learn step counter:  27951
dev_network_count:  94
EPOCH %d 31
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.038152042447694615 0.3
4790 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4791 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4792 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4793 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4794 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4795 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4796 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  28001
dev_network_count:  94
learn step counter:  28051
dev_network_count:  94
learn step counter:  28101
dev_network_count:  94
learn step counter:  28151
dev_network_count:  94

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
95  r_total and score:  218.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  28201
dev_network_count:  95
learn step counter:  28251
dev_network_count:  95
learn step counter:  28301
dev_network_count:  95
learn step counter:  28351
dev_network_count:  95
learn step counter:  28401
dev_network_count:  95
learn step counter:  28451
dev_network_count:  95

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
96  r_total and score:  218.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  28501
dev_network_count:  96
learn step counter:  28551
dev_network_count:  96
learn step counter:  28601
dev_network_count:  96
learn step counter:  28651
dev_network_count:  96
learn step counter:  28701
dev_network_count:  96
learn step counter:  28751
dev_network_count:  96

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
97  r_total and score:  218.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  28801
dev_network_count:  97
learn step counter:  28851
dev_network_count:  97
learn step counter:  28901
dev_network_count:  97
learn step counter:  28951
dev_network_count:  97
EPOCH %d 32
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.03433683820292515 0.3
2283 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
2284 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
2285 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
2286 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
2287 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
2288 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
2289 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  29001
dev_network_count:  97
learn step counter:  29051
dev_network_count:  97

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
98  r_total and score:  218.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  29101
dev_network_count:  98
learn step counter:  29151
dev_network_count:  98
learn step counter:  29201
dev_network_count:  98
learn step counter:  29251
dev_network_count:  98
learn step counter:  29301
dev_network_count:  98
learn step counter:  29351
dev_network_count:  98

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
99  r_total and score:  218.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  29401
dev_network_count:  99
learn step counter:  29451
dev_network_count:  99
learn step counter:  29501
dev_network_count:  99
learn step counter:  29551
dev_network_count:  99
learn step counter:  29601
dev_network_count:  99
learn step counter:  29651
dev_network_count:  99

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
100  r_total and score:  218.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  29701
dev_network_count:  100
learn step counter:  29751
dev_network_count:  100
learn step counter:  29801
dev_network_count:  100
learn step counter:  29851
dev_network_count:  100
learn step counter:  29901
dev_network_count:  100
learn step counter:  29951
dev_network_count:  100
EPOCH %d 33
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.030903154382632636 0.3
4776 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4777 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4778 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4779 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4780 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4781 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4782 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
101  r_total and score:  218.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  30001
dev_network_count:  101
learn step counter:  30051
dev_network_count:  101
learn step counter:  30101
dev_network_count:  101
learn step counter:  30151
dev_network_count:  101
learn step counter:  30201
dev_network_count:  101
learn step counter:  30251
dev_network_count:  101

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
102  r_total and score:  218.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  30301
dev_network_count:  102
learn step counter:  30351
dev_network_count:  102
learn step counter:  30401
dev_network_count:  102
learn step counter:  30451
dev_network_count:  102
learn step counter:  30501
dev_network_count:  102
learn step counter:  30551
dev_network_count:  102

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
103  r_total and score:  218.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  30601
dev_network_count:  103
learn step counter:  30651
dev_network_count:  103
learn step counter:  30701
dev_network_count:  103
learn step counter:  30751
dev_network_count:  103
learn step counter:  30801
dev_network_count:  103
learn step counter:  30851
dev_network_count:  103

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
104  r_total and score:  218.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  30901
dev_network_count:  104
learn step counter:  30951
dev_network_count:  104
EPOCH %d 34
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.027812838944369374 0.3
2269 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
2270 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
2271 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
2272 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
2273 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
2274 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
2275 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  31001
dev_network_count:  104
learn step counter:  31051
dev_network_count:  104
learn step counter:  31101
dev_network_count:  104
learn step counter:  31151
dev_network_count:  104

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
105  r_total and score:  218.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  31201
dev_network_count:  105
learn step counter:  31251
dev_network_count:  105
learn step counter:  31301
dev_network_count:  105
learn step counter:  31351
dev_network_count:  105
learn step counter:  31401
dev_network_count:  105
learn step counter:  31451
dev_network_count:  105

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
106  r_total and score:  221.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  31501
dev_network_count:  106
learn step counter:  31551
dev_network_count:  106
learn step counter:  31601
dev_network_count:  106
learn step counter:  31651
dev_network_count:  106
learn step counter:  31701
dev_network_count:  106
learn step counter:  31751
dev_network_count:  106

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
107  r_total and score:  221.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  31801
dev_network_count:  107
learn step counter:  31851
dev_network_count:  107
learn step counter:  31901
dev_network_count:  107
learn step counter:  31951
dev_network_count:  107
EPOCH %d 35
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.025031555049932437 0.3
4762 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4763 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4764 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4765 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4766 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4767 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4768 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  32001
dev_network_count:  107
learn step counter:  32051
dev_network_count:  107

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
108  r_total and score:  218.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  32101
dev_network_count:  108
learn step counter:  32151
dev_network_count:  108
learn step counter:  32201
dev_network_count:  108
learn step counter:  32251
dev_network_count:  108
learn step counter:  32301
dev_network_count:  108
learn step counter:  32351
dev_network_count:  108

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
109  r_total and score:  218.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  32401
dev_network_count:  109
learn step counter:  32451
dev_network_count:  109
learn step counter:  32501
dev_network_count:  109
learn step counter:  32551
dev_network_count:  109
learn step counter:  32601
dev_network_count:  109
learn step counter:  32651
dev_network_count:  109

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
110  r_total and score:  218.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  32701
dev_network_count:  110
learn step counter:  32751
dev_network_count:  110
learn step counter:  32801
dev_network_count:  110
learn step counter:  32851
dev_network_count:  110
learn step counter:  32901
dev_network_count:  110
learn step counter:  32951
dev_network_count:  110
EPOCH %d 36
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.022528399544939195 0.3
2255 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
2256 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
2257 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
2258 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
2259 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
2260 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
2261 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
111  r_total and score:  218.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  33001
dev_network_count:  111
learn step counter:  33051
dev_network_count:  111
learn step counter:  33101
dev_network_count:  111
learn step counter:  33151
dev_network_count:  111
learn step counter:  33201
dev_network_count:  111
learn step counter:  33251
dev_network_count:  111

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
112  r_total and score:  218.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  33301
dev_network_count:  112
learn step counter:  33351
dev_network_count:  112
learn step counter:  33401
dev_network_count:  112
learn step counter:  33451
dev_network_count:  112
learn step counter:  33501
dev_network_count:  112
learn step counter:  33551
dev_network_count:  112

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
113  r_total and score:  218.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  33601
dev_network_count:  113
learn step counter:  33651
dev_network_count:  113
learn step counter:  33701
dev_network_count:  113
learn step counter:  33751
dev_network_count:  113
learn step counter:  33801
dev_network_count:  113
learn step counter:  33851
dev_network_count:  113

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
114  r_total and score:  218.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  33901
dev_network_count:  114
learn step counter:  33951
dev_network_count:  114
EPOCH %d 37
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.020275559590445275 0.3
4748 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4749 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4750 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4751 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4752 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4753 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4754 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  34001
dev_network_count:  114
learn step counter:  34051
dev_network_count:  114
learn step counter:  34101
dev_network_count:  114
learn step counter:  34151
dev_network_count:  114

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
115  r_total and score:  218.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  34201
dev_network_count:  115
learn step counter:  34251
dev_network_count:  115
learn step counter:  34301
dev_network_count:  115
learn step counter:  34351
dev_network_count:  115
learn step counter:  34401
dev_network_count:  115
learn step counter:  34451
dev_network_count:  115

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
116  r_total and score:  218.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  34501
dev_network_count:  116
learn step counter:  34551
dev_network_count:  116
learn step counter:  34601
dev_network_count:  116
learn step counter:  34651
dev_network_count:  116
learn step counter:  34701
dev_network_count:  116
learn step counter:  34751
dev_network_count:  116

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
117  r_total and score:  221.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  34801
dev_network_count:  117
learn step counter:  34851
dev_network_count:  117
learn step counter:  34901
dev_network_count:  117
learn step counter:  34951
dev_network_count:  117
EPOCH %d 38
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.01824800363140075 0.3
2241 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
2242 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
2243 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
2244 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
2245 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
2246 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
2247 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  35001
dev_network_count:  117
learn step counter:  35051
dev_network_count:  117

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
118  r_total and score:  221.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  35101
dev_network_count:  118
learn step counter:  35151
dev_network_count:  118
learn step counter:  35201
dev_network_count:  118
learn step counter:  35251
dev_network_count:  118
learn step counter:  35301
dev_network_count:  118
learn step counter:  35351
dev_network_count:  118

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
119  r_total and score:  221.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  35401
dev_network_count:  119
learn step counter:  35451
dev_network_count:  119
learn step counter:  35501
dev_network_count:  119
learn step counter:  35551
dev_network_count:  119
learn step counter:  35601
dev_network_count:  119
learn step counter:  35651
dev_network_count:  119

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
120  r_total and score:  221.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  35701
dev_network_count:  120
learn step counter:  35751
dev_network_count:  120
learn step counter:  35801
dev_network_count:  120
learn step counter:  35851
dev_network_count:  120
learn step counter:  35901
dev_network_count:  120
learn step counter:  35951
dev_network_count:  120
EPOCH %d 39
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.016423203268260675 0.3
4734 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4735 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4736 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4737 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4738 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4739 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4740 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
121  r_total and score:  221.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  36001
dev_network_count:  121
learn step counter:  36051
dev_network_count:  121
learn step counter:  36101
dev_network_count:  121
learn step counter:  36151
dev_network_count:  121
learn step counter:  36201
dev_network_count:  121
learn step counter:  36251
dev_network_count:  121

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
122  r_total and score:  218.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  36301
dev_network_count:  122
learn step counter:  36351
dev_network_count:  122
learn step counter:  36401
dev_network_count:  122
learn step counter:  36451
dev_network_count:  122
learn step counter:  36501
dev_network_count:  122
learn step counter:  36551
dev_network_count:  122

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
123  r_total and score:  218.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  36601
dev_network_count:  123
learn step counter:  36651
dev_network_count:  123
learn step counter:  36701
dev_network_count:  123
learn step counter:  36751
dev_network_count:  123
learn step counter:  36801
dev_network_count:  123
learn step counter:  36851
dev_network_count:  123

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
124  r_total and score:  218.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  36901
dev_network_count:  124
learn step counter:  36951
dev_network_count:  124
EPOCH %d 40
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.014780882941434608 0.3
2227 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
2228 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
2229 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
2230 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
2231 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
2232 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
2233 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  37001
dev_network_count:  124
learn step counter:  37051
dev_network_count:  124
learn step counter:  37101
dev_network_count:  124
learn step counter:  37151
dev_network_count:  124

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
125  r_total and score:  221.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  37201
dev_network_count:  125
learn step counter:  37251
dev_network_count:  125
learn step counter:  37301
dev_network_count:  125
learn step counter:  37351
dev_network_count:  125
learn step counter:  37401
dev_network_count:  125
learn step counter:  37451
dev_network_count:  125

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
126  r_total and score:  221.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  37501
dev_network_count:  126
learn step counter:  37551
dev_network_count:  126
learn step counter:  37601
dev_network_count:  126
learn step counter:  37651
dev_network_count:  126
learn step counter:  37701
dev_network_count:  126
learn step counter:  37751
dev_network_count:  126

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
127  r_total and score:  221.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  37801
dev_network_count:  127
learn step counter:  37851
dev_network_count:  127
learn step counter:  37901
dev_network_count:  127
learn step counter:  37951
dev_network_count:  127
EPOCH %d 41
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.013302794647291146 0.3
4720 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4721 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4722 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4723 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4724 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4725 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4726 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  38001
dev_network_count:  127
learn step counter:  38051
dev_network_count:  127

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
128  r_total and score:  221.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  38101
dev_network_count:  128
learn step counter:  38151
dev_network_count:  128
learn step counter:  38201
dev_network_count:  128
learn step counter:  38251
dev_network_count:  128
learn step counter:  38301
dev_network_count:  128
learn step counter:  38351
dev_network_count:  128

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
129  r_total and score:  218.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  38401
dev_network_count:  129
learn step counter:  38451
dev_network_count:  129
learn step counter:  38501
dev_network_count:  129
learn step counter:  38551
dev_network_count:  129
learn step counter:  38601
dev_network_count:  129
learn step counter:  38651
dev_network_count:  129

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
130  r_total and score:  221.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  38701
dev_network_count:  130
learn step counter:  38751
dev_network_count:  130
learn step counter:  38801
dev_network_count:  130
learn step counter:  38851
dev_network_count:  130
learn step counter:  38901
dev_network_count:  130
learn step counter:  38951
dev_network_count:  130
EPOCH %d 42
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.011972515182562033 0.3
2213 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
2214 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
2215 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
2216 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
2217 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
2218 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
2219 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
131  r_total and score:  221.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  39001
dev_network_count:  131
learn step counter:  39051
dev_network_count:  131
learn step counter:  39101
dev_network_count:  131
learn step counter:  39151
dev_network_count:  131
learn step counter:  39201
dev_network_count:  131
learn step counter:  39251
dev_network_count:  131

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
132  r_total and score:  221.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  39301
dev_network_count:  132
learn step counter:  39351
dev_network_count:  132
learn step counter:  39401
dev_network_count:  132
learn step counter:  39451
dev_network_count:  132
learn step counter:  39501
dev_network_count:  132
learn step counter:  39551
dev_network_count:  132

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
133  r_total and score:  221.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  39601
dev_network_count:  133
learn step counter:  39651
dev_network_count:  133
learn step counter:  39701
dev_network_count:  133
learn step counter:  39751
dev_network_count:  133
learn step counter:  39801
dev_network_count:  133
learn step counter:  39851
dev_network_count:  133

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
134  r_total and score:  221.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  39901
dev_network_count:  134
learn step counter:  39951
dev_network_count:  134
EPOCH %d 43
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.01077526366430583 0.3
4706 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4707 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4708 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4709 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4710 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4711 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4712 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  40001
dev_network_count:  134
learn step counter:  40051
dev_network_count:  134
learn step counter:  40101
dev_network_count:  134
learn step counter:  40151
dev_network_count:  134

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
135  r_total and score:  221.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  40201
dev_network_count:  135
learn step counter:  40251
dev_network_count:  135
learn step counter:  40301
dev_network_count:  135
learn step counter:  40351
dev_network_count:  135
learn step counter:  40401
dev_network_count:  135
learn step counter:  40451
dev_network_count:  135

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
136  r_total and score:  221.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  40501
dev_network_count:  136
learn step counter:  40551
dev_network_count:  136
learn step counter:  40601
dev_network_count:  136
learn step counter:  40651
dev_network_count:  136
learn step counter:  40701
dev_network_count:  136
learn step counter:  40751
dev_network_count:  136

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
137  r_total and score:  221.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  40801
dev_network_count:  137
learn step counter:  40851
dev_network_count:  137
learn step counter:  40901
dev_network_count:  137
learn step counter:  40951
dev_network_count:  137
EPOCH %d 44
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.009697737297875247 0.3
2199 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
2200 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
2201 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
2202 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
2203 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
2204 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
2205 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  41001
dev_network_count:  137
learn step counter:  41051
dev_network_count:  137

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
138  r_total and score:  221.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  41101
dev_network_count:  138
learn step counter:  41151
dev_network_count:  138
learn step counter:  41201
dev_network_count:  138
learn step counter:  41251
dev_network_count:  138
learn step counter:  41301
dev_network_count:  138
learn step counter:  41351
dev_network_count:  138

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
139  r_total and score:  221.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  41401
dev_network_count:  139
learn step counter:  41451
dev_network_count:  139
learn step counter:  41501
dev_network_count:  139
learn step counter:  41551
dev_network_count:  139
learn step counter:  41601
dev_network_count:  139
learn step counter:  41651
dev_network_count:  139

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
140  r_total and score:  221.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  41701
dev_network_count:  140
learn step counter:  41751
dev_network_count:  140
learn step counter:  41801
dev_network_count:  140
learn step counter:  41851
dev_network_count:  140
learn step counter:  41901
dev_network_count:  140
learn step counter:  41951
dev_network_count:  140
EPOCH %d 45
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.008727963568087723 0.3
4692 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4693 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4694 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4695 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4696 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4697 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4698 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
141  r_total and score:  221.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  42001
dev_network_count:  141
learn step counter:  42051
dev_network_count:  141
learn step counter:  42101
dev_network_count:  141
learn step counter:  42151
dev_network_count:  141
learn step counter:  42201
dev_network_count:  141
learn step counter:  42251
dev_network_count:  141

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
142  r_total and score:  221.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  42301
dev_network_count:  142
learn step counter:  42351
dev_network_count:  142
learn step counter:  42401
dev_network_count:  142
learn step counter:  42451
dev_network_count:  142
learn step counter:  42501
dev_network_count:  142
learn step counter:  42551
dev_network_count:  142

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
143  r_total and score:  221.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  42601
dev_network_count:  143
learn step counter:  42651
dev_network_count:  143
learn step counter:  42701
dev_network_count:  143
learn step counter:  42751
dev_network_count:  143
learn step counter:  42801
dev_network_count:  143
learn step counter:  42851
dev_network_count:  143

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
144  r_total and score:  221.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  42901
dev_network_count:  144
learn step counter:  42951
dev_network_count:  144
EPOCH %d 46
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.00785516721127895 0.3
2185 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
2186 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
2187 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
2188 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
2189 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
2190 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
2191 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  43001
dev_network_count:  144
learn step counter:  43051
dev_network_count:  144
learn step counter:  43101
dev_network_count:  144
learn step counter:  43151
dev_network_count:  144

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
145  r_total and score:  221.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  43201
dev_network_count:  145
learn step counter:  43251
dev_network_count:  145
learn step counter:  43301
dev_network_count:  145
learn step counter:  43351
dev_network_count:  145
learn step counter:  43401
dev_network_count:  145
learn step counter:  43451
dev_network_count:  145

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
146  r_total and score:  221.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  43501
dev_network_count:  146
learn step counter:  43551
dev_network_count:  146
learn step counter:  43601
dev_network_count:  146
learn step counter:  43651
dev_network_count:  146
learn step counter:  43701
dev_network_count:  146
learn step counter:  43751
dev_network_count:  146

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
147  r_total and score:  221.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  43801
dev_network_count:  147
learn step counter:  43851
dev_network_count:  147
learn step counter:  43901
dev_network_count:  147
learn step counter:  43951
dev_network_count:  147
EPOCH %d 47
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.007069650490151055 0.3
4678 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4679 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4680 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4681 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4682 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4683 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4684 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  44001
dev_network_count:  147
learn step counter:  44051
dev_network_count:  147

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
148  r_total and score:  221.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  44101
dev_network_count:  148
learn step counter:  44151
dev_network_count:  148
learn step counter:  44201
dev_network_count:  148
learn step counter:  44251
dev_network_count:  148
learn step counter:  44301
dev_network_count:  148
learn step counter:  44351
dev_network_count:  148

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
149  r_total and score:  221.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  44401
dev_network_count:  149
learn step counter:  44451
dev_network_count:  149
learn step counter:  44501
dev_network_count:  149
learn step counter:  44551
dev_network_count:  149
learn step counter:  44601
dev_network_count:  149
learn step counter:  44651
dev_network_count:  149

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -2.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-1.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-1.  -0.2] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
150  r_total and score:  221.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  44701
dev_network_count:  150
learn step counter:  44751
dev_network_count:  150
