2020-03-22 18:53:47,237 Hello! This is Joey-NMT.
2020-03-22 18:53:47,240 Total params: 105088
2020-03-22 18:53:47,240 Trainable parameters: ['decoder.att_vector_layer.bias', 'decoder.att_vector_layer.weight', 'decoder.attention.key_layer.weight', 'decoder.output_layer.weight', 'decoder.rnn.bias_hh_l0', 'decoder.rnn.bias_ih_l0', 'decoder.rnn.weight_hh_l0', 'decoder.rnn.weight_ih_l0', 'encoder.rnn.bias_hh_l0', 'encoder.rnn.bias_hh_l0_reverse', 'encoder.rnn.bias_ih_l0', 'encoder.rnn.bias_ih_l0_reverse', 'encoder.rnn.weight_hh_l0', 'encoder.rnn.weight_hh_l0_reverse', 'encoder.rnn.weight_ih_l0', 'encoder.rnn.weight_ih_l0_reverse', 'src_embed.lut.weight', 'trg_embed.lut.weight']
2020-03-22 18:53:47,241 cfg.name                           : reverse_experiment
2020-03-22 18:53:47,241 cfg.data.src                       : src
2020-03-22 18:53:47,241 cfg.data.trg                       : trg
2020-03-22 18:53:47,241 cfg.data.train                     : test/data/reverse/train
2020-03-22 18:53:47,241 cfg.data.dev                       : test/data/reverse/dev
2020-03-22 18:53:47,241 cfg.data.test                      : test/data/reverse/test
2020-03-22 18:53:47,241 cfg.data.level                     : word
2020-03-22 18:53:47,241 cfg.data.lowercase                 : False
2020-03-22 18:53:47,241 cfg.data.max_sent_length           : 25
2020-03-22 18:53:47,241 cfg.data.src_voc_min_freq          : 0
2020-03-22 18:53:47,242 cfg.data.src_voc_limit             : 100
2020-03-22 18:53:47,242 cfg.data.trg_voc_min_freq          : 0
2020-03-22 18:53:47,242 cfg.data.trg_voc_limit             : 100
2020-03-22 18:53:47,242 cfg.testing.beam_size              : 1
2020-03-22 18:53:47,242 cfg.testing.alpha                  : 1.0
2020-03-22 18:53:47,242 cfg.training.random_seed           : 42
2020-03-22 18:53:47,242 cfg.training.optimizer             : adam
2020-03-22 18:53:47,242 cfg.training.learning_rate         : 0.001
2020-03-22 18:53:47,242 cfg.training.learning_rate_min     : 0.0002
2020-03-22 18:53:47,242 cfg.training.weight_decay          : 0.0
2020-03-22 18:53:47,242 cfg.training.clip_grad_norm        : 1.0
2020-03-22 18:53:47,242 cfg.training.batch_size            : 10
2020-03-22 18:53:47,243 cfg.training.batch_type            : sentence
2020-03-22 18:53:47,243 cfg.training.scheduling            : plateau
2020-03-22 18:53:47,243 cfg.training.patience              : 5
2020-03-22 18:53:47,243 cfg.training.decrease_factor       : 0.5
2020-03-22 18:53:47,243 cfg.training.early_stopping_metric : eval_metric
2020-03-22 18:53:47,243 cfg.training.epochs                : 2
2020-03-22 18:53:47,243 cfg.training.validation_freq       : 1000
2020-03-22 18:53:47,243 cfg.training.logging_freq          : 100
2020-03-22 18:53:47,243 cfg.training.eval_metric           : bleu
2020-03-22 18:53:47,243 cfg.training.model_dir             : reverse_model
2020-03-22 18:53:47,243 cfg.training.overwrite             : True
2020-03-22 18:53:47,244 cfg.training.shuffle               : True
2020-03-22 18:53:47,244 cfg.training.use_cuda              : False
2020-03-22 18:53:47,244 cfg.training.max_output_length     : 30
2020-03-22 18:53:47,244 cfg.training.print_valid_sents     : [0, 3, 6]
2020-03-22 18:53:47,244 cfg.training.keep_last_ckpts       : 2
2020-03-22 18:53:47,244 cfg.model.initializer              : xavier
2020-03-22 18:53:47,244 cfg.model.embed_initializer        : normal
2020-03-22 18:53:47,244 cfg.model.embed_init_weight        : 0.1
2020-03-22 18:53:47,245 cfg.model.bias_initializer         : zeros
2020-03-22 18:53:47,245 cfg.model.init_rnn_orthogonal      : False
2020-03-22 18:53:47,245 cfg.model.lstm_forget_gate         : 0.0
2020-03-22 18:53:47,245 cfg.model.encoder.rnn_type         : lstm
2020-03-22 18:53:47,245 cfg.model.encoder.embeddings.embedding_dim : 16
2020-03-22 18:53:47,245 cfg.model.encoder.embeddings.scale : False
2020-03-22 18:53:47,245 cfg.model.encoder.hidden_size      : 64
2020-03-22 18:53:47,245 cfg.model.encoder.bidirectional    : True
2020-03-22 18:53:47,245 cfg.model.encoder.dropout          : 0.1
2020-03-22 18:53:47,245 cfg.model.encoder.num_layers       : 1
2020-03-22 18:53:47,245 cfg.model.decoder.rnn_type         : lstm
2020-03-22 18:53:47,245 cfg.model.decoder.embeddings.embedding_dim : 16
2020-03-22 18:53:47,246 cfg.model.decoder.embeddings.scale : False
2020-03-22 18:53:47,246 cfg.model.decoder.hidden_size      : 64
2020-03-22 18:53:47,246 cfg.model.decoder.dropout          : 0.1
2020-03-22 18:53:47,246 cfg.model.decoder.hidden_dropout   : 0.1
2020-03-22 18:53:47,246 cfg.model.decoder.num_layers       : 1
2020-03-22 18:53:47,246 cfg.model.decoder.input_feeding    : True
2020-03-22 18:53:47,246 cfg.model.decoder.init_hidden      : last
2020-03-22 18:53:47,246 cfg.model.decoder.attention        : luong
2020-03-22 18:53:47,246 cfg.dqn.sample_size                : 32
2020-03-22 18:53:47,246 cfg.dqn.lr                         : 0.01
2020-03-22 18:53:47,246 cfg.dqn.egreed_max                 : 0.9
2020-03-22 18:53:47,247 cfg.dqn.egreed_min                 : 0.01
2020-03-22 18:53:47,247 cfg.dqn.gamma_max                  : 0.9
2020-03-22 18:53:47,247 cfg.dqn.gamma_min                  : 0.5
2020-03-22 18:53:47,247 cfg.dqn.nu_iter                    : 100
2020-03-22 18:53:47,247 cfg.dqn.mem_cap                    : 2000
2020-03-22 18:53:47,247 cfg.dqn.beam_min                   : 1
2020-03-22 18:53:47,247 cfg.dqn.beam_max                   : 50
2020-03-22 18:53:47,247 cfg.dqn.state_type                 : hidden
2020-03-22 18:53:47,247 Data set sizes: 
	train 50000,
	valid 1000,
	test 1001
2020-03-22 18:53:47,247 First training example:
	[SRC] 28 14 42 7 20 38 18
	[TRG] 18 38 20 7 42 14 28
2020-03-22 18:53:47,247 First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) 15 (5) 35 (6) 44 (7) 18 (8) 36 (9) 16
2020-03-22 18:53:47,248 First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) 15 (5) 35 (6) 44 (7) 18 (8) 36 (9) 16
2020-03-22 18:53:47,248 Number of Src words (types): 54
2020-03-22 18:53:47,248 Number of Trg words (types): 54
2020-03-22 18:53:47,248 Model(
	encoder=RecurrentEncoder(LSTM(16, 64, batch_first=True, bidirectional=True)),
	decoder=RecurrentDecoder(rnn=LSTM(80, 64, batch_first=True), attention=LuongAttention),
	src_embed=Embeddings(embedding_dim=16, vocab_size=54),
	trg_embed=Embeddings(embedding_dim=16, vocab_size=54))
2020-03-22 18:53:47,249 EPOCH 1
2020-03-22 18:53:56,930 Epoch   1 Step:      100 Batch Loss:    57.243568 Tokens per Sec:     1407, Lr: 0.001000
2020-03-22 18:54:06,065 Epoch   1 Step:      200 Batch Loss:     8.668252 Tokens per Sec:     1548, Lr: 0.001000
2020-03-22 18:54:16,467 Epoch   1 Step:      300 Batch Loss:    43.392662 Tokens per Sec:     1340, Lr: 0.001000
2020-03-22 18:54:24,463 Epoch   1 Step:      400 Batch Loss:    72.297943 Tokens per Sec:     1779, Lr: 0.001000
2020-03-22 18:54:32,990 Epoch   1 Step:      500 Batch Loss:    39.180248 Tokens per Sec:     1628, Lr: 0.001000
2020-03-22 18:54:42,841 Epoch   1 Step:      600 Batch Loss:    36.581722 Tokens per Sec:     1428, Lr: 0.001000
2020-03-22 18:54:51,884 Epoch   1 Step:      700 Batch Loss:     6.573117 Tokens per Sec:     1533, Lr: 0.001000
2020-03-22 18:54:59,550 Epoch   1 Step:      800 Batch Loss:    54.806805 Tokens per Sec:     1804, Lr: 0.001000
2020-03-22 18:55:08,893 Epoch   1 Step:      900 Batch Loss:    75.134766 Tokens per Sec:     1503, Lr: 0.001000
2020-03-22 18:55:17,428 Epoch   1 Step:     1000 Batch Loss:    64.440323 Tokens per Sec:     1635, Lr: 0.001000
2020-03-22 18:55:25,876 Hooray! New best validation result [eval_metric]!
2020-03-22 18:55:25,877 Saving new checkpoint.
2020-03-22 18:55:25,881 Example #0
2020-03-22 18:55:25,881 	Raw source:     ['33', '9', '15', '3', '14', '33', '32', '42', '23', '12', '14', '17', '4', '35', '0', '48', '46', '36', '46', '27', '2', '34', '35', '17', '36', '39', '7', '14', '9', '0']
2020-03-22 18:55:25,881 	Raw hypothesis: ['0', '9', '14', '33', '39', '36', '17', '35', '35', '10', '37', '27', '46', '22', '22', '46', '46', '46', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0']
2020-03-22 18:55:25,882 	Source:     33 9 15 3 14 33 32 42 23 12 14 17 4 35 0 48 46 36 46 27 2 34 35 17 36 39 7 14 9 0
2020-03-22 18:55:25,882 	Reference:  0 9 14 7 39 36 17 35 34 2 27 46 36 46 48 0 35 4 17 14 12 23 42 32 33 14 3 15 9 33
2020-03-22 18:55:25,882 	Hypothesis: 0 9 14 33 39 36 17 35 35 10 37 27 46 22 22 46 46 46 0 0 0 0 0 0 0 0 0 0 0 0
2020-03-22 18:55:25,882 Example #3
2020-03-22 18:55:25,882 	Raw source:     ['10', '43', '37', '32', '6', '9', '25', '36', '21', '29', '16', '7', '18', '27', '30', '46', '37', '15', '7', '48', '18']
2020-03-22 18:55:25,882 	Raw hypothesis: ['18', '48', '33', '15', '37', '46', '46', '30', '27', '18', '33', '29', '29', '29', '9', '9', '41', '29', '37', '43', '10', '10']
2020-03-22 18:55:25,882 	Source:     10 43 37 32 6 9 25 36 21 29 16 7 18 27 30 46 37 15 7 48 18
2020-03-22 18:55:25,882 	Reference:  18 48 7 15 37 46 30 27 18 7 16 29 21 36 25 9 6 32 37 43 10
2020-03-22 18:55:25,882 	Hypothesis: 18 48 33 15 37 46 46 30 27 18 33 29 29 29 9 9 41 29 37 43 10 10
2020-03-22 18:55:25,882 Example #6
2020-03-22 18:55:25,882 	Raw source:     ['0', '38', '14', '26', '20', '34', '10', '36', '11', '32', '29', '21']
2020-03-22 18:55:25,882 	Raw hypothesis: ['21', '29', '32', '11', '36', '10', '34', '20', '26', '26', '26', '0']
2020-03-22 18:55:25,883 	Source:     0 38 14 26 20 34 10 36 11 32 29 21
2020-03-22 18:55:25,883 	Reference:  21 29 32 11 36 10 34 20 26 14 38 0
2020-03-22 18:55:25,883 	Hypothesis: 21 29 32 11 36 10 34 20 26 26 26 0
2020-03-22 18:55:25,883 Validation result (greedy) at epoch   1, step     1000: bleu:  39.65, loss: 38863.0625, ppl:  10.8175, duration: 8.4542s
2020-03-22 18:55:37,570 Epoch   1 Step:     1100 Batch Loss:    42.405037 Tokens per Sec:     1180, Lr: 0.001000
2020-03-22 18:55:48,240 Epoch   1 Step:     1200 Batch Loss:    15.381673 Tokens per Sec:     1323, Lr: 0.001000
2020-03-22 18:55:57,741 Epoch   1 Step:     1300 Batch Loss:     2.554527 Tokens per Sec:     1486, Lr: 0.001000
2020-03-22 18:56:06,574 Epoch   1 Step:     1400 Batch Loss:     0.411741 Tokens per Sec:     1576, Lr: 0.001000
