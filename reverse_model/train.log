2020-04-01 21:48:10,408 Hello! This is Joey-NMT.
2020-04-01 21:48:10,411 Total params: 10048
2020-04-01 21:48:10,412 Trainable parameters: ['decoder.att_vector_layer.bias', 'decoder.att_vector_layer.weight', 'decoder.attention.key_layer.weight', 'decoder.bridge_layer.bias', 'decoder.bridge_layer.weight', 'decoder.output_layer.weight', 'decoder.rnn.bias_hh_l0', 'decoder.rnn.bias_ih_l0', 'decoder.rnn.weight_hh_l0', 'decoder.rnn.weight_ih_l0', 'encoder.rnn.bias_hh_l0', 'encoder.rnn.bias_hh_l0_reverse', 'encoder.rnn.bias_ih_l0', 'encoder.rnn.bias_ih_l0_reverse', 'encoder.rnn.weight_hh_l0', 'encoder.rnn.weight_hh_l0_reverse', 'encoder.rnn.weight_ih_l0', 'encoder.rnn.weight_ih_l0_reverse', 'src_embed.lut.weight', 'trg_embed.lut.weight']
2020-04-01 21:48:10,412 cfg.name                           : reverse_experiment
2020-04-01 21:48:10,413 cfg.data.src                       : src
2020-04-01 21:48:10,413 cfg.data.trg                       : trg
2020-04-01 21:48:10,413 cfg.data.train                     : test/data/reverse/train
2020-04-01 21:48:10,413 cfg.data.dev                       : test/data/reverse/dev
2020-04-01 21:48:10,413 cfg.data.test                      : test/data/reverse/test
2020-04-01 21:48:10,413 cfg.data.level                     : word
2020-04-01 21:48:10,413 cfg.data.lowercase                 : False
2020-04-01 21:48:10,413 cfg.data.max_sent_length           : 25
2020-04-01 21:48:10,414 cfg.data.src_voc_min_freq          : 0
2020-04-01 21:48:10,414 cfg.data.src_voc_limit             : 100
2020-04-01 21:48:10,414 cfg.data.trg_voc_min_freq          : 0
2020-04-01 21:48:10,414 cfg.data.trg_voc_limit             : 100
2020-04-01 21:48:10,414 cfg.testing.beam_size              : 1
2020-04-01 21:48:10,414 cfg.testing.alpha                  : 1.0
2020-04-01 21:48:10,414 cfg.training.random_seed           : 42
2020-04-01 21:48:10,414 cfg.training.optimizer             : adam
2020-04-01 21:48:10,415 cfg.training.learning_rate         : 0.001
2020-04-01 21:48:10,415 cfg.training.learning_rate_min     : 0.0002
2020-04-01 21:48:10,415 cfg.training.weight_decay          : 0.0
2020-04-01 21:48:10,415 cfg.training.clip_grad_norm        : 1.0
2020-04-01 21:48:10,415 cfg.training.batch_size            : 10
2020-04-01 21:48:10,415 cfg.training.batch_type            : sentence
2020-04-01 21:48:10,415 cfg.training.scheduling            : plateau
2020-04-01 21:48:10,415 cfg.training.patience              : 5
2020-04-01 21:48:10,415 cfg.training.decrease_factor       : 0.5
2020-04-01 21:48:10,416 cfg.training.early_stopping_metric : eval_metric
2020-04-01 21:48:10,416 cfg.training.epochs                : 2
2020-04-01 21:48:10,416 cfg.training.validation_freq       : 1000
2020-04-01 21:48:10,416 cfg.training.logging_freq          : 100
2020-04-01 21:48:10,416 cfg.training.eval_metric           : bleu
2020-04-01 21:48:10,416 cfg.training.model_dir             : reverse_model
2020-04-01 21:48:10,416 cfg.training.overwrite             : True
2020-04-01 21:48:10,416 cfg.training.shuffle               : True
2020-04-01 21:48:10,417 cfg.training.use_cuda              : False
2020-04-01 21:48:10,417 cfg.training.max_output_length     : 10
2020-04-01 21:48:10,417 cfg.training.print_valid_sents     : [0, 3, 6]
2020-04-01 21:48:10,417 cfg.training.keep_last_ckpts       : 2
2020-04-01 21:48:10,417 cfg.model.initializer              : xavier
2020-04-01 21:48:10,417 cfg.model.embed_initializer        : normal
2020-04-01 21:48:10,417 cfg.model.embed_init_weight        : 0.1
2020-04-01 21:48:10,417 cfg.model.bias_initializer         : zeros
2020-04-01 21:48:10,418 cfg.model.init_rnn_orthogonal      : False
2020-04-01 21:48:10,418 cfg.model.lstm_forget_gate         : 0.0
2020-04-01 21:48:10,418 cfg.model.encoder.rnn_type         : lstm
2020-04-01 21:48:10,418 cfg.model.encoder.embeddings.embedding_dim : 16
2020-04-01 21:48:10,418 cfg.model.encoder.embeddings.scale : False
2020-04-01 21:48:10,418 cfg.model.encoder.hidden_size      : 16
2020-04-01 21:48:10,418 cfg.model.encoder.bidirectional    : True
2020-04-01 21:48:10,418 cfg.model.encoder.dropout          : 0.1
2020-04-01 21:48:10,419 cfg.model.encoder.num_layers       : 1
2020-04-01 21:48:10,419 cfg.model.decoder.rnn_type         : lstm
2020-04-01 21:48:10,419 cfg.model.decoder.embeddings.embedding_dim : 16
2020-04-01 21:48:10,419 cfg.model.decoder.embeddings.scale : False
2020-04-01 21:48:10,419 cfg.model.decoder.hidden_size      : 16
2020-04-01 21:48:10,419 cfg.model.decoder.dropout          : 0.1
2020-04-01 21:48:10,419 cfg.model.decoder.hidden_dropout   : 0.1
2020-04-01 21:48:10,419 cfg.model.decoder.num_layers       : 1
2020-04-01 21:48:10,419 cfg.model.decoder.input_feeding    : True
2020-04-01 21:48:10,419 cfg.model.decoder.init_hidden      : bridge
2020-04-01 21:48:10,419 cfg.model.decoder.attention        : luong
2020-04-01 21:48:10,419 cfg.dqn.epochs                     : 2000
2020-04-01 21:48:10,419 cfg.dqn.sample_size                : 256
2020-04-01 21:48:10,420 cfg.dqn.lr                         : 0.01
2020-04-01 21:48:10,420 cfg.dqn.egreed_max                 : 0.9
2020-04-01 21:48:10,420 cfg.dqn.egreed_min                 : 0.001
2020-04-01 21:48:10,420 cfg.dqn.gamma_max                  : 0.9
2020-04-01 21:48:10,420 cfg.dqn.gamma_min                  : 0.3
2020-04-01 21:48:10,420 cfg.dqn.nu_iter                    : 300
2020-04-01 21:48:10,420 cfg.dqn.mem_cap                    : 5000
2020-04-01 21:48:10,420 cfg.dqn.beam_min                   : 2
2020-04-01 21:48:10,420 cfg.dqn.beam_max                   : 50
2020-04-01 21:48:10,420 cfg.dqn.state_type                 : hidden
2020-04-01 21:48:10,420 cfg.dqn.reward_type                : bleu_fin
2020-04-01 21:48:10,420 Data set sizes: 
	train 10000,
	valid 200,
	test 200
2020-04-01 21:48:10,420 First training example:
	[SRC] 7 4 6 9
	[TRG] 9 6 4 7
2020-04-01 21:48:10,420 First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) 9 (5) 5 (6) 6 (7) 4 (8) 3 (9) 2
2020-04-01 21:48:10,421 First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) 9 (5) 5 (6) 6 (7) 4 (8) 3 (9) 2
2020-04-01 21:48:10,421 Number of Src words (types): 14
2020-04-01 21:48:10,421 Number of Trg words (types): 14
2020-04-01 21:48:10,421 Model(
	encoder=RecurrentEncoder(LSTM(16, 16, batch_first=True, bidirectional=True)),
	decoder=RecurrentDecoder(rnn=LSTM(32, 16, batch_first=True), attention=LuongAttention),
	src_embed=Embeddings(embedding_dim=16, vocab_size=14),
	trg_embed=Embeddings(embedding_dim=16, vocab_size=14))
2020-04-01 21:48:10,421 EPOCH 1
2020-04-01 21:48:15,323 Epoch   1 Step:      100 Batch Loss:     6.186507 Tokens per Sec:      811, Lr: 0.001000
2020-04-01 21:48:18,077 Epoch   1 Step:      200 Batch Loss:     9.418789 Tokens per Sec:     1455, Lr: 0.001000
2020-04-01 21:48:20,467 Epoch   1 Step:      300 Batch Loss:     1.943379 Tokens per Sec:     1689, Lr: 0.001000
2020-04-01 21:48:23,532 Epoch   1 Step:      400 Batch Loss:     1.045665 Tokens per Sec:     1319, Lr: 0.001000
2020-04-01 21:48:26,607 Epoch   1 Step:      500 Batch Loss:     0.868218 Tokens per Sec:     1305, Lr: 0.001000
2020-04-01 21:48:29,239 Epoch   1 Step:      600 Batch Loss:     3.550537 Tokens per Sec:     1504, Lr: 0.001000
2020-04-01 21:48:32,328 Epoch   1 Step:      700 Batch Loss:     3.828301 Tokens per Sec:     1283, Lr: 0.001000
2020-04-01 21:48:35,871 Epoch   1 Step:      800 Batch Loss:     1.727729 Tokens per Sec:     1118, Lr: 0.001000
2020-04-01 21:48:39,533 Epoch   1 Step:      900 Batch Loss:     4.637763 Tokens per Sec:     1105, Lr: 0.001000
2020-04-01 21:48:42,845 Epoch   1 Step:     1000 Batch Loss:     4.637602 Tokens per Sec:     1213, Lr: 0.001000
2020-04-01 21:48:43,201 Hooray! New best validation result [eval_metric]!
2020-04-01 21:48:43,202 Saving new checkpoint.
2020-04-01 21:48:43,205 Example #0
2020-04-01 21:48:43,205 	Raw source:     ['5']
2020-04-01 21:48:43,205 	Raw hypothesis: ['5']
2020-04-01 21:48:43,205 	Source:     5
2020-04-01 21:48:43,205 	Reference:  5
2020-04-01 21:48:43,206 	Hypothesis: 5
2020-04-01 21:48:43,206 Example #3
2020-04-01 21:48:43,206 	Raw source:     ['9', '5', '2', '8', '9']
2020-04-01 21:48:43,206 	Raw hypothesis: ['9', '8', '2', '5', '9']
2020-04-01 21:48:43,206 	Source:     9 5 2 8 9
2020-04-01 21:48:43,206 	Reference:  9 8 2 5 9
2020-04-01 21:48:43,206 	Hypothesis: 9 8 2 5 9
2020-04-01 21:48:43,206 Example #6
2020-04-01 21:48:43,206 	Raw source:     ['8', '3', '4', '5', '8', '5']
2020-04-01 21:48:43,206 	Raw hypothesis: ['5', '8', '5', '3', '8']
2020-04-01 21:48:43,207 	Source:     8 3 4 5 8 5
2020-04-01 21:48:43,207 	Reference:  5 8 5 4 3 8
2020-04-01 21:48:43,207 	Hypothesis: 5 8 5 3 8
2020-04-01 21:48:43,207 Validation result (greedy) at epoch   1, step     1000: bleu:  70.44, loss: 495.8634, ppl:   1.7244, duration: 0.3616s
2020-04-01 21:48:44,082 Epoch   1: total training loss 4834.75
2020-04-01 21:48:44,083 EPOCH 2
2020-04-01 21:48:47,773 Epoch   2 Step:     1100 Batch Loss:     0.221553 Tokens per Sec:     1086, Lr: 0.001000
2020-04-01 21:48:51,083 Epoch   2 Step:     1200 Batch Loss:     1.396058 Tokens per Sec:     1202, Lr: 0.001000
2020-04-01 21:48:53,817 Epoch   2 Step:     1300 Batch Loss:     0.225755 Tokens per Sec:     1482, Lr: 0.001000
2020-04-01 21:48:56,111 Epoch   2 Step:     1400 Batch Loss:     0.171740 Tokens per Sec:     1749, Lr: 0.001000
2020-04-01 21:48:58,260 Epoch   2 Step:     1500 Batch Loss:     0.192410 Tokens per Sec:     1850, Lr: 0.001000
2020-04-01 21:49:01,134 Epoch   2 Step:     1600 Batch Loss:     0.378058 Tokens per Sec:     1408, Lr: 0.001000
2020-04-01 21:49:05,011 Epoch   2 Step:     1700 Batch Loss:     0.117385 Tokens per Sec:     1045, Lr: 0.001000
2020-04-01 21:49:07,381 Epoch   2 Step:     1800 Batch Loss:     0.070813 Tokens per Sec:     1668, Lr: 0.001000
2020-04-01 21:49:10,177 Epoch   2 Step:     1900 Batch Loss:     0.100804 Tokens per Sec:     1416, Lr: 0.001000
2020-04-01 21:49:13,267 Epoch   2 Step:     2000 Batch Loss:     0.046239 Tokens per Sec:     1289, Lr: 0.001000
2020-04-01 21:49:13,819 Hooray! New best validation result [eval_metric]!
2020-04-01 21:49:13,820 Saving new checkpoint.
2020-04-01 21:49:13,827 Example #0
2020-04-01 21:49:13,827 	Raw source:     ['5']
2020-04-01 21:49:13,827 	Raw hypothesis: ['5']
2020-04-01 21:49:13,827 	Source:     5
2020-04-01 21:49:13,827 	Reference:  5
2020-04-01 21:49:13,828 	Hypothesis: 5
2020-04-01 21:49:13,828 Example #3
2020-04-01 21:49:13,828 	Raw source:     ['9', '5', '2', '8', '9']
2020-04-01 21:49:13,828 	Raw hypothesis: ['9', '8', '2', '5', '9']
2020-04-01 21:49:13,828 	Source:     9 5 2 8 9
2020-04-01 21:49:13,828 	Reference:  9 8 2 5 9
2020-04-01 21:49:13,829 	Hypothesis: 9 8 2 5 9
2020-04-01 21:49:13,829 Example #6
2020-04-01 21:49:13,829 	Raw source:     ['8', '3', '4', '5', '8', '5']
2020-04-01 21:49:13,829 	Raw hypothesis: ['5', '8', '5', '4', '3']
2020-04-01 21:49:13,829 	Source:     8 3 4 5 8 5
2020-04-01 21:49:13,829 	Reference:  5 8 5 4 3 8
2020-04-01 21:49:13,830 	Hypothesis: 5 8 5 4 3
2020-04-01 21:49:13,830 Validation result (greedy) at epoch   2, step     2000: bleu:  86.47, loss: 276.8859, ppl:   1.3556, duration: 0.5616s
2020-04-01 21:49:14,728 Epoch   2: total training loss 437.83
2020-04-01 21:49:14,728 Training ended after   2 epochs.
2020-04-01 21:49:14,728 Best validation result (greedy) at step     2000:  86.47 eval_metric.
2020-04-01 21:49:15,122  dev bleu:  86.47 [Greedy decoding]
2020-04-01 21:49:15,123 Translations saved to: reverse_model/00002000.hyps.dev
2020-04-01 21:49:15,354 test bleu:   0.00 [Greedy decoding]
2020-04-01 21:49:15,354 Translations saved to: reverse_model/00002000.hyps.test
