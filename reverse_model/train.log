2020-03-15 16:29:31,677 Hello! This is Joey-NMT.
2020-03-15 16:29:31,679 Total params: 105088
2020-03-15 16:29:31,680 Trainable parameters: ['decoder.att_vector_layer.bias', 'decoder.att_vector_layer.weight', 'decoder.attention.key_layer.weight', 'decoder.output_layer.weight', 'decoder.rnn.bias_hh_l0', 'decoder.rnn.bias_ih_l0', 'decoder.rnn.weight_hh_l0', 'decoder.rnn.weight_ih_l0', 'encoder.rnn.bias_hh_l0', 'encoder.rnn.bias_hh_l0_reverse', 'encoder.rnn.bias_ih_l0', 'encoder.rnn.bias_ih_l0_reverse', 'encoder.rnn.weight_hh_l0', 'encoder.rnn.weight_hh_l0_reverse', 'encoder.rnn.weight_ih_l0', 'encoder.rnn.weight_ih_l0_reverse', 'src_embed.lut.weight', 'trg_embed.lut.weight']
2020-03-15 16:29:31,680 cfg.name                           : reverse_experiment
2020-03-15 16:29:31,681 cfg.data.src                       : src
2020-03-15 16:29:31,681 cfg.data.trg                       : trg
2020-03-15 16:29:31,681 cfg.data.train                     : test/data/reverse/train
2020-03-15 16:29:31,681 cfg.data.dev                       : test/data/reverse/dev
2020-03-15 16:29:31,681 cfg.data.test                      : test/data/reverse/test
2020-03-15 16:29:31,681 cfg.data.level                     : word
2020-03-15 16:29:31,681 cfg.data.lowercase                 : False
2020-03-15 16:29:31,681 cfg.data.max_sent_length           : 25
2020-03-15 16:29:31,682 cfg.data.src_voc_min_freq          : 0
2020-03-15 16:29:31,682 cfg.data.src_voc_limit             : 100
2020-03-15 16:29:31,682 cfg.data.trg_voc_min_freq          : 0
2020-03-15 16:29:31,682 cfg.data.trg_voc_limit             : 100
2020-03-15 16:29:31,682 cfg.testing.beam_size              : 1
2020-03-15 16:29:31,682 cfg.testing.alpha                  : 1.0
2020-03-15 16:29:31,682 cfg.training.random_seed           : 42
2020-03-15 16:29:31,682 cfg.training.optimizer             : adam
2020-03-15 16:29:31,682 cfg.training.learning_rate         : 0.001
2020-03-15 16:29:31,682 cfg.training.learning_rate_min     : 0.0002
2020-03-15 16:29:31,683 cfg.training.weight_decay          : 0.0
2020-03-15 16:29:31,683 cfg.training.clip_grad_norm        : 1.0
2020-03-15 16:29:31,683 cfg.training.batch_size            : 10
2020-03-15 16:29:31,683 cfg.training.batch_type            : sentence
2020-03-15 16:29:31,683 cfg.training.scheduling            : plateau
2020-03-15 16:29:31,685 cfg.training.patience              : 5
2020-03-15 16:29:31,685 cfg.training.decrease_factor       : 0.5
2020-03-15 16:29:31,685 cfg.training.early_stopping_metric : eval_metric
2020-03-15 16:29:31,685 cfg.training.epochs                : 1
2020-03-15 16:29:31,685 cfg.training.validation_freq       : 1000
2020-03-15 16:29:31,685 cfg.training.logging_freq          : 100
2020-03-15 16:29:31,685 cfg.training.eval_metric           : bleu
2020-03-15 16:29:31,685 cfg.training.model_dir             : reverse_model
2020-03-15 16:29:31,685 cfg.training.overwrite             : True
2020-03-15 16:29:31,685 cfg.training.shuffle               : True
2020-03-15 16:29:31,686 cfg.training.use_cuda              : False
2020-03-15 16:29:31,686 cfg.training.max_output_length     : 30
2020-03-15 16:29:31,686 cfg.training.print_valid_sents     : [0, 3, 6]
2020-03-15 16:29:31,686 cfg.training.keep_last_ckpts       : 2
2020-03-15 16:29:31,686 cfg.model.initializer              : xavier
2020-03-15 16:29:31,686 cfg.model.embed_initializer        : normal
2020-03-15 16:29:31,686 cfg.model.embed_init_weight        : 0.1
2020-03-15 16:29:31,686 cfg.model.bias_initializer         : zeros
2020-03-15 16:29:31,687 cfg.model.init_rnn_orthogonal      : False
2020-03-15 16:29:31,687 cfg.model.lstm_forget_gate         : 0.0
2020-03-15 16:29:31,687 cfg.model.encoder.rnn_type         : lstm
2020-03-15 16:29:31,687 cfg.model.encoder.embeddings.embedding_dim : 16
2020-03-15 16:29:31,687 cfg.model.encoder.embeddings.scale : False
2020-03-15 16:29:31,687 cfg.model.encoder.hidden_size      : 64
2020-03-15 16:29:31,687 cfg.model.encoder.bidirectional    : True
2020-03-15 16:29:31,687 cfg.model.encoder.dropout          : 0.1
2020-03-15 16:29:31,687 cfg.model.encoder.num_layers       : 1
2020-03-15 16:29:31,687 cfg.model.decoder.rnn_type         : lstm
2020-03-15 16:29:31,688 cfg.model.decoder.embeddings.embedding_dim : 16
2020-03-15 16:29:31,688 cfg.model.decoder.embeddings.scale : False
2020-03-15 16:29:31,688 cfg.model.decoder.hidden_size      : 64
2020-03-15 16:29:31,688 cfg.model.decoder.dropout          : 0.1
2020-03-15 16:29:31,688 cfg.model.decoder.hidden_dropout   : 0.1
2020-03-15 16:29:31,688 cfg.model.decoder.num_layers       : 1
2020-03-15 16:29:31,688 cfg.model.decoder.input_feeding    : True
2020-03-15 16:29:31,688 cfg.model.decoder.init_hidden      : zero
2020-03-15 16:29:31,688 cfg.model.decoder.attention        : luong
2020-03-15 16:29:31,689 Data set sizes: 
	train 50000,
	valid 1000,
	test 1000
2020-03-15 16:29:31,689 First training example:
	[SRC] 28 14 42 7 20 38 18
	[TRG] 18 38 20 7 42 14 28
2020-03-15 16:29:31,689 First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) 15 (5) 35 (6) 44 (7) 18 (8) 36 (9) 16
2020-03-15 16:29:31,689 First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) 15 (5) 35 (6) 44 (7) 18 (8) 36 (9) 16
2020-03-15 16:29:31,689 Number of Src words (types): 54
2020-03-15 16:29:31,689 Number of Trg words (types): 54
2020-03-15 16:29:31,693 Model(
	encoder=RecurrentEncoder(LSTM(16, 64, batch_first=True, bidirectional=True)),
	decoder=RecurrentDecoder(rnn=LSTM(80, 64, batch_first=True), attention=LuongAttention),
	src_embed=Embeddings(embedding_dim=16, vocab_size=54),
	trg_embed=Embeddings(embedding_dim=16, vocab_size=54))
2020-03-15 16:29:31,693 EPOCH 1
2020-03-15 16:29:41,855 Epoch   1 Step:      100 Batch Loss:    57.867027 Tokens per Sec:     1340, Lr: 0.001000
2020-03-15 16:29:54,668 Epoch   1 Step:      200 Batch Loss:     8.945074 Tokens per Sec:     1104, Lr: 0.001000
2020-03-15 16:30:04,717 Epoch   1 Step:      300 Batch Loss:    45.075897 Tokens per Sec:     1387, Lr: 0.001000
2020-03-15 16:30:15,094 Epoch   1 Step:      400 Batch Loss:    71.815414 Tokens per Sec:     1371, Lr: 0.001000
2020-03-15 16:30:24,252 Epoch   1 Step:      500 Batch Loss:    37.846401 Tokens per Sec:     1516, Lr: 0.001000
2020-03-15 16:30:33,491 Epoch   1 Step:      600 Batch Loss:    39.131237 Tokens per Sec:     1522, Lr: 0.001000
2020-03-15 16:30:42,628 Epoch   1 Step:      700 Batch Loss:     7.699686 Tokens per Sec:     1517, Lr: 0.001000
2020-03-15 16:30:52,109 Epoch   1 Step:      800 Batch Loss:    62.072182 Tokens per Sec:     1459, Lr: 0.001000
2020-03-15 16:31:01,638 Epoch   1 Step:      900 Batch Loss:    74.379051 Tokens per Sec:     1473, Lr: 0.001000
2020-03-15 16:31:10,855 Epoch   1 Step:     1000 Batch Loss:    72.077957 Tokens per Sec:     1514, Lr: 0.001000
2020-03-15 16:31:19,318 Hooray! New best validation result [eval_metric]!
2020-03-15 16:31:19,319 Saving new checkpoint.
2020-03-15 16:31:19,328 Example #0
2020-03-15 16:31:19,329 	Raw source:     ['33', '9', '15', '3', '14', '33', '32', '42', '23', '12', '14', '17', '4', '35', '0', '48', '46', '36', '46', '27', '2', '34', '35', '17', '36', '39', '7', '14', '9', '0']
2020-03-15 16:31:19,329 	Raw hypothesis: ['0', '9', '14', '33', '39', '36', '17', '35', '2', '14', '46', '22', '46', '12', '31', '0', '42', '42', '42', '42', '42', '42', '42', '42', '42', '42', '16', '16', '16', '16']
2020-03-15 16:31:19,329 	Source:     33 9 15 3 14 33 32 42 23 12 14 17 4 35 0 48 46 36 46 27 2 34 35 17 36 39 7 14 9 0
2020-03-15 16:31:19,329 	Reference:  0 9 14 7 39 36 17 35 34 2 27 46 36 46 48 0 35 4 17 14 12 23 42 32 33 14 3 15 9 33
2020-03-15 16:31:19,329 	Hypothesis: 0 9 14 33 39 36 17 35 2 14 46 22 46 12 31 0 42 42 42 42 42 42 42 42 42 42 16 16 16 16
2020-03-15 16:31:19,329 Example #3
2020-03-15 16:31:19,329 	Raw source:     ['10', '43', '37', '32', '6', '9', '25', '36', '21', '29', '16', '7', '18', '27', '30', '46', '37', '15', '7', '48', '18']
2020-03-15 16:31:19,329 	Raw hypothesis: ['18', '48', '33', '15', '37', '46', '27', '46', '18', '16', '37', '21', '12', '12', '41', '41', '43', '43']
2020-03-15 16:31:19,329 	Source:     10 43 37 32 6 9 25 36 21 29 16 7 18 27 30 46 37 15 7 48 18
2020-03-15 16:31:19,329 	Reference:  18 48 7 15 37 46 30 27 18 7 16 29 21 36 25 9 6 32 37 43 10
2020-03-15 16:31:19,330 	Hypothesis: 18 48 33 15 37 46 27 46 18 16 37 21 12 12 41 41 43 43
2020-03-15 16:31:19,330 Example #6
2020-03-15 16:31:19,330 	Raw source:     ['0', '38', '14', '26', '20', '34', '10', '36', '11', '32', '29', '21']
2020-03-15 16:31:19,330 	Raw hypothesis: ['21', '29', '32', '11', '36', '10', '34', '20', '14', '14', '40']
2020-03-15 16:31:19,330 	Source:     0 38 14 26 20 34 10 36 11 32 29 21
2020-03-15 16:31:19,330 	Reference:  21 29 32 11 36 10 34 20 26 14 38 0
2020-03-15 16:31:19,330 	Hypothesis: 21 29 32 11 36 10 34 20 14 14 40
2020-03-15 16:31:19,330 Validation result (greedy) at epoch   1, step     1000: bleu:  37.25, loss: 35894.4297, ppl:   9.0185, duration: 8.4746s
2020-03-15 16:31:27,160 Epoch   1 Step:     1100 Batch Loss:    40.956055 Tokens per Sec:     1762, Lr: 0.001000
2020-03-15 16:31:33,357 Epoch   1 Step:     1200 Batch Loss:    13.779056 Tokens per Sec:     2278, Lr: 0.001000
2020-03-15 16:31:41,354 Epoch   1 Step:     1300 Batch Loss:     2.815615 Tokens per Sec:     1766, Lr: 0.001000
2020-03-15 16:31:51,096 Epoch   1 Step:     1400 Batch Loss:     1.430610 Tokens per Sec:     1429, Lr: 0.001000
2020-03-15 16:32:01,009 Epoch   1 Step:     1500 Batch Loss:     2.812285 Tokens per Sec:     1423, Lr: 0.001000
2020-03-15 16:32:15,335 Epoch   1 Step:     1600 Batch Loss:     0.537401 Tokens per Sec:      984, Lr: 0.001000
2020-03-15 16:32:31,431 Epoch   1 Step:     1700 Batch Loss:    40.053127 Tokens per Sec:      846, Lr: 0.001000
2020-03-15 16:32:42,039 Epoch   1 Step:     1800 Batch Loss:    21.240047 Tokens per Sec:     1314, Lr: 0.001000
2020-03-15 16:32:48,534 Epoch   1 Step:     1900 Batch Loss:    24.066376 Tokens per Sec:     2153, Lr: 0.001000
2020-03-15 16:32:54,498 Epoch   1 Step:     2000 Batch Loss:     2.457243 Tokens per Sec:     2403, Lr: 0.001000
2020-03-15 16:33:01,386 Hooray! New best validation result [eval_metric]!
2020-03-15 16:33:01,386 Saving new checkpoint.
2020-03-15 16:33:01,390 Example #0
2020-03-15 16:33:01,390 	Raw source:     ['33', '9', '15', '3', '14', '33', '32', '42', '23', '12', '14', '17', '4', '35', '0', '48', '46', '36', '46', '27', '2', '34', '35', '17', '36', '39', '7', '14', '9', '0']
2020-03-15 16:33:01,390 	Raw hypothesis: ['0', '9', '14', '7', '39', '36', '17', '35', '34', '2', '27', '32', '36', '46', '9', '0', '4', '4', '16', '16', '16', '16', '16', '16', '16', '16', '16', '16', '16', '33']
2020-03-15 16:33:01,390 	Source:     33 9 15 3 14 33 32 42 23 12 14 17 4 35 0 48 46 36 46 27 2 34 35 17 36 39 7 14 9 0
2020-03-15 16:33:01,390 	Reference:  0 9 14 7 39 36 17 35 34 2 27 46 36 46 48 0 35 4 17 14 12 23 42 32 33 14 3 15 9 33
2020-03-15 16:33:01,390 	Hypothesis: 0 9 14 7 39 36 17 35 34 2 27 32 36 46 9 0 4 4 16 16 16 16 16 16 16 16 16 16 16 33
2020-03-15 16:33:01,390 Example #3
2020-03-15 16:33:01,391 	Raw source:     ['10', '43', '37', '32', '6', '9', '25', '36', '21', '29', '16', '7', '18', '27', '30', '46', '37', '15', '7', '48', '18']
2020-03-15 16:33:01,391 	Raw hypothesis: ['18', '48', '7', '15', '37', '46', '30', '27', '18', '7', '16', '29', '21', '36', '25', '33', '33', '24', '43', '43']
2020-03-15 16:33:01,391 	Source:     10 43 37 32 6 9 25 36 21 29 16 7 18 27 30 46 37 15 7 48 18
2020-03-15 16:33:01,391 	Reference:  18 48 7 15 37 46 30 27 18 7 16 29 21 36 25 9 6 32 37 43 10
2020-03-15 16:33:01,391 	Hypothesis: 18 48 7 15 37 46 30 27 18 7 16 29 21 36 25 33 33 24 43 43
2020-03-15 16:33:01,391 Example #6
2020-03-15 16:33:01,391 	Raw source:     ['0', '38', '14', '26', '20', '34', '10', '36', '11', '32', '29', '21']
2020-03-15 16:33:01,391 	Raw hypothesis: ['21', '29', '32', '11', '36', '10', '34', '20', '26', '14', '38', '0']
2020-03-15 16:33:01,391 	Source:     0 38 14 26 20 34 10 36 11 32 29 21
2020-03-15 16:33:01,391 	Reference:  21 29 32 11 36 10 34 20 26 14 38 0
2020-03-15 16:33:01,391 	Hypothesis: 21 29 32 11 36 10 34 20 26 14 38 0
2020-03-15 16:33:01,391 Validation result (greedy) at epoch   1, step     2000: bleu:  78.96, loss: 11028.5391, ppl:   1.9655, duration: 6.8926s
2020-03-15 16:33:12,634 Epoch   1 Step:     2100 Batch Loss:     0.563849 Tokens per Sec:     1243, Lr: 0.001000
2020-03-15 16:33:25,472 Epoch   1 Step:     2200 Batch Loss:     5.442645 Tokens per Sec:     1081, Lr: 0.001000
2020-03-15 16:33:38,776 Epoch   1 Step:     2300 Batch Loss:     0.889978 Tokens per Sec:     1057, Lr: 0.001000
2020-03-15 16:33:47,386 Epoch   1 Step:     2400 Batch Loss:     1.056945 Tokens per Sec:     1660, Lr: 0.001000
2020-03-15 16:33:57,387 Epoch   1 Step:     2500 Batch Loss:     0.458571 Tokens per Sec:     1385, Lr: 0.001000
2020-03-15 16:34:07,884 Epoch   1 Step:     2600 Batch Loss:    21.243124 Tokens per Sec:     1327, Lr: 0.001000
2020-03-15 16:34:17,861 Epoch   1 Step:     2700 Batch Loss:     0.070279 Tokens per Sec:     1369, Lr: 0.001000
2020-03-15 16:34:29,496 Epoch   1 Step:     2800 Batch Loss:     0.016876 Tokens per Sec:     1203, Lr: 0.001000
2020-03-15 16:34:36,349 Epoch   1 Step:     2900 Batch Loss:    59.008949 Tokens per Sec:     2052, Lr: 0.001000
2020-03-15 16:34:46,325 Epoch   1 Step:     3000 Batch Loss:     4.965327 Tokens per Sec:     1413, Lr: 0.001000
2020-03-15 16:34:53,962 Hooray! New best validation result [eval_metric]!
2020-03-15 16:34:53,963 Saving new checkpoint.
2020-03-15 16:34:53,971 Example #0
2020-03-15 16:34:53,971 	Raw source:     ['33', '9', '15', '3', '14', '33', '32', '42', '23', '12', '14', '17', '4', '35', '0', '48', '46', '36', '46', '27', '2', '34', '35', '17', '36', '39', '7', '14', '9', '0']
2020-03-15 16:34:53,971 	Raw hypothesis: ['0', '9', '14', '7', '39', '36', '17', '35', '34', '2', '27', '46', '36', '46', '48', '0', '35', '17', '14', '12', '23', '23', '32', '32', '32', '33', '33', '33']
2020-03-15 16:34:53,971 	Source:     33 9 15 3 14 33 32 42 23 12 14 17 4 35 0 48 46 36 46 27 2 34 35 17 36 39 7 14 9 0
2020-03-15 16:34:53,971 	Reference:  0 9 14 7 39 36 17 35 34 2 27 46 36 46 48 0 35 4 17 14 12 23 42 32 33 14 3 15 9 33
2020-03-15 16:34:53,971 	Hypothesis: 0 9 14 7 39 36 17 35 34 2 27 46 36 46 48 0 35 17 14 12 23 23 32 32 32 33 33 33
2020-03-15 16:34:53,971 Example #3
2020-03-15 16:34:53,972 	Raw source:     ['10', '43', '37', '32', '6', '9', '25', '36', '21', '29', '16', '7', '18', '27', '30', '46', '37', '15', '7', '48', '18']
2020-03-15 16:34:53,972 	Raw hypothesis: ['18', '48', '7', '15', '37', '46', '30', '27', '18', '7', '16', '29', '21', '36', '25', '9', '6', '32', '37', '43', '10']
2020-03-15 16:34:53,972 	Source:     10 43 37 32 6 9 25 36 21 29 16 7 18 27 30 46 37 15 7 48 18
2020-03-15 16:34:53,972 	Reference:  18 48 7 15 37 46 30 27 18 7 16 29 21 36 25 9 6 32 37 43 10
2020-03-15 16:34:53,972 	Hypothesis: 18 48 7 15 37 46 30 27 18 7 16 29 21 36 25 9 6 32 37 43 10
2020-03-15 16:34:53,972 Example #6
2020-03-15 16:34:53,972 	Raw source:     ['0', '38', '14', '26', '20', '34', '10', '36', '11', '32', '29', '21']
2020-03-15 16:34:53,972 	Raw hypothesis: ['21', '29', '32', '11', '36', '10', '34', '20', '26', '14', '38', '0']
2020-03-15 16:34:53,972 	Source:     0 38 14 26 20 34 10 36 11 32 29 21
2020-03-15 16:34:53,972 	Reference:  21 29 32 11 36 10 34 20 26 14 38 0
2020-03-15 16:34:53,972 	Hypothesis: 21 29 32 11 36 10 34 20 26 14 38 0
2020-03-15 16:34:53,972 Validation result (greedy) at epoch   1, step     3000: bleu:  88.85, loss: 6404.2104, ppl:   1.4805, duration: 7.6466s
2020-03-15 16:35:08,977 Epoch   1 Step:     3100 Batch Loss:     4.485588 Tokens per Sec:      923, Lr: 0.001000
2020-03-15 16:35:22,272 Epoch   1 Step:     3200 Batch Loss:     0.300644 Tokens per Sec:     1027, Lr: 0.001000
2020-03-15 16:35:32,680 Epoch   1 Step:     3300 Batch Loss:     0.112229 Tokens per Sec:     1325, Lr: 0.001000
2020-03-15 16:35:43,217 Epoch   1 Step:     3400 Batch Loss:     0.201277 Tokens per Sec:     1333, Lr: 0.001000
2020-03-15 16:35:51,801 Epoch   1 Step:     3500 Batch Loss:     5.085721 Tokens per Sec:     1664, Lr: 0.001000
2020-03-15 16:35:59,335 Epoch   1 Step:     3600 Batch Loss:     0.221661 Tokens per Sec:     1863, Lr: 0.001000
2020-03-15 16:36:11,320 Epoch   1 Step:     3700 Batch Loss:     0.015533 Tokens per Sec:     1186, Lr: 0.001000
2020-03-15 16:36:25,940 Epoch   1 Step:     3800 Batch Loss:     0.196154 Tokens per Sec:      946, Lr: 0.001000
2020-03-15 16:36:36,725 Epoch   1 Step:     3900 Batch Loss:     0.014022 Tokens per Sec:     1303, Lr: 0.001000
2020-03-15 16:36:43,420 Epoch   1 Step:     4000 Batch Loss:     0.027793 Tokens per Sec:     2101, Lr: 0.001000
2020-03-15 16:36:53,236 Hooray! New best validation result [eval_metric]!
2020-03-15 16:36:53,237 Saving new checkpoint.
2020-03-15 16:36:53,245 Example #0
2020-03-15 16:36:53,245 	Raw source:     ['33', '9', '15', '3', '14', '33', '32', '42', '23', '12', '14', '17', '4', '35', '0', '48', '46', '36', '46', '27', '2', '34', '35', '17', '36', '39', '7', '14', '9', '0']
2020-03-15 16:36:53,245 	Raw hypothesis: ['0', '9', '14', '7', '39', '36', '17', '35', '34', '2', '27', '46', '36', '46', '48', '0', '35', '4', '17', '14', '12', '23', '42', '33', '14', '0', '35', '4', '17', '17']
2020-03-15 16:36:53,245 	Source:     33 9 15 3 14 33 32 42 23 12 14 17 4 35 0 48 46 36 46 27 2 34 35 17 36 39 7 14 9 0
2020-03-15 16:36:53,245 	Reference:  0 9 14 7 39 36 17 35 34 2 27 46 36 46 48 0 35 4 17 14 12 23 42 32 33 14 3 15 9 33
2020-03-15 16:36:53,245 	Hypothesis: 0 9 14 7 39 36 17 35 34 2 27 46 36 46 48 0 35 4 17 14 12 23 42 33 14 0 35 4 17 17
2020-03-15 16:36:53,246 Example #3
2020-03-15 16:36:53,246 	Raw source:     ['10', '43', '37', '32', '6', '9', '25', '36', '21', '29', '16', '7', '18', '27', '30', '46', '37', '15', '7', '48', '18']
2020-03-15 16:36:53,246 	Raw hypothesis: ['18', '48', '7', '15', '37', '46', '30', '27', '18', '7', '16', '29', '21', '36', '25', '9', '6', '32', '37', '43', '10']
2020-03-15 16:36:53,246 	Source:     10 43 37 32 6 9 25 36 21 29 16 7 18 27 30 46 37 15 7 48 18
2020-03-15 16:36:53,246 	Reference:  18 48 7 15 37 46 30 27 18 7 16 29 21 36 25 9 6 32 37 43 10
2020-03-15 16:36:53,246 	Hypothesis: 18 48 7 15 37 46 30 27 18 7 16 29 21 36 25 9 6 32 37 43 10
2020-03-15 16:36:53,246 Example #6
2020-03-15 16:36:53,246 	Raw source:     ['0', '38', '14', '26', '20', '34', '10', '36', '11', '32', '29', '21']
2020-03-15 16:36:53,246 	Raw hypothesis: ['21', '29', '32', '11', '36', '10', '34', '20', '26', '14', '38', '0']
2020-03-15 16:36:53,246 	Source:     0 38 14 26 20 34 10 36 11 32 29 21
2020-03-15 16:36:53,247 	Reference:  21 29 32 11 36 10 34 20 26 14 38 0
2020-03-15 16:36:53,247 	Hypothesis: 21 29 32 11 36 10 34 20 26 14 38 0
2020-03-15 16:36:53,247 Validation result (greedy) at epoch   1, step     4000: bleu:  93.29, loss: 8903.9180, ppl:   1.7256, duration: 9.8256s
2020-03-15 16:37:10,645 Epoch   1 Step:     4100 Batch Loss:     0.044206 Tokens per Sec:      803, Lr: 0.001000
2020-03-15 16:37:17,398 Epoch   1 Step:     4200 Batch Loss:     1.295906 Tokens per Sec:     2025, Lr: 0.001000
2020-03-15 16:37:24,917 Epoch   1 Step:     4300 Batch Loss:     1.559860 Tokens per Sec:     1876, Lr: 0.001000
2020-03-15 16:37:32,287 Epoch   1 Step:     4400 Batch Loss:     0.005814 Tokens per Sec:     1818, Lr: 0.001000
2020-03-15 16:37:49,235 Epoch   1 Step:     4500 Batch Loss:     9.597692 Tokens per Sec:      823, Lr: 0.001000
2020-03-15 16:38:00,388 Epoch   1 Step:     4600 Batch Loss:    52.573753 Tokens per Sec:     1223, Lr: 0.001000
2020-03-15 16:38:12,287 Epoch   1 Step:     4700 Batch Loss:     7.416531 Tokens per Sec:     1171, Lr: 0.001000
2020-03-15 16:38:19,607 Epoch   1 Step:     4800 Batch Loss:     0.080555 Tokens per Sec:     1912, Lr: 0.001000
2020-03-15 16:38:26,580 Epoch   1 Step:     4900 Batch Loss:     3.326204 Tokens per Sec:     2049, Lr: 0.001000
2020-03-15 16:38:37,441 Epoch   1 Step:     5000 Batch Loss:    18.627117 Tokens per Sec:     1298, Lr: 0.001000
2020-03-15 16:38:47,263 Example #0
2020-03-15 16:38:47,263 	Raw source:     ['33', '9', '15', '3', '14', '33', '32', '42', '23', '12', '14', '17', '4', '35', '0', '48', '46', '36', '46', '27', '2', '34', '35', '17', '36', '39', '7', '14', '9', '0']
2020-03-15 16:38:47,263 	Raw hypothesis: ['0', '9', '14', '7', '39', '36', '17', '35', '34', '2', '27', '46', '36', '46', '48', '0', '4', '4', '14', '12', '23', '42', '42', '32', '14', '0', '35', '9', '17', '14']
2020-03-15 16:38:47,263 	Source:     33 9 15 3 14 33 32 42 23 12 14 17 4 35 0 48 46 36 46 27 2 34 35 17 36 39 7 14 9 0
2020-03-15 16:38:47,263 	Reference:  0 9 14 7 39 36 17 35 34 2 27 46 36 46 48 0 35 4 17 14 12 23 42 32 33 14 3 15 9 33
2020-03-15 16:38:47,263 	Hypothesis: 0 9 14 7 39 36 17 35 34 2 27 46 36 46 48 0 4 4 14 12 23 42 42 32 14 0 35 9 17 14
2020-03-15 16:38:47,264 Example #3
2020-03-15 16:38:47,264 	Raw source:     ['10', '43', '37', '32', '6', '9', '25', '36', '21', '29', '16', '7', '18', '27', '30', '46', '37', '15', '7', '48', '18']
2020-03-15 16:38:47,264 	Raw hypothesis: ['18', '48', '7', '15', '37', '46', '30', '27', '18', '7', '29', '21', '36', '25', '9', '6', '32', '37', '43', '10']
2020-03-15 16:38:47,264 	Source:     10 43 37 32 6 9 25 36 21 29 16 7 18 27 30 46 37 15 7 48 18
2020-03-15 16:38:47,264 	Reference:  18 48 7 15 37 46 30 27 18 7 16 29 21 36 25 9 6 32 37 43 10
2020-03-15 16:38:47,264 	Hypothesis: 18 48 7 15 37 46 30 27 18 7 29 21 36 25 9 6 32 37 43 10
2020-03-15 16:38:47,264 Example #6
2020-03-15 16:38:47,264 	Raw source:     ['0', '38', '14', '26', '20', '34', '10', '36', '11', '32', '29', '21']
2020-03-15 16:38:47,264 	Raw hypothesis: ['21', '29', '32', '11', '36', '10', '34', '20', '26', '38', '0']
2020-03-15 16:38:47,264 	Source:     0 38 14 26 20 34 10 36 11 32 29 21
2020-03-15 16:38:47,265 	Reference:  21 29 32 11 36 10 34 20 26 14 38 0
2020-03-15 16:38:47,265 	Hypothesis: 21 29 32 11 36 10 34 20 26 38 0
2020-03-15 16:38:47,265 Validation result (greedy) at epoch   1, step     5000: bleu:  84.31, loss: 75163.6250, ppl: 100.0162, duration: 9.8229s
2020-03-15 16:38:49,390 Epoch   1: total training loss 86796.61
2020-03-15 16:38:49,390 Training ended after   1 epochs.
2020-03-15 16:38:49,391 Best validation result (greedy) at step     4000:  93.29 eval_metric.
2020-03-15 16:38:55,622  dev bleu:  93.29 [Greedy decoding]
2020-03-15 16:38:55,623 Translations saved to: reverse_model/00004000.hyps.dev
2020-03-15 16:39:01,336 test bleu:  93.55 [Greedy decoding]
2020-03-15 16:39:01,337 Translations saved to: reverse_model/00004000.hyps.test
