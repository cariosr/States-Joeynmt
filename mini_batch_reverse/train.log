2020-04-29 22:07:16,248 Hello! This is Joey-NMT.
2020-04-29 22:07:16,253 Total params: 18984
2020-04-29 22:07:16,253 Trainable parameters: ['decoder.att_vector_layer.bias', 'decoder.att_vector_layer.weight', 'decoder.attention.key_layer.weight', 'decoder.bridge_layer.bias', 'decoder.bridge_layer.weight', 'decoder.output_layer.weight', 'decoder.rnn.bias_hh_l0', 'decoder.rnn.bias_ih_l0', 'decoder.rnn.weight_hh_l0', 'decoder.rnn.weight_ih_l0', 'encoder.rnn.bias_hh_l0', 'encoder.rnn.bias_hh_l0_reverse', 'encoder.rnn.bias_ih_l0', 'encoder.rnn.bias_ih_l0_reverse', 'encoder.rnn.weight_hh_l0', 'encoder.rnn.weight_hh_l0_reverse', 'encoder.rnn.weight_ih_l0', 'encoder.rnn.weight_ih_l0_reverse', 'src_embed.lut.weight', 'trg_embed.lut.weight']
2020-04-29 22:07:16,254 cfg.name                           : mini_batch_reverse
2020-04-29 22:07:16,255 cfg.data.src                       : src
2020-04-29 22:07:16,255 cfg.data.trg                       : trg
2020-04-29 22:07:16,255 cfg.data.train                     : test/data/mini_batch_reverse/train
2020-04-29 22:07:16,255 cfg.data.dev                       : test/data/mini_batch_reverse/dev
2020-04-29 22:07:16,255 cfg.data.test                      : test/data/mini_batch_reverse/test
2020-04-29 22:07:16,255 cfg.data.level                     : word
2020-04-29 22:07:16,255 cfg.data.lowercase                 : False
2020-04-29 22:07:16,255 cfg.data.max_sent_length           : 25
2020-04-29 22:07:16,256 cfg.data.src_voc_min_freq          : 0
2020-04-29 22:07:16,257 cfg.data.src_voc_limit             : 100
2020-04-29 22:07:16,257 cfg.data.trg_voc_min_freq          : 0
2020-04-29 22:07:16,257 cfg.data.trg_voc_limit             : 100
2020-04-29 22:07:16,257 cfg.testing.beam_size              : 1
2020-04-29 22:07:16,258 cfg.testing.alpha                  : 1.0
2020-04-29 22:07:16,258 cfg.training.random_seed           : 42
2020-04-29 22:07:16,258 cfg.training.optimizer             : adam
2020-04-29 22:07:16,258 cfg.training.learning_rate         : 0.001
2020-04-29 22:07:16,258 cfg.training.learning_rate_min     : 0.0002
2020-04-29 22:07:16,258 cfg.training.weight_decay          : 0.0
2020-04-29 22:07:16,258 cfg.training.clip_grad_norm        : 1.0
2020-04-29 22:07:16,258 cfg.training.batch_size            : 32
2020-04-29 22:07:16,258 cfg.training.batch_type            : sentence
2020-04-29 22:07:16,258 cfg.training.scheduling            : plateau
2020-04-29 22:07:16,258 cfg.training.patience              : 5
2020-04-29 22:07:16,258 cfg.training.decrease_factor       : 0.5
2020-04-29 22:07:16,258 cfg.training.early_stopping_metric : eval_metric
2020-04-29 22:07:16,259 cfg.training.epochs                : 100
2020-04-29 22:07:16,259 cfg.training.validation_freq       : 10
2020-04-29 22:07:16,259 cfg.training.logging_freq          : 10
2020-04-29 22:07:16,259 cfg.training.eval_metric           : bleu
2020-04-29 22:07:16,259 cfg.training.model_dir             : mini_batch_reverse
2020-04-29 22:07:16,259 cfg.training.overwrite             : True
2020-04-29 22:07:16,259 cfg.training.shuffle               : True
2020-04-29 22:07:16,259 cfg.training.use_cuda              : False
2020-04-29 22:07:16,259 cfg.training.max_output_length     : 10
2020-04-29 22:07:16,259 cfg.training.print_valid_sents     : [0, 3, 6]
2020-04-29 22:07:16,259 cfg.training.keep_last_ckpts       : 2
2020-04-29 22:07:16,259 cfg.model.initializer              : xavier
2020-04-29 22:07:16,259 cfg.model.embed_initializer        : normal
2020-04-29 22:07:16,259 cfg.model.embed_init_weight        : 0.1
2020-04-29 22:07:16,259 cfg.model.bias_initializer         : zeros
2020-04-29 22:07:16,260 cfg.model.init_rnn_orthogonal      : False
2020-04-29 22:07:16,260 cfg.model.lstm_forget_gate         : 0.0
2020-04-29 22:07:16,260 cfg.model.encoder.rnn_type         : lstm
2020-04-29 22:07:16,260 cfg.model.encoder.embeddings.embedding_dim : 16
2020-04-29 22:07:16,260 cfg.model.encoder.embeddings.scale : False
2020-04-29 22:07:16,260 cfg.model.encoder.hidden_size      : 24
2020-04-29 22:07:16,260 cfg.model.encoder.bidirectional    : True
2020-04-29 22:07:16,260 cfg.model.encoder.dropout          : 0.1
2020-04-29 22:07:16,260 cfg.model.encoder.num_layers       : 1
2020-04-29 22:07:16,261 cfg.model.decoder.rnn_type         : lstm
2020-04-29 22:07:16,261 cfg.model.decoder.embeddings.embedding_dim : 16
2020-04-29 22:07:16,261 cfg.model.decoder.embeddings.scale : False
2020-04-29 22:07:16,261 cfg.model.decoder.hidden_size      : 24
2020-04-29 22:07:16,261 cfg.model.decoder.dropout          : 0.1
2020-04-29 22:07:16,261 cfg.model.decoder.hidden_dropout   : 0.1
2020-04-29 22:07:16,262 cfg.model.decoder.num_layers       : 1
2020-04-29 22:07:16,262 cfg.model.decoder.input_feeding    : True
2020-04-29 22:07:16,262 cfg.model.decoder.init_hidden      : bridge
2020-04-29 22:07:16,262 cfg.model.decoder.attention        : luong
2020-04-29 22:07:16,262 cfg.dqn.epochs                     : 2000
2020-04-29 22:07:16,262 cfg.dqn.sample_size                : 64
2020-04-29 22:07:16,262 cfg.dqn.lr                         : 5e-05
2020-04-29 22:07:16,262 cfg.dqn.egreed_max                 : 0.9
2020-04-29 22:07:16,262 cfg.dqn.egreed_min                 : 0.001
2020-04-29 22:07:16,262 cfg.dqn.gamma_max                  : 0.9
2020-04-29 22:07:16,263 cfg.dqn.gamma_min                  : 0.3
2020-04-29 22:07:16,263 cfg.dqn.nu_iter                    : 50
2020-04-29 22:07:16,263 cfg.dqn.mem_cap                    : 500
2020-04-29 22:07:16,263 cfg.dqn.beam_min                   : 1
2020-04-29 22:07:16,263 cfg.dqn.beam_max                   : 50
2020-04-29 22:07:16,263 cfg.dqn.state_type                 : hidden
2020-04-29 22:07:16,263 cfg.dqn.reward_type                : bleu_batch
2020-04-29 22:07:16,264 cfg.dqn.nu_pretrain                : 0
2020-04-29 22:07:16,264 cfg.dqn.other_descrip              : _mini_batch_test_
2020-04-29 22:07:16,264 Data set sizes: 
	train 20,
	valid 20,
	test 20
2020-04-29 22:07:16,264 First training example:
	[SRC] 1 2 3 4 5
	[TRG] 5 4 3 2 1
2020-04-29 22:07:16,264 First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) 1 (5) 2 (6) 3 (7) 4 (8) 5
2020-04-29 22:07:16,264 First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) 1 (5) 2 (6) 3 (7) 4 (8) 5
2020-04-29 22:07:16,265 Number of Src words (types): 9
2020-04-29 22:07:16,265 Number of Trg words (types): 9
2020-04-29 22:07:16,265 Model(
	encoder=RecurrentEncoder(LSTM(16, 24, batch_first=True, bidirectional=True)),
	decoder=RecurrentDecoder(rnn=LSTM(40, 24, batch_first=True), attention=LuongAttention),
	src_embed=Embeddings(embedding_dim=16, vocab_size=9),
	trg_embed=Embeddings(embedding_dim=16, vocab_size=9))
2020-04-29 22:07:16,266 EPOCH 1
2020-04-29 22:07:16,485 Epoch   1: total training loss 13.23
2020-04-29 22:07:16,486 EPOCH 2
2020-04-29 22:07:16,765 Epoch   2: total training loss 13.17
2020-04-29 22:07:16,766 EPOCH 3
2020-04-29 22:07:16,827 Epoch   3: total training loss 13.12
2020-04-29 22:07:16,828 EPOCH 4
2020-04-29 22:07:16,947 Epoch   4: total training loss 13.08
2020-04-29 22:07:16,949 EPOCH 5
2020-04-29 22:07:17,025 Epoch   5: total training loss 13.04
2020-04-29 22:07:17,026 EPOCH 6
2020-04-29 22:07:17,121 Epoch   6: total training loss 12.97
2020-04-29 22:07:17,122 EPOCH 7
2020-04-29 22:07:17,217 Epoch   7: total training loss 12.95
2020-04-29 22:07:17,218 EPOCH 8
2020-04-29 22:07:17,313 Epoch   8: total training loss 12.89
2020-04-29 22:07:17,315 EPOCH 9
2020-04-29 22:07:17,440 Epoch   9: total training loss 12.84
2020-04-29 22:07:17,441 EPOCH 10
2020-04-29 22:07:17,584 Epoch  10 Step:       10 Batch Loss:    12.788071 Tokens per Sec:      838, Lr: 0.001000
2020-04-29 22:07:17,661 Hooray! New best validation result [eval_metric]!
2020-04-29 22:07:17,662 Saving new checkpoint.
2020-04-29 22:07:17,672 Example #0
2020-04-29 22:07:17,673 	Raw source:     ['1', '2', '3', '4', '5']
2020-04-29 22:07:17,673 	Raw hypothesis: ['2', '2', '2', '2', '2', '2', '2', '2', '2', '2']
2020-04-29 22:07:17,673 	Source:     1 2 3 4 5
2020-04-29 22:07:17,673 	Reference:  5 4 3 2 1
2020-04-29 22:07:17,674 	Hypothesis: 2 2 2 2 2 2 2 2 2 2
2020-04-29 22:07:17,674 Example #3
2020-04-29 22:07:17,674 	Raw source:     ['1', '2', '3', '4', '5']
2020-04-29 22:07:17,674 	Raw hypothesis: ['2', '2', '2', '2', '2', '2', '2', '2', '2', '2']
2020-04-29 22:07:17,674 	Source:     1 2 3 4 5
2020-04-29 22:07:17,675 	Reference:  5 4 3 2 1
2020-04-29 22:07:17,675 	Hypothesis: 2 2 2 2 2 2 2 2 2 2
2020-04-29 22:07:17,675 Example #6
2020-04-29 22:07:17,675 	Raw source:     ['1', '2', '3', '4', '5']
2020-04-29 22:07:17,675 	Raw hypothesis: ['2', '2', '2', '2', '2', '2', '2', '2', '2', '2']
2020-04-29 22:07:17,675 	Source:     1 2 3 4 5
2020-04-29 22:07:17,676 	Reference:  5 4 3 2 1
2020-04-29 22:07:17,676 	Hypothesis: 2 2 2 2 2 2 2 2 2 2
2020-04-29 22:07:17,676 Validation result (greedy) at epoch  10, step       10: bleu:   0.00, loss: 254.5641, ppl:   8.3425, duration: 0.0897s
2020-04-29 22:07:20,806 Epoch  10: total training loss 12.79
2020-04-29 22:07:20,806 EPOCH 11
2020-04-29 22:07:20,928 Epoch  11: total training loss 12.73
2020-04-29 22:07:20,928 EPOCH 12
2020-04-29 22:07:21,040 Epoch  12: total training loss 12.67
2020-04-29 22:07:21,042 EPOCH 13
2020-04-29 22:07:21,208 Epoch  13: total training loss 12.61
2020-04-29 22:07:21,209 EPOCH 14
2020-04-29 22:07:21,432 Epoch  14: total training loss 12.53
2020-04-29 22:07:21,433 EPOCH 15
2020-04-29 22:07:21,581 Epoch  15: total training loss 12.47
2020-04-29 22:07:21,582 EPOCH 16
2020-04-29 22:07:21,759 Epoch  16: total training loss 12.39
2020-04-29 22:07:21,762 EPOCH 17
2020-04-29 22:07:21,880 Epoch  17: total training loss 12.35
2020-04-29 22:07:21,881 EPOCH 18
2020-04-29 22:07:22,004 Epoch  18: total training loss 12.24
2020-04-29 22:07:22,005 EPOCH 19
2020-04-29 22:07:22,128 Epoch  19: total training loss 12.17
2020-04-29 22:07:22,128 EPOCH 20
2020-04-29 22:07:22,253 Epoch  20 Step:       20 Batch Loss:    12.074071 Tokens per Sec:      966, Lr: 0.001000
2020-04-29 22:07:22,402 Example #0
2020-04-29 22:07:22,403 	Raw source:     ['1', '2', '3', '4', '5']
2020-04-29 22:07:22,403 	Raw hypothesis: ['2', '2', '2', '3', '3', '2', '3', '3', '3', '2']
2020-04-29 22:07:22,403 	Source:     1 2 3 4 5
2020-04-29 22:07:22,403 	Reference:  5 4 3 2 1
2020-04-29 22:07:22,403 	Hypothesis: 2 2 2 3 3 2 3 3 3 2
2020-04-29 22:07:22,404 Example #3
2020-04-29 22:07:22,404 	Raw source:     ['1', '2', '3', '4', '5']
2020-04-29 22:07:22,404 	Raw hypothesis: ['2', '2', '2', '3', '3', '2', '3', '3', '3', '2']
2020-04-29 22:07:22,404 	Source:     1 2 3 4 5
2020-04-29 22:07:22,407 	Reference:  5 4 3 2 1
2020-04-29 22:07:22,407 	Hypothesis: 2 2 2 3 3 2 3 3 3 2
2020-04-29 22:07:22,407 Example #6
2020-04-29 22:07:22,407 	Raw source:     ['1', '2', '3', '4', '5']
2020-04-29 22:07:22,407 	Raw hypothesis: ['2', '2', '2', '3', '3', '2', '3', '3', '3', '2']
2020-04-29 22:07:22,407 	Source:     1 2 3 4 5
2020-04-29 22:07:22,407 	Reference:  5 4 3 2 1
2020-04-29 22:07:22,408 	Hypothesis: 2 2 2 3 3 2 3 3 3 2
2020-04-29 22:07:22,408 Validation result (greedy) at epoch  20, step       20: bleu:   0.00, loss: 239.3724, ppl:   7.3505, duration: 0.1541s
2020-04-29 22:07:25,742 Epoch  20: total training loss 12.07
2020-04-29 22:07:25,743 EPOCH 21
2020-04-29 22:07:26,005 Epoch  21: total training loss 11.99
2020-04-29 22:07:26,005 EPOCH 22
2020-04-29 22:07:26,107 Epoch  22: total training loss 11.87
2020-04-29 22:07:26,108 EPOCH 23
2020-04-29 22:07:26,267 Epoch  23: total training loss 11.76
2020-04-29 22:07:26,269 EPOCH 24
2020-04-29 22:07:26,444 Epoch  24: total training loss 11.67
2020-04-29 22:07:26,445 EPOCH 25
2020-04-29 22:07:26,744 Epoch  25: total training loss 11.53
2020-04-29 22:07:26,745 EPOCH 26
2020-04-29 22:07:26,900 Epoch  26: total training loss 11.48
2020-04-29 22:07:26,901 EPOCH 27
2020-04-29 22:07:27,007 Epoch  27: total training loss 11.36
2020-04-29 22:07:27,008 EPOCH 28
2020-04-29 22:07:27,246 Epoch  28: total training loss 11.21
2020-04-29 22:07:27,250 EPOCH 29
2020-04-29 22:07:27,594 Epoch  29: total training loss 11.10
2020-04-29 22:07:27,595 EPOCH 30
2020-04-29 22:07:27,699 Epoch  30 Step:       30 Batch Loss:    10.997220 Tokens per Sec:     1159, Lr: 0.001000
2020-04-29 22:07:27,799 Example #0
2020-04-29 22:07:27,802 	Raw source:     ['1', '2', '3', '4', '5']
2020-04-29 22:07:27,802 	Raw hypothesis: ['2', '2', '3', '3', '3', '3', '3', '3', '3', '3']
2020-04-29 22:07:27,802 	Source:     1 2 3 4 5
2020-04-29 22:07:27,802 	Reference:  5 4 3 2 1
2020-04-29 22:07:27,803 	Hypothesis: 2 2 3 3 3 3 3 3 3 3
2020-04-29 22:07:27,803 Example #3
2020-04-29 22:07:27,803 	Raw source:     ['1', '2', '3', '4', '5']
2020-04-29 22:07:27,803 	Raw hypothesis: ['2', '2', '3', '3', '3', '3', '3', '3', '3', '3']
2020-04-29 22:07:27,803 	Source:     1 2 3 4 5
2020-04-29 22:07:27,803 	Reference:  5 4 3 2 1
2020-04-29 22:07:27,803 	Hypothesis: 2 2 3 3 3 3 3 3 3 3
2020-04-29 22:07:27,803 Example #6
2020-04-29 22:07:27,803 	Raw source:     ['1', '2', '3', '4', '5']
2020-04-29 22:07:27,804 	Raw hypothesis: ['2', '2', '3', '3', '3', '3', '3', '3', '3', '3']
2020-04-29 22:07:27,805 	Source:     1 2 3 4 5
2020-04-29 22:07:27,805 	Reference:  5 4 3 2 1
2020-04-29 22:07:27,805 	Hypothesis: 2 2 3 3 3 3 3 3 3 3
2020-04-29 22:07:27,805 Validation result (greedy) at epoch  30, step       30: bleu:   0.00, loss: 216.8296, ppl:   6.0916, duration: 0.1054s
2020-04-29 22:07:30,528 Epoch  30: total training loss 11.00
2020-04-29 22:07:30,529 EPOCH 31
2020-04-29 22:07:30,603 Epoch  31: total training loss 10.88
2020-04-29 22:07:30,604 EPOCH 32
2020-04-29 22:07:30,723 Epoch  32: total training loss 10.75
2020-04-29 22:07:30,724 EPOCH 33
2020-04-29 22:07:30,886 Epoch  33: total training loss 10.64
2020-04-29 22:07:30,887 EPOCH 34
2020-04-29 22:07:30,966 Epoch  34: total training loss 10.55
2020-04-29 22:07:30,966 EPOCH 35
2020-04-29 22:07:31,093 Epoch  35: total training loss 10.38
2020-04-29 22:07:31,093 EPOCH 36
2020-04-29 22:07:31,179 Epoch  36: total training loss 10.24
2020-04-29 22:07:31,179 EPOCH 37
2020-04-29 22:07:31,277 Epoch  37: total training loss 10.18
2020-04-29 22:07:31,279 EPOCH 38
2020-04-29 22:07:31,364 Epoch  38: total training loss 9.99
2020-04-29 22:07:31,365 EPOCH 39
2020-04-29 22:07:31,471 Epoch  39: total training loss 9.85
2020-04-29 22:07:31,475 EPOCH 40
2020-04-29 22:07:31,568 Epoch  40 Step:       40 Batch Loss:     9.735497 Tokens per Sec:     1331, Lr: 0.001000
2020-04-29 22:07:31,663 Example #0
2020-04-29 22:07:31,664 	Raw source:     ['1', '2', '3', '4', '5']
2020-04-29 22:07:31,664 	Raw hypothesis: ['2', '2', '2', '3', '2', '3', '2', '3', '2', '3']
2020-04-29 22:07:31,664 	Source:     1 2 3 4 5
2020-04-29 22:07:31,664 	Reference:  5 4 3 2 1
2020-04-29 22:07:31,664 	Hypothesis: 2 2 2 3 2 3 2 3 2 3
2020-04-29 22:07:31,664 Example #3
2020-04-29 22:07:31,665 	Raw source:     ['1', '2', '3', '4', '5']
2020-04-29 22:07:31,665 	Raw hypothesis: ['2', '2', '2', '3', '2', '3', '2', '3', '2', '3']
2020-04-29 22:07:31,665 	Source:     1 2 3 4 5
2020-04-29 22:07:31,665 	Reference:  5 4 3 2 1
2020-04-29 22:07:31,665 	Hypothesis: 2 2 2 3 2 3 2 3 2 3
2020-04-29 22:07:31,665 Example #6
2020-04-29 22:07:31,666 	Raw source:     ['1', '2', '3', '4', '5']
2020-04-29 22:07:31,666 	Raw hypothesis: ['2', '2', '2', '3', '2', '3', '2', '3', '2', '3']
2020-04-29 22:07:31,666 	Source:     1 2 3 4 5
2020-04-29 22:07:31,666 	Reference:  5 4 3 2 1
2020-04-29 22:07:31,666 	Hypothesis: 2 2 2 3 2 3 2 3 2 3
2020-04-29 22:07:31,666 Validation result (greedy) at epoch  40, step       40: bleu:   0.00, loss: 190.7269, ppl:   4.9008, duration: 0.0979s
2020-04-29 22:07:33,514 Epoch  40: total training loss 9.74
2020-04-29 22:07:33,515 EPOCH 41
2020-04-29 22:07:33,716 Epoch  41: total training loss 9.56
2020-04-29 22:07:33,717 EPOCH 42
2020-04-29 22:07:33,821 Epoch  42: total training loss 9.45
2020-04-29 22:07:33,821 EPOCH 43
2020-04-29 22:07:33,923 Epoch  43: total training loss 9.27
2020-04-29 22:07:33,924 EPOCH 44
2020-04-29 22:07:34,022 Epoch  44: total training loss 9.18
2020-04-29 22:07:34,022 EPOCH 45
2020-04-29 22:07:34,121 Epoch  45: total training loss 8.93
2020-04-29 22:07:34,122 EPOCH 46
2020-04-29 22:07:34,182 Epoch  46: total training loss 8.89
2020-04-29 22:07:34,182 EPOCH 47
2020-04-29 22:07:34,282 Epoch  47: total training loss 8.71
2020-04-29 22:07:34,283 EPOCH 48
2020-04-29 22:07:34,368 Epoch  48: total training loss 8.50
2020-04-29 22:07:34,368 EPOCH 49
2020-04-29 22:07:34,532 Epoch  49: total training loss 8.30
2020-04-29 22:07:34,533 EPOCH 50
2020-04-29 22:07:34,733 Epoch  50 Step:       50 Batch Loss:     8.211332 Tokens per Sec:      602, Lr: 0.001000
2020-04-29 22:07:34,806 Hooray! New best validation result [eval_metric]!
2020-04-29 22:07:34,807 Saving new checkpoint.
2020-04-29 22:07:34,813 Example #0
2020-04-29 22:07:34,813 	Raw source:     ['1', '2', '3', '4', '5']
2020-04-29 22:07:34,813 	Raw hypothesis: ['5', '4', '3', '2']
2020-04-29 22:07:34,813 	Source:     1 2 3 4 5
2020-04-29 22:07:34,813 	Reference:  5 4 3 2 1
2020-04-29 22:07:34,813 	Hypothesis: 5 4 3 2
2020-04-29 22:07:34,814 Example #3
2020-04-29 22:07:34,814 	Raw source:     ['1', '2', '3', '4', '5']
2020-04-29 22:07:34,814 	Raw hypothesis: ['5', '4', '3', '2']
2020-04-29 22:07:34,814 	Source:     1 2 3 4 5
2020-04-29 22:07:34,814 	Reference:  5 4 3 2 1
2020-04-29 22:07:34,814 	Hypothesis: 5 4 3 2
2020-04-29 22:07:34,814 Example #6
2020-04-29 22:07:34,814 	Raw source:     ['1', '2', '3', '4', '5']
2020-04-29 22:07:34,815 	Raw hypothesis: ['5', '4', '3', '2']
2020-04-29 22:07:34,815 	Source:     1 2 3 4 5
2020-04-29 22:07:34,815 	Reference:  5 4 3 2 1
2020-04-29 22:07:34,815 	Hypothesis: 5 4 3 2
2020-04-29 22:07:34,815 Validation result (greedy) at epoch  50, step       50: bleu:  77.88, loss: 159.0018, ppl:   3.7622, duration: 0.0809s
2020-04-29 22:07:36,806 Epoch  50: total training loss 8.21
2020-04-29 22:07:36,806 EPOCH 51
2020-04-29 22:07:36,882 Epoch  51: total training loss 8.09
2020-04-29 22:07:36,883 EPOCH 52
2020-04-29 22:07:36,960 Epoch  52: total training loss 7.93
2020-04-29 22:07:36,960 EPOCH 53
2020-04-29 22:07:37,037 Epoch  53: total training loss 7.66
2020-04-29 22:07:37,038 EPOCH 54
2020-04-29 22:07:37,114 Epoch  54: total training loss 7.67
2020-04-29 22:07:37,115 EPOCH 55
2020-04-29 22:07:37,195 Epoch  55: total training loss 7.47
2020-04-29 22:07:37,196 EPOCH 56
2020-04-29 22:07:37,337 Epoch  56: total training loss 7.28
2020-04-29 22:07:37,338 EPOCH 57
2020-04-29 22:07:37,429 Epoch  57: total training loss 7.09
2020-04-29 22:07:37,432 EPOCH 58
2020-04-29 22:07:37,574 Epoch  58: total training loss 7.04
2020-04-29 22:07:37,575 EPOCH 59
2020-04-29 22:07:37,690 Epoch  59: total training loss 6.74
2020-04-29 22:07:37,691 EPOCH 60
2020-04-29 22:07:37,796 Epoch  60 Step:       60 Batch Loss:     6.592050 Tokens per Sec:     1141, Lr: 0.001000
2020-04-29 22:07:37,853 Hooray! New best validation result [eval_metric]!
2020-04-29 22:07:37,855 Saving new checkpoint.
2020-04-29 22:07:37,872 Example #0
2020-04-29 22:07:37,872 	Raw source:     ['1', '2', '3', '4', '5']
2020-04-29 22:07:37,873 	Raw hypothesis: ['5', '4', '3', '2', '1']
2020-04-29 22:07:37,873 	Source:     1 2 3 4 5
2020-04-29 22:07:37,873 	Reference:  5 4 3 2 1
2020-04-29 22:07:37,873 	Hypothesis: 5 4 3 2 1
2020-04-29 22:07:37,873 Example #3
2020-04-29 22:07:37,873 	Raw source:     ['1', '2', '3', '4', '5']
2020-04-29 22:07:37,873 	Raw hypothesis: ['5', '4', '3', '2', '1']
2020-04-29 22:07:37,873 	Source:     1 2 3 4 5
2020-04-29 22:07:37,874 	Reference:  5 4 3 2 1
2020-04-29 22:07:37,874 	Hypothesis: 5 4 3 2 1
2020-04-29 22:07:37,874 Example #6
2020-04-29 22:07:37,882 	Raw source:     ['1', '2', '3', '4', '5']
2020-04-29 22:07:37,882 	Raw hypothesis: ['5', '4', '3', '2', '1']
2020-04-29 22:07:37,883 	Source:     1 2 3 4 5
2020-04-29 22:07:37,883 	Reference:  5 4 3 2 1
2020-04-29 22:07:37,883 	Hypothesis: 5 4 3 2 1
2020-04-29 22:07:37,883 Validation result (greedy) at epoch  60, step       60: bleu: 100.00, loss: 126.8176, ppl:   2.8772, duration: 0.0864s
2020-04-29 22:07:40,192 Epoch  60: total training loss 6.59
2020-04-29 22:07:40,193 EPOCH 61
2020-04-29 22:07:40,248 Epoch  61: total training loss 6.40
2020-04-29 22:07:40,251 EPOCH 62
2020-04-29 22:07:40,336 Epoch  62: total training loss 6.38
2020-04-29 22:07:40,337 EPOCH 63
2020-04-29 22:07:40,435 Epoch  63: total training loss 6.15
2020-04-29 22:07:40,436 EPOCH 64
2020-04-29 22:07:40,532 Epoch  64: total training loss 6.06
2020-04-29 22:07:40,533 EPOCH 65
2020-04-29 22:07:40,604 Epoch  65: total training loss 5.85
2020-04-29 22:07:40,605 EPOCH 66
2020-04-29 22:07:40,691 Epoch  66: total training loss 5.60
2020-04-29 22:07:40,692 EPOCH 67
2020-04-29 22:07:40,818 Epoch  67: total training loss 5.61
2020-04-29 22:07:40,819 EPOCH 68
2020-04-29 22:07:40,962 Epoch  68: total training loss 5.44
2020-04-29 22:07:40,964 EPOCH 69
2020-04-29 22:07:41,159 Epoch  69: total training loss 5.24
2020-04-29 22:07:41,161 EPOCH 70
2020-04-29 22:07:41,270 Epoch  70 Step:       70 Batch Loss:     5.127400 Tokens per Sec:     1106, Lr: 0.001000
2020-04-29 22:07:41,355 Example #0
2020-04-29 22:07:41,356 	Raw source:     ['1', '2', '3', '4', '5']
2020-04-29 22:07:41,356 	Raw hypothesis: ['5', '4', '3', '2', '1']
2020-04-29 22:07:41,356 	Source:     1 2 3 4 5
2020-04-29 22:07:41,360 	Reference:  5 4 3 2 1
2020-04-29 22:07:41,360 	Hypothesis: 5 4 3 2 1
2020-04-29 22:07:41,360 Example #3
2020-04-29 22:07:41,361 	Raw source:     ['1', '2', '3', '4', '5']
2020-04-29 22:07:41,361 	Raw hypothesis: ['5', '4', '3', '2', '1']
2020-04-29 22:07:41,361 	Source:     1 2 3 4 5
2020-04-29 22:07:41,361 	Reference:  5 4 3 2 1
2020-04-29 22:07:41,361 	Hypothesis: 5 4 3 2 1
2020-04-29 22:07:41,361 Example #6
2020-04-29 22:07:41,361 	Raw source:     ['1', '2', '3', '4', '5']
2020-04-29 22:07:41,361 	Raw hypothesis: ['5', '4', '3', '2', '1']
2020-04-29 22:07:41,361 	Source:     1 2 3 4 5
2020-04-29 22:07:41,362 	Reference:  5 4 3 2 1
2020-04-29 22:07:41,362 	Hypothesis: 5 4 3 2 1
2020-04-29 22:07:41,362 Validation result (greedy) at epoch  70, step       70: bleu: 100.00, loss:  97.0120, ppl:   2.2444, duration: 0.0908s
2020-04-29 22:07:43,001 Epoch  70: total training loss 5.13
2020-04-29 22:07:43,002 EPOCH 71
2020-04-29 22:07:43,142 Epoch  71: total training loss 4.99
2020-04-29 22:07:43,144 EPOCH 72
2020-04-29 22:07:43,278 Epoch  72: total training loss 4.81
2020-04-29 22:07:43,280 EPOCH 73
2020-04-29 22:07:43,398 Epoch  73: total training loss 4.82
2020-04-29 22:07:43,401 EPOCH 74
2020-04-29 22:07:43,523 Epoch  74: total training loss 4.68
2020-04-29 22:07:43,526 EPOCH 75
2020-04-29 22:07:43,693 Epoch  75: total training loss 4.45
2020-04-29 22:07:43,693 EPOCH 76
2020-04-29 22:07:43,790 Epoch  76: total training loss 4.41
2020-04-29 22:07:43,795 EPOCH 77
2020-04-29 22:07:43,877 Epoch  77: total training loss 4.17
2020-04-29 22:07:43,879 EPOCH 78
2020-04-29 22:07:44,028 Epoch  78: total training loss 4.15
2020-04-29 22:07:44,028 EPOCH 79
2020-04-29 22:07:44,121 Epoch  79: total training loss 4.06
2020-04-29 22:07:44,122 EPOCH 80
2020-04-29 22:07:44,277 Epoch  80 Step:       80 Batch Loss:     3.978279 Tokens per Sec:      775, Lr: 0.001000
2020-04-29 22:07:44,326 Example #0
2020-04-29 22:07:44,327 	Raw source:     ['1', '2', '3', '4', '5']
2020-04-29 22:07:44,327 	Raw hypothesis: ['5', '4', '3', '2', '1']
2020-04-29 22:07:44,327 	Source:     1 2 3 4 5
2020-04-29 22:07:44,327 	Reference:  5 4 3 2 1
2020-04-29 22:07:44,327 	Hypothesis: 5 4 3 2 1
2020-04-29 22:07:44,328 Example #3
2020-04-29 22:07:44,328 	Raw source:     ['1', '2', '3', '4', '5']
2020-04-29 22:07:44,328 	Raw hypothesis: ['5', '4', '3', '2', '1']
2020-04-29 22:07:44,328 	Source:     1 2 3 4 5
2020-04-29 22:07:44,328 	Reference:  5 4 3 2 1
2020-04-29 22:07:44,329 	Hypothesis: 5 4 3 2 1
2020-04-29 22:07:44,329 Example #6
2020-04-29 22:07:44,329 	Raw source:     ['1', '2', '3', '4', '5']
2020-04-29 22:07:44,329 	Raw hypothesis: ['5', '4', '3', '2', '1']
2020-04-29 22:07:44,329 	Source:     1 2 3 4 5
2020-04-29 22:07:44,329 	Reference:  5 4 3 2 1
2020-04-29 22:07:44,330 	Hypothesis: 5 4 3 2 1
2020-04-29 22:07:44,330 Validation result (greedy) at epoch  80, step       80: bleu: 100.00, loss:  72.9905, ppl:   1.8372, duration: 0.0512s
2020-04-29 22:07:46,055 Epoch  80: total training loss 3.98
2020-04-29 22:07:46,056 EPOCH 81
2020-04-29 22:07:46,269 Epoch  81: total training loss 3.85
2020-04-29 22:07:46,269 EPOCH 82
2020-04-29 22:07:46,345 Epoch  82: total training loss 3.77
2020-04-29 22:07:46,349 EPOCH 83
2020-04-29 22:07:46,538 Epoch  83: total training loss 3.60
2020-04-29 22:07:46,539 EPOCH 84
2020-04-29 22:07:46,682 Epoch  84: total training loss 3.61
2020-04-29 22:07:46,684 EPOCH 85
2020-04-29 22:07:46,924 Epoch  85: total training loss 3.44
2020-04-29 22:07:46,925 EPOCH 86
2020-04-29 22:07:47,001 Epoch  86: total training loss 3.29
2020-04-29 22:07:47,002 EPOCH 87
2020-04-29 22:07:47,097 Epoch  87: total training loss 3.29
2020-04-29 22:07:47,099 EPOCH 88
2020-04-29 22:07:47,227 Epoch  88: total training loss 3.09
2020-04-29 22:07:47,229 EPOCH 89
2020-04-29 22:07:47,439 Epoch  89: total training loss 3.08
2020-04-29 22:07:47,442 EPOCH 90
2020-04-29 22:07:47,594 Epoch  90 Step:       90 Batch Loss:     3.038573 Tokens per Sec:      794, Lr: 0.001000
2020-04-29 22:07:47,720 Example #0
2020-04-29 22:07:47,720 	Raw source:     ['1', '2', '3', '4', '5']
2020-04-29 22:07:47,720 	Raw hypothesis: ['5', '4', '3', '2', '1']
2020-04-29 22:07:47,720 	Source:     1 2 3 4 5
2020-04-29 22:07:47,724 	Reference:  5 4 3 2 1
2020-04-29 22:07:47,724 	Hypothesis: 5 4 3 2 1
2020-04-29 22:07:47,724 Example #3
2020-04-29 22:07:47,725 	Raw source:     ['1', '2', '3', '4', '5']
2020-04-29 22:07:47,725 	Raw hypothesis: ['5', '4', '3', '2', '1']
2020-04-29 22:07:47,725 	Source:     1 2 3 4 5
2020-04-29 22:07:47,725 	Reference:  5 4 3 2 1
2020-04-29 22:07:47,725 	Hypothesis: 5 4 3 2 1
2020-04-29 22:07:47,725 Example #6
2020-04-29 22:07:47,725 	Raw source:     ['1', '2', '3', '4', '5']
2020-04-29 22:07:47,725 	Raw hypothesis: ['5', '4', '3', '2', '1']
2020-04-29 22:07:47,726 	Source:     1 2 3 4 5
2020-04-29 22:07:47,726 	Reference:  5 4 3 2 1
2020-04-29 22:07:47,726 	Hypothesis: 5 4 3 2 1
2020-04-29 22:07:47,726 Validation result (greedy) at epoch  90, step       90: bleu: 100.00, loss:  54.1671, ppl:   1.5705, duration: 0.1310s
2020-04-29 22:07:49,506 Epoch  90: total training loss 3.04
2020-04-29 22:07:49,507 EPOCH 91
2020-04-29 22:07:49,650 Epoch  91: total training loss 2.87
2020-04-29 22:07:49,651 EPOCH 92
2020-04-29 22:07:49,721 Epoch  92: total training loss 2.75
2020-04-29 22:07:49,722 EPOCH 93
2020-04-29 22:07:49,838 Epoch  93: total training loss 2.80
2020-04-29 22:07:49,839 EPOCH 94
2020-04-29 22:07:49,884 Epoch  94: total training loss 2.66
2020-04-29 22:07:49,884 EPOCH 95
2020-04-29 22:07:49,940 Epoch  95: total training loss 2.62
2020-04-29 22:07:49,941 EPOCH 96
2020-04-29 22:07:50,000 Epoch  96: total training loss 2.59
2020-04-29 22:07:50,001 EPOCH 97
2020-04-29 22:07:50,087 Epoch  97: total training loss 2.48
2020-04-29 22:07:50,089 EPOCH 98
2020-04-29 22:07:50,253 Epoch  98: total training loss 2.38
2020-04-29 22:07:50,254 EPOCH 99
2020-04-29 22:07:50,395 Epoch  99: total training loss 2.29
2020-04-29 22:07:50,396 EPOCH 100
2020-04-29 22:07:50,501 Epoch 100 Step:      100 Batch Loss:     2.290709 Tokens per Sec:     1149, Lr: 0.001000
2020-04-29 22:07:50,617 Example #0
2020-04-29 22:07:50,618 	Raw source:     ['1', '2', '3', '4', '5']
2020-04-29 22:07:50,619 	Raw hypothesis: ['5', '4', '3', '2', '1']
2020-04-29 22:07:50,628 	Source:     1 2 3 4 5
2020-04-29 22:07:50,628 	Reference:  5 4 3 2 1
2020-04-29 22:07:50,628 	Hypothesis: 5 4 3 2 1
2020-04-29 22:07:50,628 Example #3
2020-04-29 22:07:50,628 	Raw source:     ['1', '2', '3', '4', '5']
2020-04-29 22:07:50,629 	Raw hypothesis: ['5', '4', '3', '2', '1']
2020-04-29 22:07:50,629 	Source:     1 2 3 4 5
2020-04-29 22:07:50,629 	Reference:  5 4 3 2 1
2020-04-29 22:07:50,629 	Hypothesis: 5 4 3 2 1
2020-04-29 22:07:50,629 Example #6
2020-04-29 22:07:50,629 	Raw source:     ['1', '2', '3', '4', '5']
2020-04-29 22:07:50,630 	Raw hypothesis: ['5', '4', '3', '2', '1']
2020-04-29 22:07:50,630 	Source:     1 2 3 4 5
2020-04-29 22:07:50,630 	Reference:  5 4 3 2 1
2020-04-29 22:07:50,630 	Hypothesis: 5 4 3 2 1
2020-04-29 22:07:50,630 Validation result (greedy) at epoch 100, step      100: bleu: 100.00, loss:  39.3501, ppl:   1.3881, duration: 0.1285s
2020-04-29 22:07:53,819 Epoch 100: total training loss 2.29
2020-04-29 22:07:53,820 Training ended after 100 epochs.
2020-04-29 22:07:53,820 Best validation result (greedy) at step       60: 100.00 eval_metric.
2020-04-29 22:07:53,969  dev bleu: 100.00 [Greedy decoding]
2020-04-29 22:07:53,970 Translations saved to: mini_batch_reverse/00000060.hyps.dev
2020-04-29 22:07:54,014 test bleu: 100.00 [Greedy decoding]
2020-04-29 22:07:54,015 Translations saved to: mini_batch_reverse/00000060.hyps.test
