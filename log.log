Sample size:  256
State size:  24
Action size:  9
bleu_seq
You select the reward based on the sequence accuaracy bleu_seq
EPOCH %d 1
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.9 0.9

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

0 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

2 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
3 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 4 5 3]]
Reward:  [1.  0.8 0.6] 

4 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
5 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
6 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 3]
Eval  :  [[2 4 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 

EPOCH %d 2
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.81 0.81
2493 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
2494 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
2495 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
2496 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
2497 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
2498 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
2499 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
EPOCH %d 3
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.7290000000000001 0.7290000000000001
4986 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4987 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4988 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4989 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4990 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4991 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4992 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
As referece this first test on dev data. Is maded with the Q networks, initialized randomly : 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.9378,  0.6153, -0.6642])
So far:  [array([2]), array([4]), array([4])]  the state[:3] is:  tensor([-0.9584,  0.7692, -0.9055])
So far:  [array([2]), array([4]), array([4]), array([4])]  the state[:3] is:  tensor([-0.9540,  0.8120, -0.9279])
So far:  [array([2]), array([4]), array([4]), array([4]), array([4])]  the state[:3] is:  tensor([-0.9512,  0.8205, -0.9322])
So far:  [array([2]), array([4]), array([4]), array([4]), array([4]), array([4])]  the state[:3] is:  tensor([-0.9491,  0.8240, -0.9336])
So far:  [array([2]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4])]  the state[:3] is:  tensor([-0.9477,  0.8256, -0.9341])
So far:  [array([2]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4])]  the state[:3] is:  tensor([-0.9469,  0.8265, -0.9344])
So far:  [array([2]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4])]  the state[:3] is:  tensor([-0.9464,  0.8269, -0.9346])
So far:  [array([2]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4])]  the state[:3] is:  tensor([-0.9461,  0.8272, -0.9346])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  3 10 10 10 10 10 10 10 10]
Eval  :  [[2 4 4 4 4 4 4 4 4 4 4]]
Reward:  [-2.  -0.4 -0.8 -1.2 -1.6 -2.  -2.4 -2.8 -3.2 -3.6] 

roptimal:  tensor([[2, 7]]) [[2 7 3]]
[1.  0.8]
So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.9850,  0.9089, -0.9363])
So far:  [array([2]), array([4]), array([4])]  the state[:3] is:  tensor([-0.9922,  0.9398, -0.9843])
So far:  [array([2]), array([4]), array([4]), array([4])]  the state[:3] is:  tensor([-0.9903,  0.9516, -0.9886])
So far:  [array([2]), array([4]), array([4]), array([4]), array([4])]  the state[:3] is:  tensor([-0.9894,  0.9543, -0.9892])
So far:  [array([2]), array([4]), array([4]), array([4]), array([4]), array([4])]  the state[:3] is:  tensor([-0.9887,  0.9556, -0.9894])
So far:  [array([2]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4])]  the state[:3] is:  tensor([-0.9882,  0.9562, -0.9895])
So far:  [array([2]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4])]  the state[:3] is:  tensor([-0.9880,  0.9565, -0.9895])
So far:  [array([2]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4])]  the state[:3] is:  tensor([-0.9879,  0.9567, -0.9895])
So far:  [array([2]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4])]  the state[:3] is:  tensor([-0.9878,  0.9567, -0.9895])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  8  3 10 10 10 10 10 10 10 10]
Eval  :  [[2 4 4 4 4 4 4 4 4 4 4]]
Reward:  [-2.  -0.4 -0.8 -1.2 -1.6 -2.  -2.4 -2.8 -3.2 -3.6] 

roptimal:  tensor([[2, 8]]) [[2 8 3]]
[1.  0.8]
So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])
So far:  [array([2]), array([4]), array([4])]  the state[:3] is:  tensor([-0.9673,  0.9915, -0.7579])
So far:  [array([2]), array([4]), array([4]), array([4])]  the state[:3] is:  tensor([-0.9833,  0.9930, -0.9192])
So far:  [array([2]), array([4]), array([4]), array([4]), array([4])]  the state[:3] is:  tensor([-0.9852,  0.9933, -0.9380])
So far:  [array([2]), array([4]), array([4]), array([4]), array([4]), array([4])]  the state[:3] is:  tensor([-0.9853,  0.9933, -0.9427])
So far:  [array([2]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4])]  the state[:3] is:  tensor([-0.9850,  0.9934, -0.9443])
So far:  [array([2]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4])]  the state[:3] is:  tensor([-0.9848,  0.9934, -0.9449])
So far:  [array([2]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4])]  the state[:3] is:  tensor([-0.9846,  0.9934, -0.9452])
So far:  [array([2]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4])]  the state[:3] is:  tensor([-0.9845,  0.9934, -0.9453])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  4  5  3 10 10 10 10 10 10 10]
Eval  :  [[2 4 4 4 4 4 4 4 4 4 4]]
Reward:  [ 1.  -4.4 -0.8 -1.2 -1.6 -2.  -2.4 -2.8 -3.2 -3.6] 

roptimal:  tensor([[2, 4, 5]]) [[2 4 5 3]]
[1.  0.8 0.6]

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10 10 10 10 10 10 10]
Eval  :  [[2 4 4 4 4 4 4 4 4 4 4]]
Reward:  [-2.  -0.4 -0.8 -1.2 -1.6 -2.  -2.4 -2.8 -3.2 -3.6] 

roptimal:  tensor([[2, 7, 6]]) [[2 7 6 3]]
[1.  0.8 0.6]

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  8  3 10 10 10 10 10 10 10 10]
Eval  :  [[2 4 4 4 4 4 4 4 4 4 4]]
Reward:  [-2.  -0.4 -0.8 -1.2 -1.6 -2.  -2.4 -2.8 -3.2 -3.6] 

roptimal:  tensor([[2, 8]]) [[2 8 3]]
[1.  0.8]

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  5  3 10 10 10 10 10 10 10 10]
Eval  :  [[2 4 4 4 4 4 4 4 4 4 4]]
Reward:  [-2.  -0.4 -0.8 -1.2 -1.6 -2.  -2.4 -2.8 -3.2 -3.6] 

roptimal:  tensor([[2, 5]]) [[2 5 3]]
[1.  0.8]

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  6  3 10 10 10 10 10 10 10 10]
Eval  :  [[2 4 4 4 4 4 4 4 4 4 4]]
Reward:  [-2.  -0.4 -0.8 -1.2 -1.6 -2.  -2.4 -2.8 -3.2 -3.6] 

roptimal:  tensor([[2, 6]]) [[2 6 3]]
[1.  0.8]

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  4  6  3 10 10 10 10 10 10 10]
Eval  :  [[2 4 4 4 4 4 4 4 4 4 4]]
Reward:  [ 1.  -4.4 -0.8 -1.2 -1.6 -2.  -2.4 -2.8 -3.2 -3.6] 

roptimal:  tensor([[2, 4, 6]]) [[2 4 6 3]]
[1.  0.8 0.6]

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  5  3 10 10 10 10 10 10 10]
Eval  :  [[2 4 4 4 4 4 4 4 4 4 4]]
Reward:  [-2.  -0.4 -0.8 -1.2 -1.6 -2.  -2.4 -2.8 -3.2 -3.6] 

roptimal:  tensor([[2, 7, 5]]) [[2 7 5 3]]
[1.  0.8 0.6]

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  3 10 10 10 10 10 10 10 10]
Eval  :  [[2 4 4 4 4 4 4 4 4 4 4]]
Reward:  [-2.  -0.4 -0.8 -1.2 -1.6 -2.  -2.4 -2.8 -3.2 -3.6] 

roptimal:  tensor([[2, 7]]) [[2 7 3]]
[1.  0.8]
roptimal:  tensor([[2, 6, 7]]) [[2 6 7 3]]
[1.  0.8 0.6]
roptimal:  tensor([[2, 7]]) [[2 7 3]]
[1.  0.8]
roptimal:  tensor([[2, 7, 6]]) [[2 7 6 3]]
[1.  0.8 0.6]
roptimal:  tensor([[2, 6]]) [[2 6 3]]
[1.  0.8]
roptimal:  tensor([[2, 4]]) [[2 4 3]]
[1.  0.8]
roptimal:  tensor([[2, 8, 8]]) [[2 8 8 3]]
[1.  0.8 0.6]
roptimal:  tensor([[2, 8]]) [[2 8 3]]
[1.  0.8]
roptimal:  tensor([[2, 5, 4]]) [[2 5 4 3]]
[1.  0.8 0.6]
roptimal:  tensor([[2, 4, 8]]) [[2 4 8 3]]
[1.  0.8 0.6]
roptimal:  tensor([[2, 5]]) [[2 5 3]]
[1.  0.8]
roptimal:  tensor([[2, 8]]) [[2 8 3]]
[1.  0.8]
roptimal:  tensor([[2, 8]]) [[2 8 3]]
[1.  0.8]
roptimal:  tensor([[2, 5]]) [[2 5 3]]
[1.  0.8]
roptimal:  tensor([[2, 7, 8]]) [[2 7 8 3]]
[1.  0.8 0.6]
roptimal:  tensor([[2, 7]]) [[2 7 3]]
[1.  0.8]
roptimal:  tensor([[2, 6, 5]]) [[2 6 5 3]]
[1.  0.8 0.6]
roptimal:  tensor([[2, 7]]) [[2 7 3]]
[1.  0.8]
roptimal:  tensor([[2, 7, 6]]) [[2 7 6 3]]
[1.  0.8 0.6]
roptimal:  tensor([[2, 4, 7]]) [[2 4 7 3]]
[1.  0.8 0.6]
roptimal:  tensor([[2, 6]]) [[2 6 3]]
[1.  0.8]
roptimal:  tensor([[2, 8, 6]]) [[2 8 6 3]]
[1.  0.8 0.6]
roptimal:  tensor([[2, 8]]) [[2 8 3]]
[1.  0.8]
roptimal:  tensor([[2, 6, 6]]) [[2 6 6 3]]
[1.  0.8 0.6]
roptimal:  tensor([[2, 4, 6]]) [[2 4 6 3]]
[1.  0.8 0.6]
roptimal:  tensor([[2, 6, 8]]) [[2 6 8 3]]
[1.  0.8 0.6]
roptimal:  tensor([[2, 4, 6]]) [[2 4 6 3]]
[1.  0.8 0.6]
roptimal:  tensor([[2, 4]]) [[2 4 3]]
[1.  0.8]
roptimal:  tensor([[2, 5, 6]]) [[2 5 6 3]]
[1.  0.8 0.6]
roptimal:  tensor([[2, 6]]) [[2 6 3]]
[1.  0.8]
roptimal:  tensor([[2, 6, 4]]) [[2 6 4 3]]
[1.  0.8 0.6]
roptimal:  tensor([[2, 6]]) [[2 6 3]]
[1.  0.8]
roptimal:  tensor([[2, 6, 6]]) [[2 6 6 3]]
[1.  0.8 0.6]
roptimal:  tensor([[2, 5, 7]]) [[2 5 7 3]]
[1.  0.8 0.6]
roptimal:  tensor([[2, 5]]) [[2 5 3]]
[1.  0.8]
roptimal:  tensor([[2, 6, 8]]) [[2 6 8 3]]
[1.  0.8 0.6]
roptimal:  tensor([[2, 4, 6]]) [[2 4 6 3]]
[1.  0.8 0.6]
roptimal:  tensor([[2, 4, 7]]) [[2 4 7 3]]
[1.  0.8 0.6]
roptimal:  tensor([[2, 4, 7]]) [[2 4 7 3]]
[1.  0.8 0.6]
roptimal:  tensor([[2, 5, 8]]) [[2 5 8 3]]
[1.  0.8 0.6]
roptimal:  tensor([[2, 7]]) [[2 7 3]]
[1.  0.8]
roptimal:  tensor([[2, 4]]) [[2 4 3]]
[1.  0.8]
roptimal:  tensor([[2, 4]]) [[2 4 3]]
[1.  0.8]
roptimal:  tensor([[2, 5]]) [[2 5 3]]
[1.  0.8]
roptimal:  tensor([[2, 5]]) [[2 5 3]]
[1.  0.8]
roptimal:  tensor([[2, 5, 5]]) [[2 5 5 3]]
[1.  0.8 0.6]
roptimal:  tensor([[2, 4]]) [[2 4 3]]
[1.  0.8]
roptimal:  tensor([[2, 5, 6]]) [[2 5 6 3]]
[1.  0.8 0.6]
roptimal:  tensor([[2, 4, 4]]) [[2 4 4 3]]
[1.  0.8 0.6]
roptimal:  tensor([[2, 6, 7]]) [[2 6 7 3]]
[1.  0.8 0.6]
roptimal:  tensor([[2, 4, 8]]) [[2 4 8 3]]
[1.  0.8 0.6]
roptimal:  tensor([[2, 5, 8]]) [[2 5 8 3]]
[1.  0.8 0.6]
roptimal:  tensor([[2, 5]]) [[2 5 3]]
[1.  0.8]
roptimal:  tensor([[2, 8, 6]]) [[2 8 6 3]]
[1.  0.8 0.6]
roptimal:  tensor([[2, 7]]) [[2 7 3]]
[1.  0.8]
roptimal:  tensor([[2, 6]]) [[2 6 3]]
[1.  0.8]
roptimal:  tensor([[2, 5, 6]]) [[2 5 6 3]]
[1.  0.8 0.6]
roptimal:  tensor([[2, 8]]) [[2 8 3]]
[1.  0.8]
roptimal:  tensor([[2, 6]]) [[2 6 3]]
[1.  0.8]
roptimal:  tensor([[2, 8]]) [[2 8 3]]
[1.  0.8]
roptimal:  tensor([[2, 6, 5]]) [[2 6 5 3]]
[1.  0.8 0.6]
roptimal:  tensor([[2, 4, 6]]) [[2 4 6 3]]
[1.  0.8 0.6]
roptimal:  tensor([[2, 7]]) [[2 7 3]]
[1.  0.8]
roptimal:  tensor([[2, 7]]) [[2 7 3]]
[1.  0.8]
roptimal:  tensor([[2, 8]]) [[2 8 3]]
[1.  0.8]
roptimal:  tensor([[2, 8]]) [[2 8 3]]
[1.  0.8]
roptimal:  tensor([[2, 8, 8]]) [[2 8 8 3]]
[1.  0.8 0.6]
roptimal:  tensor([[2, 6, 6]]) [[2 6 6 3]]
[1.  0.8 0.6]
roptimal:  tensor([[2, 7, 4]]) [[2 7 4 3]]
[1.  0.8 0.6]
roptimal:  tensor([[2, 5]]) [[2 5 3]]
[1.  0.8]
roptimal:  tensor([[2, 4]]) [[2 4 3]]
[1.  0.8]
roptimal:  tensor([[2, 4]]) [[2 4 3]]
[1.  0.8]
roptimal:  tensor([[2, 5]]) [[2 5 3]]
[1.  0.8]
roptimal:  tensor([[2, 5, 7]]) [[2 5 7 3]]
[1.  0.8 0.6]
roptimal:  tensor([[2, 5]]) [[2 5 3]]
[1.  0.8]
roptimal:  tensor([[2, 4, 4]]) [[2 4 4 3]]
[1.  0.8 0.6]
roptimal:  tensor([[2, 7]]) [[2 7 3]]
[1.  0.8]
roptimal:  tensor([[2, 4, 8]]) [[2 4 8 3]]
[1.  0.8 0.6]
roptimal:  tensor([[2, 6, 6]]) [[2 6 6 3]]
[1.  0.8 0.6]
roptimal:  tensor([[2, 4, 7]]) [[2 4 7 3]]
[1.  0.8 0.6]
roptimal:  tensor([[2, 4]]) [[2 4 3]]
[1.  0.8]
roptimal:  tensor([[2, 5]]) [[2 5 3]]
[1.  0.8]
roptimal:  tensor([[2, 5, 4]]) [[2 5 4 3]]
[1.  0.8 0.6]
roptimal:  tensor([[2, 8]]) [[2 8 3]]
[1.  0.8]
roptimal:  tensor([[2, 4, 7]]) [[2 4 7 3]]
[1.  0.8 0.6]
roptimal:  tensor([[2, 4]]) [[2 4 3]]
[1.  0.8]
roptimal:  tensor([[2, 7, 7]]) [[2 7 7 3]]
[1.  0.8 0.6]
roptimal:  tensor([[2, 7]]) [[2 7 3]]
[1.  0.8]
roptimal:  tensor([[2, 5, 8]]) [[2 5 8 3]]
[1.  0.8 0.6]
roptimal:  tensor([[2, 8, 6]]) [[2 8 6 3]]
[1.  0.8 0.6]
roptimal:  tensor([[2, 7, 5]]) [[2 7 5 3]]
[1.  0.8 0.6]
roptimal:  tensor([[2, 4]]) [[2 4 3]]
[1.  0.8]
roptimal:  tensor([[2, 6]]) [[2 6 3]]
[1.  0.8]
roptimal:  tensor([[2, 7, 6]]) [[2 7 6 3]]
[1.  0.8 0.6]
The optimal reward is:  216.60000000000042
valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['1 1 1 1 1 1 1 1 1 1', '1 1 1 1 1 1 1 1 1 1', '1 1 1 1 1 1 1 1 1 1', '1 1 1 1 1 1 1 1 1 1', '1 1 1 1 1 1 1 1 1 1', '1 1 1 1 1 1 1 1 1 1', '1 1 1 1 1 1 1 1 1 1', '1 1 1 1 1 1 1 1 1 1', '1 1 1 1 1 1 1 1 1 1', '1 1 1 1 1 1 1 1 1 1']
1  r_total and score:  38.8 0.0
Current Bleu score is:  0.0
learn step counter:  1
dev_network_count:  1
Using pretraining...
learn step counter:  51
dev_network_count:  1
learn step counter:  101
dev_network_count:  1
learn step counter:  151
dev_network_count:  1
learn step counter:  201
dev_network_count:  1
learn step counter:  251
dev_network_count:  1

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-2.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])
So far:  [array([2]), array([4]), array([7])]  the state[:3] is:  tensor([-0.9601,  0.9921, -0.7420])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 4 7 3]]
Reward:  [ 1.  -4.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 5 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 8 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 7 7 3]]
Reward:  [ 1.  -4.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-2.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '1 3', '4 4', '2', '0', '4', '2 4', '3 3', '4']
2  r_total and score:  276.0000000000004 49.56385459568427
Current Bleu score is:  49.56385459568427
learn step counter:  301
dev_network_count:  2
learn step counter:  351
dev_network_count:  2
learn step counter:  401
dev_network_count:  2
learn step counter:  451
dev_network_count:  2
learn step counter:  501
dev_network_count:  2
learn step counter:  551
dev_network_count:  2

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-2.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([5])]  the state[:3] is:  tensor([-0.5256,  0.9823,  0.5156])
So far:  [array([2]), array([5]), array([4])]  the state[:3] is:  tensor([-0.9656,  0.9911, -0.7185])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 5 4 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 5 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 8 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 5 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-2.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '0 1', '4 4', '2', '0', '4', '2 4', '0 3', '4']
3  r_total and score:  270.8000000000004 51.83960757134804
Current Bleu score is:  51.83960757134804
learn step counter:  601
dev_network_count:  3
learn step counter:  651
dev_network_count:  3
learn step counter:  701
dev_network_count:  3
learn step counter:  751
dev_network_count:  3
learn step counter:  801
dev_network_count:  3
learn step counter:  851
dev_network_count:  3

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-2.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([5])]  the state[:3] is:  tensor([-0.5256,  0.9823,  0.5156])
So far:  [array([2]), array([5]), array([4])]  the state[:3] is:  tensor([-0.9656,  0.9911, -0.7185])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 5 4 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 5 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 8 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 5 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-2.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '0 1', '4 4', '2', '0', '4', '2 4', '0 3', '4']
4  r_total and score:  264.20000000000044 56.511488983186176
Current Bleu score is:  56.511488983186176
learn step counter:  901
dev_network_count:  4
learn step counter:  951
dev_network_count:  4
EPOCH %d 4
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.6561 0.6561
2479 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
2480 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
2481 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
2482 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
Starting using Q target net....
2483 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
2484 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
2485 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  1001
dev_network_count:  4
learn step counter:  1051
dev_network_count:  4
learn step counter:  1101
dev_network_count:  4
learn step counter:  1151
dev_network_count:  4

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-2.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([5])]  the state[:3] is:  tensor([-0.5256,  0.9823,  0.5156])
So far:  [array([2]), array([5]), array([4])]  the state[:3] is:  tensor([-0.9656,  0.9911, -0.7185])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 5 4 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 5 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 8 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 5 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-2.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '0 1', '4 4', '2', '0', '4', '2 4', '0 3', '4']
5  r_total and score:  260.8000000000004 56.633681297993704
Current Bleu score is:  56.633681297993704
learn step counter:  1201
dev_network_count:  5
learn step counter:  1251
dev_network_count:  5
learn step counter:  1301
dev_network_count:  5
learn step counter:  1351
dev_network_count:  5
learn step counter:  1401
dev_network_count:  5
learn step counter:  1451
dev_network_count:  5

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-2.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])
So far:  [array([2]), array([4]), array([7])]  the state[:3] is:  tensor([-0.9601,  0.9921, -0.7420])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 4 7 3]]
Reward:  [ 1.  -4.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 5 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 8 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 5 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-2.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '1 3', '4 4', '2', '0', '4', '2 4', '0 3', '4']
6  r_total and score:  266.6000000000004 57.781568665270356
Current Bleu score is:  57.781568665270356
learn step counter:  1501
dev_network_count:  6
learn step counter:  1551
dev_network_count:  6
learn step counter:  1601
dev_network_count:  6
learn step counter:  1651
dev_network_count:  6
learn step counter:  1701
dev_network_count:  6
learn step counter:  1751
dev_network_count:  6

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-2.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])
So far:  [array([2]), array([4]), array([4])]  the state[:3] is:  tensor([-0.9673,  0.9915, -0.7579])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 4 4 3]]
Reward:  [ 1.  -4.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 5 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 8 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 5 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-2.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '1 1', '4 4', '2', '0', '4', '2 4', '0 3', '4']
7  r_total and score:  263.40000000000043 59.02432522055574
Current Bleu score is:  59.02432522055574
learn step counter:  1801
dev_network_count:  7
learn step counter:  1851
dev_network_count:  7
learn step counter:  1901
dev_network_count:  7
learn step counter:  1951
dev_network_count:  7
EPOCH %d 5
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.5904900000000001 0.5904900000000001
4972 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4973 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4974 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4975 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4976 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4977 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4978 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  2001
dev_network_count:  7
learn step counter:  2051
dev_network_count:  7

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-2.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])
So far:  [array([2]), array([4]), array([7])]  the state[:3] is:  tensor([-0.9601,  0.9921, -0.7420])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 4 7 3]]
Reward:  [ 1.  -4.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 5 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 4 6 3]]
Reward:  [1.  0.8 0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-2.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '1 3', '4 4', '2', '0', '4', '1 4', '4 3', '4']
8  r_total and score:  257.8000000000004 69.88114688402233
Current Bleu score is:  69.88114688402233
learn step counter:  2101
dev_network_count:  8
learn step counter:  2151
dev_network_count:  8
learn step counter:  2201
dev_network_count:  8
learn step counter:  2251
dev_network_count:  8
learn step counter:  2301
dev_network_count:  8
learn step counter:  2351
dev_network_count:  8

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-2.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])
So far:  [array([2]), array([4]), array([4])]  the state[:3] is:  tensor([-0.9673,  0.9915, -0.7579])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 4 4 3]]
Reward:  [ 1.  -4.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 5 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 4 6 3]]
Reward:  [1.  0.8 0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-2.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '1 1', '4 4', '2', '0', '4', '1 4', '4 3', '4']
9  r_total and score:  253.4000000000004 72.61510072703895
Current Bleu score is:  72.61510072703895
learn step counter:  2401
dev_network_count:  9
learn step counter:  2451
dev_network_count:  9
learn step counter:  2501
dev_network_count:  9
learn step counter:  2551
dev_network_count:  9
learn step counter:  2601
dev_network_count:  9
learn step counter:  2651
dev_network_count:  9

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-2.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])
So far:  [array([2]), array([4]), array([7])]  the state[:3] is:  tensor([-0.9601,  0.9921, -0.7420])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 4 7 3]]
Reward:  [ 1.  -4.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 5 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 4 6 3]]
Reward:  [1.  0.8 0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-2.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '1 3', '4 4', '2', '0', '4', '1 4', '4 3', '4']
10  r_total and score:  253.20000000000041 72.90498252127077
Current Bleu score is:  72.90498252127077
learn step counter:  2701
dev_network_count:  10
learn step counter:  2751
dev_network_count:  10
learn step counter:  2801
dev_network_count:  10
learn step counter:  2851
dev_network_count:  10
learn step counter:  2901
dev_network_count:  10
learn step counter:  2951
dev_network_count:  10
EPOCH %d 6
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.531441 0.531441
2465 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
2466 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
2467 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
2468 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
2469 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
2470 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
2471 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])
So far:  [array([2]), array([4]), array([7])]  the state[:3] is:  tensor([-0.9601,  0.9921, -0.7420])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 4 7 3]]
Reward:  [ 1.  -4.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 7 3]]
Reward:  [-2.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 4 6 3]]
Reward:  [1.  0.8 0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 5 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1 3', '4 4', '2', '3', '4', '1 4', '0 3', '3']
11  r_total and score:  253.60000000000045 68.37635211648524
Current Bleu score is:  68.37635211648524
learn step counter:  3001
dev_network_count:  11
learn step counter:  3051
dev_network_count:  11
learn step counter:  3101
dev_network_count:  11
learn step counter:  3151
dev_network_count:  11
learn step counter:  3201
dev_network_count:  11
learn step counter:  3251
dev_network_count:  11

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-2.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])
So far:  [array([2]), array([4]), array([7])]  the state[:3] is:  tensor([-0.9601,  0.9921, -0.7420])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 4 7 3]]
Reward:  [ 1.  -4.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 7 3]]
Reward:  [-2.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 4 6 3]]
Reward:  [1.  0.8 0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-2.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '1 3', '4 4', '2', '3', '4', '1 4', '4 3', '4']
12  r_total and score:  264.40000000000043 69.34647072837512
Current Bleu score is:  69.34647072837512
learn step counter:  3301
dev_network_count:  12
learn step counter:  3351
dev_network_count:  12
learn step counter:  3401
dev_network_count:  12
learn step counter:  3451
dev_network_count:  12
learn step counter:  3501
dev_network_count:  12
learn step counter:  3551
dev_network_count:  12

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-2.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])
So far:  [array([2]), array([4]), array([7])]  the state[:3] is:  tensor([-0.9601,  0.9921, -0.7420])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 4 7 3]]
Reward:  [ 1.  -4.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 5 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 4 6 3]]
Reward:  [1.  0.8 0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-2.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '1 3', '4 4', '2', '0', '4', '1 4', '4 3', '4']
13  r_total and score:  257.8000000000004 69.88114688402233
Current Bleu score is:  69.88114688402233
learn step counter:  3601
dev_network_count:  13
learn step counter:  3651
dev_network_count:  13
learn step counter:  3701
dev_network_count:  13
learn step counter:  3751
dev_network_count:  13
learn step counter:  3801
dev_network_count:  13
learn step counter:  3851
dev_network_count:  13

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-2.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])
So far:  [array([2]), array([4]), array([7])]  the state[:3] is:  tensor([-0.9601,  0.9921, -0.7420])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 4 7 3]]
Reward:  [ 1.  -4.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 7 3]]
Reward:  [-2.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 4 6 3]]
Reward:  [1.  0.8 0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-2.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '1 3', '4 4', '2', '3', '4', '1 4', '4 3', '4']
14  r_total and score:  268.8000000000004 66.68320721193214
Current Bleu score is:  66.68320721193214
learn step counter:  3901
dev_network_count:  14
learn step counter:  3951
dev_network_count:  14
EPOCH %d 7
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.4782969000000001 0.4782969000000001
4958 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4959 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4960 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4961 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4962 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4963 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4964 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  4001
dev_network_count:  14
learn step counter:  4051
dev_network_count:  14
learn step counter:  4101
dev_network_count:  14
learn step counter:  4151
dev_network_count:  14

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-2.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])
So far:  [array([2]), array([4]), array([7])]  the state[:3] is:  tensor([-0.9601,  0.9921, -0.7420])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 4 7 3]]
Reward:  [ 1.  -4.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 6 3]]
Reward:  [-2.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-2.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '1 3', '4 4', '2', '4', '4', '4 4', '4 3', '4']
15  r_total and score:  279.20000000000033 52.24489004658875
Current Bleu score is:  52.24489004658875
learn step counter:  4201
dev_network_count:  15
learn step counter:  4251
dev_network_count:  15
learn step counter:  4301
dev_network_count:  15
learn step counter:  4351
dev_network_count:  15
learn step counter:  4401
dev_network_count:  15
learn step counter:  4451
dev_network_count:  15

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-2.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])
So far:  [array([2]), array([4]), array([7])]  the state[:3] is:  tensor([-0.9601,  0.9921, -0.7420])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 4 7 3]]
Reward:  [ 1.  -4.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 6 3]]
Reward:  [-2.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 4 6 3]]
Reward:  [1.  0.8 0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-2.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '1 3', '4 4', '2', '4', '4', '1 4', '4 3', '4']
16  r_total and score:  274.20000000000033 59.30741713350149
Current Bleu score is:  59.30741713350149
learn step counter:  4501
dev_network_count:  16
learn step counter:  4551
dev_network_count:  16
learn step counter:  4601
dev_network_count:  16
learn step counter:  4651
dev_network_count:  16
learn step counter:  4701
dev_network_count:  16
learn step counter:  4751
dev_network_count:  16

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-2.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])
So far:  [array([2]), array([4]), array([7])]  the state[:3] is:  tensor([-0.9601,  0.9921, -0.7420])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 4 7 3]]
Reward:  [ 1.  -4.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 6 3]]
Reward:  [-2.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 4 6 3]]
Reward:  [1.  0.8 0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-2.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '1 3', '4 4', '2', '4', '4', '1 4', '4 3', '4']
17  r_total and score:  274.20000000000033 59.30741713350149
Current Bleu score is:  59.30741713350149
learn step counter:  4801
dev_network_count:  17
learn step counter:  4851
dev_network_count:  17
learn step counter:  4901
dev_network_count:  17
learn step counter:  4951
dev_network_count:  17
EPOCH %d 8
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.4304672100000001 0.4304672100000001
2451 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
2452 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
2453 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
2454 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
2455 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
2456 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
2457 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  5001
dev_network_count:  17
learn step counter:  5051
dev_network_count:  17

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-2.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])
So far:  [array([2]), array([4]), array([7])]  the state[:3] is:  tensor([-0.9601,  0.9921, -0.7420])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 4 7 3]]
Reward:  [ 1.  -4.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 6 3]]
Reward:  [-2.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-2.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '1 3', '4 4', '2', '4', '4', '4 4', '4 3', '4']
18  r_total and score:  272.2000000000004 52.31947714390207
Current Bleu score is:  52.31947714390207
learn step counter:  5101
dev_network_count:  18
learn step counter:  5151
dev_network_count:  18
learn step counter:  5201
dev_network_count:  18
learn step counter:  5251
dev_network_count:  18
learn step counter:  5301
dev_network_count:  18
learn step counter:  5351
dev_network_count:  18

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-2.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])
So far:  [array([2]), array([4]), array([7])]  the state[:3] is:  tensor([-0.9601,  0.9921, -0.7420])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 4 7 3]]
Reward:  [ 1.  -4.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 6 3]]
Reward:  [-2.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-2.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '1 3', '4 4', '2', '4', '4', '4 4', '4 3', '4']
19  r_total and score:  275.80000000000035 51.98300924937251
Current Bleu score is:  51.98300924937251
learn step counter:  5401
dev_network_count:  19
learn step counter:  5451
dev_network_count:  19
learn step counter:  5501
dev_network_count:  19
learn step counter:  5551
dev_network_count:  19
learn step counter:  5601
dev_network_count:  19
learn step counter:  5651
dev_network_count:  19

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])
So far:  [array([2]), array([4]), array([7])]  the state[:3] is:  tensor([-0.9601,  0.9921, -0.7420])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 4 7 3]]
Reward:  [ 1.  -4.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 6 3]]
Reward:  [-2.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1 3', '4 4', '2', '4', '4', '4 4', '4 3', '3']
20  r_total and score:  264.8000000000004 54.79490295253894
Current Bleu score is:  54.79490295253894
learn step counter:  5701
dev_network_count:  20
learn step counter:  5751
dev_network_count:  20
learn step counter:  5801
dev_network_count:  20
learn step counter:  5851
dev_network_count:  20
learn step counter:  5901
dev_network_count:  20
learn step counter:  5951
dev_network_count:  20
EPOCH %d 9
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.3874204890000001 0.3874204890000001
4944 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4945 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4946 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4947 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4948 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4949 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4950 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])
So far:  [array([2]), array([4]), array([7])]  the state[:3] is:  tensor([-0.9601,  0.9921, -0.7420])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 4 7 3]]
Reward:  [ 1.  -4.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 4 7 3]]
Reward:  [ 1.  -4.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1 3', '4 4', '2', '', '4', '1 3', '4', '3']
21  r_total and score:  229.00000000000026 50.18142486417601
Current Bleu score is:  50.18142486417601
learn step counter:  6001
dev_network_count:  21
learn step counter:  6051
dev_network_count:  21
learn step counter:  6101
dev_network_count:  21
learn step counter:  6151
dev_network_count:  21
learn step counter:  6201
dev_network_count:  21
learn step counter:  6251
dev_network_count:  21

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])
So far:  [array([2]), array([4]), array([7])]  the state[:3] is:  tensor([-0.9601,  0.9921, -0.7420])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 4 7 3]]
Reward:  [ 1.  -4.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 6 3]]
Reward:  [-2.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1 3', '4 4', '2', '4', '4', '3 4', '4 3', '3']
22  r_total and score:  261.0000000000004 55.399683972027674
Current Bleu score is:  55.399683972027674
learn step counter:  6301
dev_network_count:  22
learn step counter:  6351
dev_network_count:  22
learn step counter:  6401
dev_network_count:  22
learn step counter:  6451
dev_network_count:  22
learn step counter:  6501
dev_network_count:  22
learn step counter:  6551
dev_network_count:  22

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])
So far:  [array([2]), array([4]), array([7])]  the state[:3] is:  tensor([-0.9601,  0.9921, -0.7420])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 4 7 3]]
Reward:  [ 1.  -4.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 6 3]]
Reward:  [-2.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1 3', '4 4', '2', '4', '4', '4 4', '4 3', '3']
23  r_total and score:  261.0000000000004 55.399683972027674
Current Bleu score is:  55.399683972027674
learn step counter:  6601
dev_network_count:  23
learn step counter:  6651
dev_network_count:  23
learn step counter:  6701
dev_network_count:  23
learn step counter:  6751
dev_network_count:  23
learn step counter:  6801
dev_network_count:  23
learn step counter:  6851
dev_network_count:  23

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])
So far:  [array([2]), array([4]), array([7])]  the state[:3] is:  tensor([-0.9601,  0.9921, -0.7420])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 4 7 3]]
Reward:  [ 1.  -4.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 6 3]]
Reward:  [-2.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1 3', '4 4', '2', '4', '4', '4 4', '4 3', '3']
24  r_total and score:  261.0000000000004 55.399683972027674
Current Bleu score is:  55.399683972027674
learn step counter:  6901
dev_network_count:  24
learn step counter:  6951
dev_network_count:  24
EPOCH %d 10
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.3486784401000001 0.3486784401000001
2437 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
2438 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
2439 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
2440 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
2441 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
2442 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
2443 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  7001
dev_network_count:  24
learn step counter:  7051
dev_network_count:  24
learn step counter:  7101
dev_network_count:  24
learn step counter:  7151
dev_network_count:  24

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])
So far:  [array([2]), array([4]), array([7])]  the state[:3] is:  tensor([-0.9601,  0.9921, -0.7420])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 4 7 3]]
Reward:  [ 1.  -4.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1 3', '4 4', '2', '', '4', '3 4', '4', '3']
25  r_total and score:  222.0000000000003 49.950704447290875
Current Bleu score is:  49.950704447290875
learn step counter:  7201
dev_network_count:  25
learn step counter:  7251
dev_network_count:  25
learn step counter:  7301
dev_network_count:  25
learn step counter:  7351
dev_network_count:  25
learn step counter:  7401
dev_network_count:  25
learn step counter:  7451
dev_network_count:  25

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])
So far:  [array([2]), array([4]), array([7])]  the state[:3] is:  tensor([-0.9601,  0.9921, -0.7420])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 4 7 3]]
Reward:  [ 1.  -4.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1 3', '4 4', '2', '', '4', '3 4', '4', '3']
26  r_total and score:  222.0000000000003 49.950704447290875
Current Bleu score is:  49.950704447290875
learn step counter:  7501
dev_network_count:  26
learn step counter:  7551
dev_network_count:  26
learn step counter:  7601
dev_network_count:  26
learn step counter:  7651
dev_network_count:  26
learn step counter:  7701
dev_network_count:  26
learn step counter:  7751
dev_network_count:  26

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])
So far:  [array([2]), array([4]), array([7])]  the state[:3] is:  tensor([-0.9601,  0.9921, -0.7420])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 4 7 3]]
Reward:  [ 1.  -4.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1 3', '4 4', '2', '', '4', '4 4', '4', '3']
27  r_total and score:  222.0000000000003 49.950704447290875
Current Bleu score is:  49.950704447290875
learn step counter:  7801
dev_network_count:  27
learn step counter:  7851
dev_network_count:  27
learn step counter:  7901
dev_network_count:  27
learn step counter:  7951
dev_network_count:  27
EPOCH %d 11
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.31381059609000006 0.31381059609000006
4930 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4931 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4932 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4933 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4934 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4935 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4936 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  8001
dev_network_count:  27
learn step counter:  8051
dev_network_count:  27

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '4 4', '4', '3']
28  r_total and score:  216.8000000000003 37.79707763349747
Current Bleu score is:  37.79707763349747
learn step counter:  8101
dev_network_count:  28
learn step counter:  8151
dev_network_count:  28
learn step counter:  8201
dev_network_count:  28
learn step counter:  8251
dev_network_count:  28
learn step counter:  8301
dev_network_count:  28
learn step counter:  8351
dev_network_count:  28

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
29  r_total and score:  217.80000000000024 36.840034252847836
Current Bleu score is:  36.840034252847836
learn step counter:  8401
dev_network_count:  29
learn step counter:  8451
dev_network_count:  29
learn step counter:  8501
dev_network_count:  29
learn step counter:  8551
dev_network_count:  29
learn step counter:  8601
dev_network_count:  29
learn step counter:  8651
dev_network_count:  29

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
30  r_total and score:  214.80000000000024 36.840034252847836
Current Bleu score is:  36.840034252847836
learn step counter:  8701
dev_network_count:  30
learn step counter:  8751
dev_network_count:  30
learn step counter:  8801
dev_network_count:  30
learn step counter:  8851
dev_network_count:  30
learn step counter:  8901
dev_network_count:  30
learn step counter:  8951
dev_network_count:  30
EPOCH %d 12
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.2824295364810001 0.3
2423 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
2424 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
2425 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
2426 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
2427 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
2428 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
2429 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
31  r_total and score:  217.80000000000024 36.840034252847836
Current Bleu score is:  36.840034252847836
learn step counter:  9001
dev_network_count:  31
learn step counter:  9051
dev_network_count:  31
learn step counter:  9101
dev_network_count:  31
learn step counter:  9151
dev_network_count:  31
learn step counter:  9201
dev_network_count:  31
learn step counter:  9251
dev_network_count:  31

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
32  r_total and score:  217.80000000000024 36.840034252847836
Current Bleu score is:  36.840034252847836
learn step counter:  9301
dev_network_count:  32
learn step counter:  9351
dev_network_count:  32
learn step counter:  9401
dev_network_count:  32
learn step counter:  9451
dev_network_count:  32
learn step counter:  9501
dev_network_count:  32
learn step counter:  9551
dev_network_count:  32

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
33  r_total and score:  214.80000000000024 36.840034252847836
Current Bleu score is:  36.840034252847836
learn step counter:  9601
dev_network_count:  33
learn step counter:  9651
dev_network_count:  33
learn step counter:  9701
dev_network_count:  33
learn step counter:  9751
dev_network_count:  33
learn step counter:  9801
dev_network_count:  33
learn step counter:  9851
dev_network_count:  33

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
34  r_total and score:  214.80000000000024 36.840034252847836
Current Bleu score is:  36.840034252847836
learn step counter:  9901
dev_network_count:  34
learn step counter:  9951
dev_network_count:  34
EPOCH %d 13
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.2541865828329001 0.3
4916 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4917 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4918 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4919 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4920 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4921 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4922 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  10001
dev_network_count:  34
learn step counter:  10051
dev_network_count:  34
learn step counter:  10101
dev_network_count:  34
learn step counter:  10151
dev_network_count:  34

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
35  r_total and score:  214.80000000000024 36.840034252847836
Current Bleu score is:  36.840034252847836
learn step counter:  10201
dev_network_count:  35
learn step counter:  10251
dev_network_count:  35
learn step counter:  10301
dev_network_count:  35
learn step counter:  10351
dev_network_count:  35
learn step counter:  10401
dev_network_count:  35
learn step counter:  10451
dev_network_count:  35

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
36  r_total and score:  214.80000000000024 36.840034252847836
Current Bleu score is:  36.840034252847836
learn step counter:  10501
dev_network_count:  36
learn step counter:  10551
dev_network_count:  36
learn step counter:  10601
dev_network_count:  36
learn step counter:  10651
dev_network_count:  36
learn step counter:  10701
dev_network_count:  36
learn step counter:  10751
dev_network_count:  36

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
37  r_total and score:  214.80000000000024 36.840034252847836
Current Bleu score is:  36.840034252847836
learn step counter:  10801
dev_network_count:  37
learn step counter:  10851
dev_network_count:  37
learn step counter:  10901
dev_network_count:  37
learn step counter:  10951
dev_network_count:  37
EPOCH %d 14
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.2287679245496101 0.3
2409 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
2410 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
2411 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
2412 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
2413 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
2414 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
2415 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  11001
dev_network_count:  37
learn step counter:  11051
dev_network_count:  37

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
38  r_total and score:  214.80000000000024 36.840034252847836
Current Bleu score is:  36.840034252847836
learn step counter:  11101
dev_network_count:  38
learn step counter:  11151
dev_network_count:  38
learn step counter:  11201
dev_network_count:  38
learn step counter:  11251
dev_network_count:  38
learn step counter:  11301
dev_network_count:  38
learn step counter:  11351
dev_network_count:  38

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
39  r_total and score:  214.80000000000024 36.840034252847836
Current Bleu score is:  36.840034252847836
learn step counter:  11401
dev_network_count:  39
learn step counter:  11451
dev_network_count:  39
learn step counter:  11501
dev_network_count:  39
learn step counter:  11551
dev_network_count:  39
learn step counter:  11601
dev_network_count:  39
learn step counter:  11651
dev_network_count:  39

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
40  r_total and score:  214.80000000000024 36.840034252847836
Current Bleu score is:  36.840034252847836
learn step counter:  11701
dev_network_count:  40
learn step counter:  11751
dev_network_count:  40
learn step counter:  11801
dev_network_count:  40
learn step counter:  11851
dev_network_count:  40
learn step counter:  11901
dev_network_count:  40
learn step counter:  11951
dev_network_count:  40
EPOCH %d 15
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.20589113209464907 0.3
4902 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4903 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4904 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4905 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4906 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4907 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4908 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
41  r_total and score:  214.80000000000024 36.840034252847836
Current Bleu score is:  36.840034252847836
learn step counter:  12001
dev_network_count:  41
learn step counter:  12051
dev_network_count:  41
learn step counter:  12101
dev_network_count:  41
learn step counter:  12151
dev_network_count:  41
learn step counter:  12201
dev_network_count:  41
learn step counter:  12251
dev_network_count:  41

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
42  r_total and score:  214.80000000000024 36.840034252847836
Current Bleu score is:  36.840034252847836
learn step counter:  12301
dev_network_count:  42
learn step counter:  12351
dev_network_count:  42
learn step counter:  12401
dev_network_count:  42
learn step counter:  12451
dev_network_count:  42
learn step counter:  12501
dev_network_count:  42
learn step counter:  12551
dev_network_count:  42

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '4 3', '4', '3']
43  r_total and score:  217.80000000000024 31.746272569217034
Current Bleu score is:  31.746272569217034
learn step counter:  12601
dev_network_count:  43
learn step counter:  12651
dev_network_count:  43
learn step counter:  12701
dev_network_count:  43
learn step counter:  12751
dev_network_count:  43
learn step counter:  12801
dev_network_count:  43
learn step counter:  12851
dev_network_count:  43

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
44  r_total and score:  217.80000000000024 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  12901
dev_network_count:  44
learn step counter:  12951
dev_network_count:  44
EPOCH %d 16
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.18530201888518416 0.3
2395 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
2396 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
2397 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
2398 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
2399 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
2400 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
2401 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  13001
dev_network_count:  44
learn step counter:  13051
dev_network_count:  44
learn step counter:  13101
dev_network_count:  44
learn step counter:  13151
dev_network_count:  44

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
45  r_total and score:  217.80000000000024 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  13201
dev_network_count:  45
learn step counter:  13251
dev_network_count:  45
learn step counter:  13301
dev_network_count:  45
learn step counter:  13351
dev_network_count:  45
learn step counter:  13401
dev_network_count:  45
learn step counter:  13451
dev_network_count:  45

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
46  r_total and score:  217.80000000000024 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  13501
dev_network_count:  46
learn step counter:  13551
dev_network_count:  46
learn step counter:  13601
dev_network_count:  46
learn step counter:  13651
dev_network_count:  46
learn step counter:  13701
dev_network_count:  46
learn step counter:  13751
dev_network_count:  46

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
47  r_total and score:  217.80000000000024 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  13801
dev_network_count:  47
learn step counter:  13851
dev_network_count:  47
learn step counter:  13901
dev_network_count:  47
learn step counter:  13951
dev_network_count:  47
EPOCH %d 17
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.16677181699666577 0.3
4888 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4889 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4890 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4891 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4892 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4893 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4894 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  14001
dev_network_count:  47
learn step counter:  14051
dev_network_count:  47

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
48  r_total and score:  217.80000000000024 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  14101
dev_network_count:  48
learn step counter:  14151
dev_network_count:  48
learn step counter:  14201
dev_network_count:  48
learn step counter:  14251
dev_network_count:  48
learn step counter:  14301
dev_network_count:  48
learn step counter:  14351
dev_network_count:  48

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
49  r_total and score:  217.80000000000024 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  14401
dev_network_count:  49
learn step counter:  14451
dev_network_count:  49
learn step counter:  14501
dev_network_count:  49
learn step counter:  14551
dev_network_count:  49
learn step counter:  14601
dev_network_count:  49
learn step counter:  14651
dev_network_count:  49

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
50  r_total and score:  217.80000000000024 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  14701
dev_network_count:  50
learn step counter:  14751
dev_network_count:  50
learn step counter:  14801
dev_network_count:  50
learn step counter:  14851
dev_network_count:  50
learn step counter:  14901
dev_network_count:  50
learn step counter:  14951
dev_network_count:  50
EPOCH %d 18
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.15009463529699918 0.3
2381 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
2382 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
2383 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
2384 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
2385 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
2386 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
2387 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
51  r_total and score:  217.80000000000024 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  15001
dev_network_count:  51
learn step counter:  15051
dev_network_count:  51
learn step counter:  15101
dev_network_count:  51
learn step counter:  15151
dev_network_count:  51
learn step counter:  15201
dev_network_count:  51
learn step counter:  15251
dev_network_count:  51

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
52  r_total and score:  217.80000000000024 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  15301
dev_network_count:  52
learn step counter:  15351
dev_network_count:  52
learn step counter:  15401
dev_network_count:  52
learn step counter:  15451
dev_network_count:  52
learn step counter:  15501
dev_network_count:  52
learn step counter:  15551
dev_network_count:  52

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
53  r_total and score:  217.80000000000024 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  15601
dev_network_count:  53
learn step counter:  15651
dev_network_count:  53
learn step counter:  15701
dev_network_count:  53
learn step counter:  15751
dev_network_count:  53
learn step counter:  15801
dev_network_count:  53
learn step counter:  15851
dev_network_count:  53

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
54  r_total and score:  217.80000000000024 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  15901
dev_network_count:  54
learn step counter:  15951
dev_network_count:  54
EPOCH %d 19
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.13508517176729928 0.3
4874 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4875 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4876 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4877 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4878 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4879 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4880 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  16001
dev_network_count:  54
learn step counter:  16051
dev_network_count:  54
learn step counter:  16101
dev_network_count:  54
learn step counter:  16151
dev_network_count:  54

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
55  r_total and score:  217.80000000000024 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  16201
dev_network_count:  55
learn step counter:  16251
dev_network_count:  55
learn step counter:  16301
dev_network_count:  55
learn step counter:  16351
dev_network_count:  55
learn step counter:  16401
dev_network_count:  55
learn step counter:  16451
dev_network_count:  55

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
56  r_total and score:  217.80000000000024 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  16501
dev_network_count:  56
learn step counter:  16551
dev_network_count:  56
learn step counter:  16601
dev_network_count:  56
learn step counter:  16651
dev_network_count:  56
learn step counter:  16701
dev_network_count:  56
learn step counter:  16751
dev_network_count:  56

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
57  r_total and score:  217.80000000000024 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  16801
dev_network_count:  57
learn step counter:  16851
dev_network_count:  57
learn step counter:  16901
dev_network_count:  57
learn step counter:  16951
dev_network_count:  57
EPOCH %d 20
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.12157665459056935 0.3
2367 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
2368 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
2369 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
2370 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
2371 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
2372 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
2373 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  17001
dev_network_count:  57
learn step counter:  17051
dev_network_count:  57

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
58  r_total and score:  217.80000000000024 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  17101
dev_network_count:  58
learn step counter:  17151
dev_network_count:  58
learn step counter:  17201
dev_network_count:  58
learn step counter:  17251
dev_network_count:  58
learn step counter:  17301
dev_network_count:  58
learn step counter:  17351
dev_network_count:  58

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
59  r_total and score:  217.80000000000024 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  17401
dev_network_count:  59
learn step counter:  17451
dev_network_count:  59
learn step counter:  17501
dev_network_count:  59
learn step counter:  17551
dev_network_count:  59
learn step counter:  17601
dev_network_count:  59
learn step counter:  17651
dev_network_count:  59

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
60  r_total and score:  217.80000000000024 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  17701
dev_network_count:  60
learn step counter:  17751
dev_network_count:  60
learn step counter:  17801
dev_network_count:  60
learn step counter:  17851
dev_network_count:  60
learn step counter:  17901
dev_network_count:  60
learn step counter:  17951
dev_network_count:  60
EPOCH %d 21
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.10941898913151242 0.3
4860 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4861 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4862 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4863 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4864 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4865 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4866 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
61  r_total and score:  217.80000000000024 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  18001
dev_network_count:  61
learn step counter:  18051
dev_network_count:  61
learn step counter:  18101
dev_network_count:  61
learn step counter:  18151
dev_network_count:  61
learn step counter:  18201
dev_network_count:  61
learn step counter:  18251
dev_network_count:  61

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
62  r_total and score:  217.80000000000024 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  18301
dev_network_count:  62
learn step counter:  18351
dev_network_count:  62
learn step counter:  18401
dev_network_count:  62
learn step counter:  18451
dev_network_count:  62
learn step counter:  18501
dev_network_count:  62
learn step counter:  18551
dev_network_count:  62

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
63  r_total and score:  217.80000000000024 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  18601
dev_network_count:  63
learn step counter:  18651
dev_network_count:  63
learn step counter:  18701
dev_network_count:  63
learn step counter:  18751
dev_network_count:  63
learn step counter:  18801
dev_network_count:  63
learn step counter:  18851
dev_network_count:  63

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
64  r_total and score:  217.80000000000024 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  18901
dev_network_count:  64
learn step counter:  18951
dev_network_count:  64
EPOCH %d 22
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.09847709021836118 0.3
2353 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
2354 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
2355 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
2356 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
2357 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
2358 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
2359 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  19001
dev_network_count:  64
learn step counter:  19051
dev_network_count:  64
learn step counter:  19101
dev_network_count:  64
learn step counter:  19151
dev_network_count:  64

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
65  r_total and score:  217.80000000000024 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  19201
dev_network_count:  65
learn step counter:  19251
dev_network_count:  65
learn step counter:  19301
dev_network_count:  65
learn step counter:  19351
dev_network_count:  65
learn step counter:  19401
dev_network_count:  65
learn step counter:  19451
dev_network_count:  65

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
66  r_total and score:  217.80000000000024 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  19501
dev_network_count:  66
learn step counter:  19551
dev_network_count:  66
learn step counter:  19601
dev_network_count:  66
learn step counter:  19651
dev_network_count:  66
learn step counter:  19701
dev_network_count:  66
learn step counter:  19751
dev_network_count:  66

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
67  r_total and score:  217.80000000000024 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  19801
dev_network_count:  67
learn step counter:  19851
dev_network_count:  67
learn step counter:  19901
dev_network_count:  67
learn step counter:  19951
dev_network_count:  67
EPOCH %d 23
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.08862938119652507 0.3
4846 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4847 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4848 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4849 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4850 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4851 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4852 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  20001
dev_network_count:  67
learn step counter:  20051
dev_network_count:  67

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
68  r_total and score:  221.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  20101
dev_network_count:  68
learn step counter:  20151
dev_network_count:  68
learn step counter:  20201
dev_network_count:  68
learn step counter:  20251
dev_network_count:  68
learn step counter:  20301
dev_network_count:  68
learn step counter:  20351
dev_network_count:  68

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
69  r_total and score:  217.80000000000024 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  20401
dev_network_count:  69
learn step counter:  20451
dev_network_count:  69
learn step counter:  20501
dev_network_count:  69
learn step counter:  20551
dev_network_count:  69
learn step counter:  20601
dev_network_count:  69
learn step counter:  20651
dev_network_count:  69

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
70  r_total and score:  217.80000000000024 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  20701
dev_network_count:  70
learn step counter:  20751
dev_network_count:  70
learn step counter:  20801
dev_network_count:  70
learn step counter:  20851
dev_network_count:  70
learn step counter:  20901
dev_network_count:  70
learn step counter:  20951
dev_network_count:  70
EPOCH %d 24
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.07976644307687256 0.3
2339 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
2340 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
2341 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
2342 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
2343 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
2344 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
2345 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
71  r_total and score:  217.80000000000024 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  21001
dev_network_count:  71
learn step counter:  21051
dev_network_count:  71
learn step counter:  21101
dev_network_count:  71
learn step counter:  21151
dev_network_count:  71
learn step counter:  21201
dev_network_count:  71
learn step counter:  21251
dev_network_count:  71

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
72  r_total and score:  217.80000000000024 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  21301
dev_network_count:  72
learn step counter:  21351
dev_network_count:  72
learn step counter:  21401
dev_network_count:  72
learn step counter:  21451
dev_network_count:  72
learn step counter:  21501
dev_network_count:  72
learn step counter:  21551
dev_network_count:  72

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
73  r_total and score:  217.80000000000024 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  21601
dev_network_count:  73
learn step counter:  21651
dev_network_count:  73
learn step counter:  21701
dev_network_count:  73
learn step counter:  21751
dev_network_count:  73
learn step counter:  21801
dev_network_count:  73
learn step counter:  21851
dev_network_count:  73

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
74  r_total and score:  217.80000000000024 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  21901
dev_network_count:  74
learn step counter:  21951
dev_network_count:  74
EPOCH %d 25
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.0717897987691853 0.3
4832 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4833 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4834 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4835 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4836 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4837 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4838 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  22001
dev_network_count:  74
learn step counter:  22051
dev_network_count:  74
learn step counter:  22101
dev_network_count:  74
learn step counter:  22151
dev_network_count:  74

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
75  r_total and score:  217.80000000000024 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  22201
dev_network_count:  75
learn step counter:  22251
dev_network_count:  75
learn step counter:  22301
dev_network_count:  75
learn step counter:  22351
dev_network_count:  75
learn step counter:  22401
dev_network_count:  75
learn step counter:  22451
dev_network_count:  75

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
76  r_total and score:  217.80000000000024 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  22501
dev_network_count:  76
learn step counter:  22551
dev_network_count:  76
learn step counter:  22601
dev_network_count:  76
learn step counter:  22651
dev_network_count:  76
learn step counter:  22701
dev_network_count:  76
learn step counter:  22751
dev_network_count:  76

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
77  r_total and score:  218.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  22801
dev_network_count:  77
learn step counter:  22851
dev_network_count:  77
learn step counter:  22901
dev_network_count:  77
learn step counter:  22951
dev_network_count:  77
EPOCH %d 26
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.06461081889226677 0.3
2325 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
2326 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
2327 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
2328 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
2329 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
2330 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
2331 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  23001
dev_network_count:  77
learn step counter:  23051
dev_network_count:  77

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
78  r_total and score:  218.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  23101
dev_network_count:  78
learn step counter:  23151
dev_network_count:  78
learn step counter:  23201
dev_network_count:  78
learn step counter:  23251
dev_network_count:  78
learn step counter:  23301
dev_network_count:  78
learn step counter:  23351
dev_network_count:  78

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
79  r_total and score:  218.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  23401
dev_network_count:  79
learn step counter:  23451
dev_network_count:  79
learn step counter:  23501
dev_network_count:  79
learn step counter:  23551
dev_network_count:  79
learn step counter:  23601
dev_network_count:  79
learn step counter:  23651
dev_network_count:  79

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
80  r_total and score:  218.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  23701
dev_network_count:  80
learn step counter:  23751
dev_network_count:  80
learn step counter:  23801
dev_network_count:  80
learn step counter:  23851
dev_network_count:  80
learn step counter:  23901
dev_network_count:  80
learn step counter:  23951
dev_network_count:  80
EPOCH %d 27
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.058149737003040096 0.3
4818 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4819 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4820 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4821 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4822 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4823 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4824 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
81  r_total and score:  218.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  24001
dev_network_count:  81
learn step counter:  24051
dev_network_count:  81
learn step counter:  24101
dev_network_count:  81
learn step counter:  24151
dev_network_count:  81
learn step counter:  24201
dev_network_count:  81
learn step counter:  24251
dev_network_count:  81

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
82  r_total and score:  218.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  24301
dev_network_count:  82
learn step counter:  24351
dev_network_count:  82
learn step counter:  24401
dev_network_count:  82
learn step counter:  24451
dev_network_count:  82
learn step counter:  24501
dev_network_count:  82
learn step counter:  24551
dev_network_count:  82

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
83  r_total and score:  217.80000000000024 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  24601
dev_network_count:  83
learn step counter:  24651
dev_network_count:  83
learn step counter:  24701
dev_network_count:  83
learn step counter:  24751
dev_network_count:  83
learn step counter:  24801
dev_network_count:  83
learn step counter:  24851
dev_network_count:  83

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
84  r_total and score:  218.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  24901
dev_network_count:  84
learn step counter:  24951
dev_network_count:  84
EPOCH %d 28
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.05233476330273609 0.3
2311 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
2312 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
2313 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
2314 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
2315 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
2316 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
2317 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  25001
dev_network_count:  84
learn step counter:  25051
dev_network_count:  84
learn step counter:  25101
dev_network_count:  84
learn step counter:  25151
dev_network_count:  84

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
85  r_total and score:  217.80000000000024 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  25201
dev_network_count:  85
learn step counter:  25251
dev_network_count:  85
learn step counter:  25301
dev_network_count:  85
learn step counter:  25351
dev_network_count:  85
learn step counter:  25401
dev_network_count:  85
learn step counter:  25451
dev_network_count:  85

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
86  r_total and score:  218.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  25501
dev_network_count:  86
learn step counter:  25551
dev_network_count:  86
learn step counter:  25601
dev_network_count:  86
learn step counter:  25651
dev_network_count:  86
learn step counter:  25701
dev_network_count:  86
learn step counter:  25751
dev_network_count:  86

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
87  r_total and score:  218.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  25801
dev_network_count:  87
learn step counter:  25851
dev_network_count:  87
learn step counter:  25901
dev_network_count:  87
learn step counter:  25951
dev_network_count:  87
EPOCH %d 29
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.047101286972462485 0.3
4804 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4805 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4806 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4807 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4808 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4809 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4810 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  26001
dev_network_count:  87
learn step counter:  26051
dev_network_count:  87

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
88  r_total and score:  218.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  26101
dev_network_count:  88
learn step counter:  26151
dev_network_count:  88
learn step counter:  26201
dev_network_count:  88
learn step counter:  26251
dev_network_count:  88
learn step counter:  26301
dev_network_count:  88
learn step counter:  26351
dev_network_count:  88

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
89  r_total and score:  218.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  26401
dev_network_count:  89
learn step counter:  26451
dev_network_count:  89
learn step counter:  26501
dev_network_count:  89
learn step counter:  26551
dev_network_count:  89
learn step counter:  26601
dev_network_count:  89
learn step counter:  26651
dev_network_count:  89

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
90  r_total and score:  218.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  26701
dev_network_count:  90
learn step counter:  26751
dev_network_count:  90
learn step counter:  26801
dev_network_count:  90
learn step counter:  26851
dev_network_count:  90
learn step counter:  26901
dev_network_count:  90
learn step counter:  26951
dev_network_count:  90
EPOCH %d 30
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.04239115827521624 0.3
2297 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
2298 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
2299 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
2300 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
2301 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
2302 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
2303 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
91  r_total and score:  218.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  27001
dev_network_count:  91
learn step counter:  27051
dev_network_count:  91
learn step counter:  27101
dev_network_count:  91
learn step counter:  27151
dev_network_count:  91
learn step counter:  27201
dev_network_count:  91
learn step counter:  27251
dev_network_count:  91

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
92  r_total and score:  217.80000000000024 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  27301
dev_network_count:  92
learn step counter:  27351
dev_network_count:  92
learn step counter:  27401
dev_network_count:  92
learn step counter:  27451
dev_network_count:  92
learn step counter:  27501
dev_network_count:  92
learn step counter:  27551
dev_network_count:  92

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
93  r_total and score:  221.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  27601
dev_network_count:  93
learn step counter:  27651
dev_network_count:  93
learn step counter:  27701
dev_network_count:  93
learn step counter:  27751
dev_network_count:  93
learn step counter:  27801
dev_network_count:  93
learn step counter:  27851
dev_network_count:  93

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
94  r_total and score:  218.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  27901
dev_network_count:  94
learn step counter:  27951
dev_network_count:  94
EPOCH %d 31
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.038152042447694615 0.3
4790 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4791 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4792 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4793 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4794 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4795 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4796 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  28001
dev_network_count:  94
learn step counter:  28051
dev_network_count:  94
learn step counter:  28101
dev_network_count:  94
learn step counter:  28151
dev_network_count:  94

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
95  r_total and score:  218.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  28201
dev_network_count:  95
learn step counter:  28251
dev_network_count:  95
learn step counter:  28301
dev_network_count:  95
learn step counter:  28351
dev_network_count:  95
learn step counter:  28401
dev_network_count:  95
learn step counter:  28451
dev_network_count:  95

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
96  r_total and score:  218.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  28501
dev_network_count:  96
learn step counter:  28551
dev_network_count:  96
learn step counter:  28601
dev_network_count:  96
learn step counter:  28651
dev_network_count:  96
learn step counter:  28701
dev_network_count:  96
learn step counter:  28751
dev_network_count:  96

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
97  r_total and score:  218.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  28801
dev_network_count:  97
learn step counter:  28851
dev_network_count:  97
learn step counter:  28901
dev_network_count:  97
learn step counter:  28951
dev_network_count:  97
EPOCH %d 32
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.03433683820292515 0.3
2283 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
2284 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
2285 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
2286 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
2287 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
2288 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
2289 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  29001
dev_network_count:  97
learn step counter:  29051
dev_network_count:  97

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
98  r_total and score:  218.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  29101
dev_network_count:  98
learn step counter:  29151
dev_network_count:  98
learn step counter:  29201
dev_network_count:  98
learn step counter:  29251
dev_network_count:  98
learn step counter:  29301
dev_network_count:  98
learn step counter:  29351
dev_network_count:  98

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
99  r_total and score:  218.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  29401
dev_network_count:  99
learn step counter:  29451
dev_network_count:  99
learn step counter:  29501
dev_network_count:  99
learn step counter:  29551
dev_network_count:  99
learn step counter:  29601
dev_network_count:  99
learn step counter:  29651
dev_network_count:  99

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
100  r_total and score:  218.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  29701
dev_network_count:  100
learn step counter:  29751
dev_network_count:  100
learn step counter:  29801
dev_network_count:  100
learn step counter:  29851
dev_network_count:  100
learn step counter:  29901
dev_network_count:  100
learn step counter:  29951
dev_network_count:  100
EPOCH %d 33
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.030903154382632636 0.3
4776 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4777 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4778 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4779 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4780 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4781 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4782 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
101  r_total and score:  218.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  30001
dev_network_count:  101
learn step counter:  30051
dev_network_count:  101
learn step counter:  30101
dev_network_count:  101
learn step counter:  30151
dev_network_count:  101
learn step counter:  30201
dev_network_count:  101
learn step counter:  30251
dev_network_count:  101

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
102  r_total and score:  218.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  30301
dev_network_count:  102
learn step counter:  30351
dev_network_count:  102
learn step counter:  30401
dev_network_count:  102
learn step counter:  30451
dev_network_count:  102
learn step counter:  30501
dev_network_count:  102
learn step counter:  30551
dev_network_count:  102

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
103  r_total and score:  218.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  30601
dev_network_count:  103
learn step counter:  30651
dev_network_count:  103
learn step counter:  30701
dev_network_count:  103
learn step counter:  30751
dev_network_count:  103
learn step counter:  30801
dev_network_count:  103
learn step counter:  30851
dev_network_count:  103

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
104  r_total and score:  218.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  30901
dev_network_count:  104
learn step counter:  30951
dev_network_count:  104
EPOCH %d 34
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.027812838944369374 0.3
2269 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
2270 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
2271 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
2272 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
2273 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
2274 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
2275 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  31001
dev_network_count:  104
learn step counter:  31051
dev_network_count:  104
learn step counter:  31101
dev_network_count:  104
learn step counter:  31151
dev_network_count:  104

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
105  r_total and score:  218.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  31201
dev_network_count:  105
learn step counter:  31251
dev_network_count:  105
learn step counter:  31301
dev_network_count:  105
learn step counter:  31351
dev_network_count:  105
learn step counter:  31401
dev_network_count:  105
learn step counter:  31451
dev_network_count:  105

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
106  r_total and score:  221.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  31501
dev_network_count:  106
learn step counter:  31551
dev_network_count:  106
learn step counter:  31601
dev_network_count:  106
learn step counter:  31651
dev_network_count:  106
learn step counter:  31701
dev_network_count:  106
learn step counter:  31751
dev_network_count:  106

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
107  r_total and score:  221.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  31801
dev_network_count:  107
learn step counter:  31851
dev_network_count:  107
learn step counter:  31901
dev_network_count:  107
learn step counter:  31951
dev_network_count:  107
EPOCH %d 35
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.025031555049932437 0.3
4762 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4763 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4764 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4765 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4766 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4767 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4768 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  32001
dev_network_count:  107
learn step counter:  32051
dev_network_count:  107

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
108  r_total and score:  218.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  32101
dev_network_count:  108
learn step counter:  32151
dev_network_count:  108
learn step counter:  32201
dev_network_count:  108
learn step counter:  32251
dev_network_count:  108
learn step counter:  32301
dev_network_count:  108
learn step counter:  32351
dev_network_count:  108

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
109  r_total and score:  218.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  32401
dev_network_count:  109
learn step counter:  32451
dev_network_count:  109
learn step counter:  32501
dev_network_count:  109
learn step counter:  32551
dev_network_count:  109
learn step counter:  32601
dev_network_count:  109
learn step counter:  32651
dev_network_count:  109

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
110  r_total and score:  218.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  32701
dev_network_count:  110
learn step counter:  32751
dev_network_count:  110
learn step counter:  32801
dev_network_count:  110
learn step counter:  32851
dev_network_count:  110
learn step counter:  32901
dev_network_count:  110
learn step counter:  32951
dev_network_count:  110
EPOCH %d 36
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.022528399544939195 0.3
2255 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
2256 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
2257 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
2258 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
2259 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
2260 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
2261 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
111  r_total and score:  218.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  33001
dev_network_count:  111
learn step counter:  33051
dev_network_count:  111
learn step counter:  33101
dev_network_count:  111
learn step counter:  33151
dev_network_count:  111
learn step counter:  33201
dev_network_count:  111
learn step counter:  33251
dev_network_count:  111

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
112  r_total and score:  218.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  33301
dev_network_count:  112
learn step counter:  33351
dev_network_count:  112
learn step counter:  33401
dev_network_count:  112
learn step counter:  33451
dev_network_count:  112
learn step counter:  33501
dev_network_count:  112
learn step counter:  33551
dev_network_count:  112

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
113  r_total and score:  218.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  33601
dev_network_count:  113
learn step counter:  33651
dev_network_count:  113
learn step counter:  33701
dev_network_count:  113
learn step counter:  33751
dev_network_count:  113
learn step counter:  33801
dev_network_count:  113
learn step counter:  33851
dev_network_count:  113

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
114  r_total and score:  218.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  33901
dev_network_count:  114
learn step counter:  33951
dev_network_count:  114
EPOCH %d 37
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.020275559590445275 0.3
4748 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4749 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4750 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4751 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4752 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4753 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4754 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  34001
dev_network_count:  114
learn step counter:  34051
dev_network_count:  114
learn step counter:  34101
dev_network_count:  114
learn step counter:  34151
dev_network_count:  114

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
115  r_total and score:  218.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  34201
dev_network_count:  115
learn step counter:  34251
dev_network_count:  115
learn step counter:  34301
dev_network_count:  115
learn step counter:  34351
dev_network_count:  115
learn step counter:  34401
dev_network_count:  115
learn step counter:  34451
dev_network_count:  115

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
116  r_total and score:  218.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  34501
dev_network_count:  116
learn step counter:  34551
dev_network_count:  116
learn step counter:  34601
dev_network_count:  116
learn step counter:  34651
dev_network_count:  116
learn step counter:  34701
dev_network_count:  116
learn step counter:  34751
dev_network_count:  116

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
117  r_total and score:  221.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  34801
dev_network_count:  117
learn step counter:  34851
dev_network_count:  117
learn step counter:  34901
dev_network_count:  117
learn step counter:  34951
dev_network_count:  117
EPOCH %d 38
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.01824800363140075 0.3
2241 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
2242 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
2243 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
2244 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
2245 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
2246 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
2247 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  35001
dev_network_count:  117
learn step counter:  35051
dev_network_count:  117

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
118  r_total and score:  221.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  35101
dev_network_count:  118
learn step counter:  35151
dev_network_count:  118
learn step counter:  35201
dev_network_count:  118
learn step counter:  35251
dev_network_count:  118
learn step counter:  35301
dev_network_count:  118
learn step counter:  35351
dev_network_count:  118

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
119  r_total and score:  221.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  35401
dev_network_count:  119
learn step counter:  35451
dev_network_count:  119
learn step counter:  35501
dev_network_count:  119
learn step counter:  35551
dev_network_count:  119
learn step counter:  35601
dev_network_count:  119
learn step counter:  35651
dev_network_count:  119

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
120  r_total and score:  221.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  35701
dev_network_count:  120
learn step counter:  35751
dev_network_count:  120
learn step counter:  35801
dev_network_count:  120
learn step counter:  35851
dev_network_count:  120
learn step counter:  35901
dev_network_count:  120
learn step counter:  35951
dev_network_count:  120
EPOCH %d 39
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.016423203268260675 0.3
4734 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4735 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4736 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4737 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4738 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4739 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4740 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
121  r_total and score:  221.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  36001
dev_network_count:  121
learn step counter:  36051
dev_network_count:  121
learn step counter:  36101
dev_network_count:  121
learn step counter:  36151
dev_network_count:  121
learn step counter:  36201
dev_network_count:  121
learn step counter:  36251
dev_network_count:  121

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
122  r_total and score:  218.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  36301
dev_network_count:  122
learn step counter:  36351
dev_network_count:  122
learn step counter:  36401
dev_network_count:  122
learn step counter:  36451
dev_network_count:  122
learn step counter:  36501
dev_network_count:  122
learn step counter:  36551
dev_network_count:  122

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
123  r_total and score:  218.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  36601
dev_network_count:  123
learn step counter:  36651
dev_network_count:  123
learn step counter:  36701
dev_network_count:  123
learn step counter:  36751
dev_network_count:  123
learn step counter:  36801
dev_network_count:  123
learn step counter:  36851
dev_network_count:  123

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
124  r_total and score:  218.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  36901
dev_network_count:  124
learn step counter:  36951
dev_network_count:  124
EPOCH %d 40
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.014780882941434608 0.3
2227 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
2228 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
2229 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
2230 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
2231 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
2232 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
2233 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  37001
dev_network_count:  124
learn step counter:  37051
dev_network_count:  124
learn step counter:  37101
dev_network_count:  124
learn step counter:  37151
dev_network_count:  124

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
125  r_total and score:  221.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  37201
dev_network_count:  125
learn step counter:  37251
dev_network_count:  125
learn step counter:  37301
dev_network_count:  125
learn step counter:  37351
dev_network_count:  125
learn step counter:  37401
dev_network_count:  125
learn step counter:  37451
dev_network_count:  125

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
126  r_total and score:  221.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  37501
dev_network_count:  126
learn step counter:  37551
dev_network_count:  126
learn step counter:  37601
dev_network_count:  126
learn step counter:  37651
dev_network_count:  126
learn step counter:  37701
dev_network_count:  126
learn step counter:  37751
dev_network_count:  126

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
127  r_total and score:  221.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  37801
dev_network_count:  127
learn step counter:  37851
dev_network_count:  127
learn step counter:  37901
dev_network_count:  127
learn step counter:  37951
dev_network_count:  127
EPOCH %d 41
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.013302794647291146 0.3
4720 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4721 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4722 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4723 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4724 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4725 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4726 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  38001
dev_network_count:  127
learn step counter:  38051
dev_network_count:  127

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
128  r_total and score:  221.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  38101
dev_network_count:  128
learn step counter:  38151
dev_network_count:  128
learn step counter:  38201
dev_network_count:  128
learn step counter:  38251
dev_network_count:  128
learn step counter:  38301
dev_network_count:  128
learn step counter:  38351
dev_network_count:  128

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
129  r_total and score:  218.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  38401
dev_network_count:  129
learn step counter:  38451
dev_network_count:  129
learn step counter:  38501
dev_network_count:  129
learn step counter:  38551
dev_network_count:  129
learn step counter:  38601
dev_network_count:  129
learn step counter:  38651
dev_network_count:  129

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
130  r_total and score:  221.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  38701
dev_network_count:  130
learn step counter:  38751
dev_network_count:  130
learn step counter:  38801
dev_network_count:  130
learn step counter:  38851
dev_network_count:  130
learn step counter:  38901
dev_network_count:  130
learn step counter:  38951
dev_network_count:  130
EPOCH %d 42
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.011972515182562033 0.3
2213 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
2214 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
2215 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
2216 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
2217 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
2218 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
2219 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
131  r_total and score:  221.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  39001
dev_network_count:  131
learn step counter:  39051
dev_network_count:  131
learn step counter:  39101
dev_network_count:  131
learn step counter:  39151
dev_network_count:  131
learn step counter:  39201
dev_network_count:  131
learn step counter:  39251
dev_network_count:  131

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
132  r_total and score:  221.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  39301
dev_network_count:  132
learn step counter:  39351
dev_network_count:  132
learn step counter:  39401
dev_network_count:  132
learn step counter:  39451
dev_network_count:  132
learn step counter:  39501
dev_network_count:  132
learn step counter:  39551
dev_network_count:  132

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
133  r_total and score:  221.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  39601
dev_network_count:  133
learn step counter:  39651
dev_network_count:  133
learn step counter:  39701
dev_network_count:  133
learn step counter:  39751
dev_network_count:  133
learn step counter:  39801
dev_network_count:  133
learn step counter:  39851
dev_network_count:  133

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
134  r_total and score:  221.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  39901
dev_network_count:  134
learn step counter:  39951
dev_network_count:  134
EPOCH %d 43
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.01077526366430583 0.3
4706 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4707 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4708 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4709 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4710 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4711 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4712 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  40001
dev_network_count:  134
learn step counter:  40051
dev_network_count:  134
learn step counter:  40101
dev_network_count:  134
learn step counter:  40151
dev_network_count:  134

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
135  r_total and score:  221.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  40201
dev_network_count:  135
learn step counter:  40251
dev_network_count:  135
learn step counter:  40301
dev_network_count:  135
learn step counter:  40351
dev_network_count:  135
learn step counter:  40401
dev_network_count:  135
learn step counter:  40451
dev_network_count:  135

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
136  r_total and score:  221.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  40501
dev_network_count:  136
learn step counter:  40551
dev_network_count:  136
learn step counter:  40601
dev_network_count:  136
learn step counter:  40651
dev_network_count:  136
learn step counter:  40701
dev_network_count:  136
learn step counter:  40751
dev_network_count:  136

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
137  r_total and score:  221.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  40801
dev_network_count:  137
learn step counter:  40851
dev_network_count:  137
learn step counter:  40901
dev_network_count:  137
learn step counter:  40951
dev_network_count:  137
EPOCH %d 44
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.009697737297875247 0.3
2199 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
2200 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
2201 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
2202 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
2203 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
2204 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
2205 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  41001
dev_network_count:  137
learn step counter:  41051
dev_network_count:  137

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
138  r_total and score:  221.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  41101
dev_network_count:  138
learn step counter:  41151
dev_network_count:  138
learn step counter:  41201
dev_network_count:  138
learn step counter:  41251
dev_network_count:  138
learn step counter:  41301
dev_network_count:  138
learn step counter:  41351
dev_network_count:  138

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
139  r_total and score:  221.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  41401
dev_network_count:  139
learn step counter:  41451
dev_network_count:  139
learn step counter:  41501
dev_network_count:  139
learn step counter:  41551
dev_network_count:  139
learn step counter:  41601
dev_network_count:  139
learn step counter:  41651
dev_network_count:  139

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
140  r_total and score:  221.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  41701
dev_network_count:  140
learn step counter:  41751
dev_network_count:  140
learn step counter:  41801
dev_network_count:  140
learn step counter:  41851
dev_network_count:  140
learn step counter:  41901
dev_network_count:  140
learn step counter:  41951
dev_network_count:  140
EPOCH %d 45
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.008727963568087723 0.3
4692 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4693 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4694 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4695 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4696 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4697 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4698 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
141  r_total and score:  221.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  42001
dev_network_count:  141
learn step counter:  42051
dev_network_count:  141
learn step counter:  42101
dev_network_count:  141
learn step counter:  42151
dev_network_count:  141
learn step counter:  42201
dev_network_count:  141
learn step counter:  42251
dev_network_count:  141

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
142  r_total and score:  221.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  42301
dev_network_count:  142
learn step counter:  42351
dev_network_count:  142
learn step counter:  42401
dev_network_count:  142
learn step counter:  42451
dev_network_count:  142
learn step counter:  42501
dev_network_count:  142
learn step counter:  42551
dev_network_count:  142

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
143  r_total and score:  221.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  42601
dev_network_count:  143
learn step counter:  42651
dev_network_count:  143
learn step counter:  42701
dev_network_count:  143
learn step counter:  42751
dev_network_count:  143
learn step counter:  42801
dev_network_count:  143
learn step counter:  42851
dev_network_count:  143

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
144  r_total and score:  221.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  42901
dev_network_count:  144
learn step counter:  42951
dev_network_count:  144
EPOCH %d 46
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.00785516721127895 0.3
2185 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
2186 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
2187 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
2188 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
2189 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
2190 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
2191 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  43001
dev_network_count:  144
learn step counter:  43051
dev_network_count:  144
learn step counter:  43101
dev_network_count:  144
learn step counter:  43151
dev_network_count:  144

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
145  r_total and score:  221.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  43201
dev_network_count:  145
learn step counter:  43251
dev_network_count:  145
learn step counter:  43301
dev_network_count:  145
learn step counter:  43351
dev_network_count:  145
learn step counter:  43401
dev_network_count:  145
learn step counter:  43451
dev_network_count:  145

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
146  r_total and score:  221.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  43501
dev_network_count:  146
learn step counter:  43551
dev_network_count:  146
learn step counter:  43601
dev_network_count:  146
learn step counter:  43651
dev_network_count:  146
learn step counter:  43701
dev_network_count:  146
learn step counter:  43751
dev_network_count:  146

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
147  r_total and score:  221.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  43801
dev_network_count:  147
learn step counter:  43851
dev_network_count:  147
learn step counter:  43901
dev_network_count:  147
learn step counter:  43951
dev_network_count:  147
EPOCH %d 47
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.007069650490151055 0.3
4678 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4679 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4680 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4681 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4682 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4683 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4684 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  44001
dev_network_count:  147
learn step counter:  44051
dev_network_count:  147

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
148  r_total and score:  221.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  44101
dev_network_count:  148
learn step counter:  44151
dev_network_count:  148
learn step counter:  44201
dev_network_count:  148
learn step counter:  44251
dev_network_count:  148
learn step counter:  44301
dev_network_count:  148
learn step counter:  44351
dev_network_count:  148

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
149  r_total and score:  221.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  44401
dev_network_count:  149
learn step counter:  44451
dev_network_count:  149
learn step counter:  44501
dev_network_count:  149
learn step counter:  44551
dev_network_count:  149
learn step counter:  44601
dev_network_count:  149
learn step counter:  44651
dev_network_count:  149

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
150  r_total and score:  221.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  44701
dev_network_count:  150
learn step counter:  44751
dev_network_count:  150
learn step counter:  44801
dev_network_count:  150
learn step counter:  44851
dev_network_count:  150
learn step counter:  44901
dev_network_count:  150
learn step counter:  44951
dev_network_count:  150
EPOCH %d 48
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.00636268544113595 0.3
2171 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
2172 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
2173 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
2174 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
2175 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
2176 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
2177 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
151  r_total and score:  221.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  45001
dev_network_count:  151
learn step counter:  45051
dev_network_count:  151
learn step counter:  45101
dev_network_count:  151
learn step counter:  45151
dev_network_count:  151
learn step counter:  45201
dev_network_count:  151
learn step counter:  45251
dev_network_count:  151

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
152  r_total and score:  221.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  45301
dev_network_count:  152
learn step counter:  45351
dev_network_count:  152
learn step counter:  45401
dev_network_count:  152
learn step counter:  45451
dev_network_count:  152
learn step counter:  45501
dev_network_count:  152
learn step counter:  45551
dev_network_count:  152

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
153  r_total and score:  221.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  45601
dev_network_count:  153
learn step counter:  45651
dev_network_count:  153
learn step counter:  45701
dev_network_count:  153
learn step counter:  45751
dev_network_count:  153
learn step counter:  45801
dev_network_count:  153
learn step counter:  45851
dev_network_count:  153

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
154  r_total and score:  221.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  45901
dev_network_count:  154
learn step counter:  45951
dev_network_count:  154
EPOCH %d 49
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.005726416897022355 0.3
4664 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4665 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4666 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4667 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4668 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4669 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4670 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  46001
dev_network_count:  154
learn step counter:  46051
dev_network_count:  154
learn step counter:  46101
dev_network_count:  154
learn step counter:  46151
dev_network_count:  154

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
155  r_total and score:  221.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  46201
dev_network_count:  155
learn step counter:  46251
dev_network_count:  155
learn step counter:  46301
dev_network_count:  155
learn step counter:  46351
dev_network_count:  155
learn step counter:  46401
dev_network_count:  155
learn step counter:  46451
dev_network_count:  155

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
156  r_total and score:  221.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  46501
dev_network_count:  156
learn step counter:  46551
dev_network_count:  156
learn step counter:  46601
dev_network_count:  156
learn step counter:  46651
dev_network_count:  156
learn step counter:  46701
dev_network_count:  156
learn step counter:  46751
dev_network_count:  156

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
157  r_total and score:  221.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  46801
dev_network_count:  157
learn step counter:  46851
dev_network_count:  157
learn step counter:  46901
dev_network_count:  157
learn step counter:  46951
dev_network_count:  157
EPOCH %d 50
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.00515377520732012 0.3
2157 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
2158 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
2159 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
2160 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
2161 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
2162 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
2163 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  47001
dev_network_count:  157
learn step counter:  47051
dev_network_count:  157

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
158  r_total and score:  221.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  47101
dev_network_count:  158
learn step counter:  47151
dev_network_count:  158
learn step counter:  47201
dev_network_count:  158
learn step counter:  47251
dev_network_count:  158
learn step counter:  47301
dev_network_count:  158
learn step counter:  47351
dev_network_count:  158

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
159  r_total and score:  221.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  47401
dev_network_count:  159
learn step counter:  47451
dev_network_count:  159
learn step counter:  47501
dev_network_count:  159
learn step counter:  47551
dev_network_count:  159
learn step counter:  47601
dev_network_count:  159
learn step counter:  47651
dev_network_count:  159

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
160  r_total and score:  221.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  47701
dev_network_count:  160
learn step counter:  47751
dev_network_count:  160
learn step counter:  47801
dev_network_count:  160
learn step counter:  47851
dev_network_count:  160
learn step counter:  47901
dev_network_count:  160
learn step counter:  47951
dev_network_count:  160
EPOCH %d 51
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.004638397686588108 0.3
4650 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4651 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4652 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4653 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4654 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4655 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4656 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
161  r_total and score:  221.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  48001
dev_network_count:  161
learn step counter:  48051
dev_network_count:  161
learn step counter:  48101
dev_network_count:  161
learn step counter:  48151
dev_network_count:  161
learn step counter:  48201
dev_network_count:  161
learn step counter:  48251
dev_network_count:  161

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
162  r_total and score:  221.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  48301
dev_network_count:  162
learn step counter:  48351
dev_network_count:  162
learn step counter:  48401
dev_network_count:  162
learn step counter:  48451
dev_network_count:  162
learn step counter:  48501
dev_network_count:  162
learn step counter:  48551
dev_network_count:  162

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
163  r_total and score:  221.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  48601
dev_network_count:  163
learn step counter:  48651
dev_network_count:  163
learn step counter:  48701
dev_network_count:  163
learn step counter:  48751
dev_network_count:  163
learn step counter:  48801
dev_network_count:  163
learn step counter:  48851
dev_network_count:  163

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
164  r_total and score:  221.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  48901
dev_network_count:  164
learn step counter:  48951
dev_network_count:  164
EPOCH %d 52
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.004174557917929297 0.3
2143 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
2144 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
2145 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
2146 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
2147 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
2148 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
2149 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  49001
dev_network_count:  164
learn step counter:  49051
dev_network_count:  164
learn step counter:  49101
dev_network_count:  164
learn step counter:  49151
dev_network_count:  164

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
165  r_total and score:  221.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  49201
dev_network_count:  165
learn step counter:  49251
dev_network_count:  165
learn step counter:  49301
dev_network_count:  165
learn step counter:  49351
dev_network_count:  165
learn step counter:  49401
dev_network_count:  165
learn step counter:  49451
dev_network_count:  165

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
166  r_total and score:  221.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  49501
dev_network_count:  166
learn step counter:  49551
dev_network_count:  166
learn step counter:  49601
dev_network_count:  166
learn step counter:  49651
dev_network_count:  166
learn step counter:  49701
dev_network_count:  166
learn step counter:  49751
dev_network_count:  166

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
167  r_total and score:  221.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  49801
dev_network_count:  167
learn step counter:  49851
dev_network_count:  167
learn step counter:  49901
dev_network_count:  167
learn step counter:  49951
dev_network_count:  167
EPOCH %d 53
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.0037571021261363674 0.3
4636 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4637 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4638 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4639 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4640 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4641 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4642 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  50001
dev_network_count:  167
learn step counter:  50051
dev_network_count:  167

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
168  r_total and score:  221.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  50101
dev_network_count:  168
learn step counter:  50151
dev_network_count:  168
learn step counter:  50201
dev_network_count:  168
learn step counter:  50251
dev_network_count:  168
learn step counter:  50301
dev_network_count:  168
learn step counter:  50351
dev_network_count:  168

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
169  r_total and score:  221.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  50401
dev_network_count:  169
learn step counter:  50451
dev_network_count:  169
learn step counter:  50501
dev_network_count:  169
learn step counter:  50551
dev_network_count:  169
learn step counter:  50601
dev_network_count:  169
learn step counter:  50651
dev_network_count:  169

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
170  r_total and score:  221.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  50701
dev_network_count:  170
learn step counter:  50751
dev_network_count:  170
learn step counter:  50801
dev_network_count:  170
learn step counter:  50851
dev_network_count:  170
learn step counter:  50901
dev_network_count:  170
learn step counter:  50951
dev_network_count:  170
EPOCH %d 54
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.0033813919135227306 0.3
2129 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
2130 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
2131 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
2132 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
2133 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
2134 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
2135 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
171  r_total and score:  221.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  51001
dev_network_count:  171
learn step counter:  51051
dev_network_count:  171
learn step counter:  51101
dev_network_count:  171
learn step counter:  51151
dev_network_count:  171
learn step counter:  51201
dev_network_count:  171
learn step counter:  51251
dev_network_count:  171

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
172  r_total and score:  221.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  51301
dev_network_count:  172
learn step counter:  51351
dev_network_count:  172
learn step counter:  51401
dev_network_count:  172
learn step counter:  51451
dev_network_count:  172
learn step counter:  51501
dev_network_count:  172
learn step counter:  51551
dev_network_count:  172

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
173  r_total and score:  221.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  51601
dev_network_count:  173
learn step counter:  51651
dev_network_count:  173
learn step counter:  51701
dev_network_count:  173
learn step counter:  51751
dev_network_count:  173
learn step counter:  51801
dev_network_count:  173
learn step counter:  51851
dev_network_count:  173

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
174  r_total and score:  221.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  51901
dev_network_count:  174
learn step counter:  51951
dev_network_count:  174
EPOCH %d 55
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.0030432527221704577 0.3
4622 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4623 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4624 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4625 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4626 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4627 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4628 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  52001
dev_network_count:  174
learn step counter:  52051
dev_network_count:  174
learn step counter:  52101
dev_network_count:  174
learn step counter:  52151
dev_network_count:  174

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
175  r_total and score:  221.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  52201
dev_network_count:  175
learn step counter:  52251
dev_network_count:  175
learn step counter:  52301
dev_network_count:  175
learn step counter:  52351
dev_network_count:  175
learn step counter:  52401
dev_network_count:  175
learn step counter:  52451
dev_network_count:  175

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
176  r_total and score:  221.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  52501
dev_network_count:  176
learn step counter:  52551
dev_network_count:  176
learn step counter:  52601
dev_network_count:  176
learn step counter:  52651
dev_network_count:  176
learn step counter:  52701
dev_network_count:  176
learn step counter:  52751
dev_network_count:  176

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
177  r_total and score:  221.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  52801
dev_network_count:  177
learn step counter:  52851
dev_network_count:  177
learn step counter:  52901
dev_network_count:  177
learn step counter:  52951
dev_network_count:  177
EPOCH %d 56
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.002738927449953412 0.3
2115 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
2116 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
2117 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
2118 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
2119 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
2120 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
2121 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  53001
dev_network_count:  177
learn step counter:  53051
dev_network_count:  177

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
178  r_total and score:  221.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  53101
dev_network_count:  178
learn step counter:  53151
dev_network_count:  178
learn step counter:  53201
dev_network_count:  178
learn step counter:  53251
dev_network_count:  178
learn step counter:  53301
dev_network_count:  178
learn step counter:  53351
dev_network_count:  178

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
179  r_total and score:  219.4000000000002 34.940233815300765
Current Bleu score is:  34.940233815300765
learn step counter:  53401
dev_network_count:  179
learn step counter:  53451
dev_network_count:  179
learn step counter:  53501
dev_network_count:  179
learn step counter:  53551
dev_network_count:  179
learn step counter:  53601
dev_network_count:  179
learn step counter:  53651
dev_network_count:  179

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
180  r_total and score:  219.4000000000002 34.940233815300765
Current Bleu score is:  34.940233815300765
learn step counter:  53701
dev_network_count:  180
learn step counter:  53751
dev_network_count:  180
learn step counter:  53801
dev_network_count:  180
learn step counter:  53851
dev_network_count:  180
learn step counter:  53901
dev_network_count:  180
learn step counter:  53951
dev_network_count:  180
EPOCH %d 57
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.002465034704958071 0.3
4608 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4609 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4610 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4611 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4612 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4613 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4614 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
181  r_total and score:  221.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  54001
dev_network_count:  181
learn step counter:  54051
dev_network_count:  181
learn step counter:  54101
dev_network_count:  181
learn step counter:  54151
dev_network_count:  181
learn step counter:  54201
dev_network_count:  181
learn step counter:  54251
dev_network_count:  181

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
182  r_total and score:  221.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  54301
dev_network_count:  182
learn step counter:  54351
dev_network_count:  182
learn step counter:  54401
dev_network_count:  182
learn step counter:  54451
dev_network_count:  182
learn step counter:  54501
dev_network_count:  182
learn step counter:  54551
dev_network_count:  182

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
183  r_total and score:  221.4000000000002 30.91724498395411
Current Bleu score is:  30.91724498395411
learn step counter:  54601
dev_network_count:  183
learn step counter:  54651
dev_network_count:  183
learn step counter:  54701
dev_network_count:  183
learn step counter:  54751
dev_network_count:  183
learn step counter:  54801
dev_network_count:  183
learn step counter:  54851
dev_network_count:  183

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
184  r_total and score:  219.4000000000002 34.940233815300765
Current Bleu score is:  34.940233815300765
learn step counter:  54901
dev_network_count:  184
learn step counter:  54951
dev_network_count:  184
EPOCH %d 58
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.002218531234462264 0.3
2101 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
2102 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
2103 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
2104 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
2105 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
2106 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
2107 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  55001
dev_network_count:  184
learn step counter:  55051
dev_network_count:  184
learn step counter:  55101
dev_network_count:  184
learn step counter:  55151
dev_network_count:  184

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
185  r_total and score:  215.00000000000023 38.680147332529344
Current Bleu score is:  38.680147332529344
learn step counter:  55201
dev_network_count:  185
learn step counter:  55251
dev_network_count:  185
learn step counter:  55301
dev_network_count:  185
learn step counter:  55351
dev_network_count:  185
learn step counter:  55401
dev_network_count:  185
learn step counter:  55451
dev_network_count:  185

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
186  r_total and score:  219.4000000000002 34.940233815300765
Current Bleu score is:  34.940233815300765
learn step counter:  55501
dev_network_count:  186
learn step counter:  55551
dev_network_count:  186
learn step counter:  55601
dev_network_count:  186
learn step counter:  55651
dev_network_count:  186
learn step counter:  55701
dev_network_count:  186
learn step counter:  55751
dev_network_count:  186

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
187  r_total and score:  219.4000000000002 34.940233815300765
Current Bleu score is:  34.940233815300765
learn step counter:  55801
dev_network_count:  187
learn step counter:  55851
dev_network_count:  187
learn step counter:  55901
dev_network_count:  187
learn step counter:  55951
dev_network_count:  187
EPOCH %d 59
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.0019966781110160375 0.3
4594 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4595 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4596 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4597 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4598 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4599 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4600 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  56001
dev_network_count:  187
learn step counter:  56051
dev_network_count:  187

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
188  r_total and score:  219.4000000000002 34.940233815300765
Current Bleu score is:  34.940233815300765
learn step counter:  56101
dev_network_count:  188
learn step counter:  56151
dev_network_count:  188
learn step counter:  56201
dev_network_count:  188
learn step counter:  56251
dev_network_count:  188
learn step counter:  56301
dev_network_count:  188
learn step counter:  56351
dev_network_count:  188

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
189  r_total and score:  216.4000000000002 40.46871582856826
Current Bleu score is:  40.46871582856826
learn step counter:  56401
dev_network_count:  189
learn step counter:  56451
dev_network_count:  189
learn step counter:  56501
dev_network_count:  189
learn step counter:  56551
dev_network_count:  189
learn step counter:  56601
dev_network_count:  189
learn step counter:  56651
dev_network_count:  189

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
190  r_total and score:  219.4000000000002 34.940233815300765
Current Bleu score is:  34.940233815300765
learn step counter:  56701
dev_network_count:  190
learn step counter:  56751
dev_network_count:  190
learn step counter:  56801
dev_network_count:  190
learn step counter:  56851
dev_network_count:  190
learn step counter:  56901
dev_network_count:  190
learn step counter:  56951
dev_network_count:  190
EPOCH %d 60
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001797010299914434 0.3
2087 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
2088 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
2089 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
2090 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
2091 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
2092 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
2093 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
191  r_total and score:  215.00000000000023 38.680147332529344
Current Bleu score is:  38.680147332529344
learn step counter:  57001
dev_network_count:  191
learn step counter:  57051
dev_network_count:  191
learn step counter:  57101
dev_network_count:  191
learn step counter:  57151
dev_network_count:  191
learn step counter:  57201
dev_network_count:  191
learn step counter:  57251
dev_network_count:  191

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
192  r_total and score:  219.4000000000002 34.940233815300765
Current Bleu score is:  34.940233815300765
learn step counter:  57301
dev_network_count:  192
learn step counter:  57351
dev_network_count:  192
learn step counter:  57401
dev_network_count:  192
learn step counter:  57451
dev_network_count:  192
learn step counter:  57501
dev_network_count:  192
learn step counter:  57551
dev_network_count:  192

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
193  r_total and score:  215.00000000000023 38.680147332529344
Current Bleu score is:  38.680147332529344
learn step counter:  57601
dev_network_count:  193
learn step counter:  57651
dev_network_count:  193
learn step counter:  57701
dev_network_count:  193
learn step counter:  57751
dev_network_count:  193
learn step counter:  57801
dev_network_count:  193
learn step counter:  57851
dev_network_count:  193

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
194  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  57901
dev_network_count:  194
learn step counter:  57951
dev_network_count:  194
EPOCH %d 61
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.0016173092699229906 0.3
4580 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4581 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4582 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4583 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4584 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4585 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4586 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  58001
dev_network_count:  194
learn step counter:  58051
dev_network_count:  194
learn step counter:  58101
dev_network_count:  194
learn step counter:  58151
dev_network_count:  194

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
195  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  58201
dev_network_count:  195
learn step counter:  58251
dev_network_count:  195
learn step counter:  58301
dev_network_count:  195
learn step counter:  58351
dev_network_count:  195
learn step counter:  58401
dev_network_count:  195
learn step counter:  58451
dev_network_count:  195

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
196  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  58501
dev_network_count:  196
learn step counter:  58551
dev_network_count:  196
learn step counter:  58601
dev_network_count:  196
learn step counter:  58651
dev_network_count:  196
learn step counter:  58701
dev_network_count:  196
learn step counter:  58751
dev_network_count:  196

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
197  r_total and score:  215.00000000000023 38.680147332529344
Current Bleu score is:  38.680147332529344
learn step counter:  58801
dev_network_count:  197
learn step counter:  58851
dev_network_count:  197
learn step counter:  58901
dev_network_count:  197
learn step counter:  58951
dev_network_count:  197
EPOCH %d 62
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.0014555783429306916 0.3
2073 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
2074 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
2075 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
2076 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
2077 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
2078 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
2079 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  59001
dev_network_count:  197
learn step counter:  59051
dev_network_count:  197

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
198  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  59101
dev_network_count:  198
learn step counter:  59151
dev_network_count:  198
learn step counter:  59201
dev_network_count:  198
learn step counter:  59251
dev_network_count:  198
learn step counter:  59301
dev_network_count:  198
learn step counter:  59351
dev_network_count:  198

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
199  r_total and score:  215.00000000000023 38.680147332529344
Current Bleu score is:  38.680147332529344
learn step counter:  59401
dev_network_count:  199
learn step counter:  59451
dev_network_count:  199
learn step counter:  59501
dev_network_count:  199
learn step counter:  59551
dev_network_count:  199
learn step counter:  59601
dev_network_count:  199
learn step counter:  59651
dev_network_count:  199

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
200  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  59701
dev_network_count:  200
learn step counter:  59751
dev_network_count:  200
learn step counter:  59801
dev_network_count:  200
learn step counter:  59851
dev_network_count:  200
learn step counter:  59901
dev_network_count:  200
learn step counter:  59951
dev_network_count:  200
EPOCH %d 63
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.0013100205086376223 0.3
4566 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4567 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4568 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4569 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4570 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4571 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4572 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
201  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  60001
dev_network_count:  201
learn step counter:  60051
dev_network_count:  201
learn step counter:  60101
dev_network_count:  201
learn step counter:  60151
dev_network_count:  201
learn step counter:  60201
dev_network_count:  201
learn step counter:  60251
dev_network_count:  201

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
202  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  60301
dev_network_count:  202
learn step counter:  60351
dev_network_count:  202
learn step counter:  60401
dev_network_count:  202
learn step counter:  60451
dev_network_count:  202
learn step counter:  60501
dev_network_count:  202
learn step counter:  60551
dev_network_count:  202

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
203  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  60601
dev_network_count:  203
learn step counter:  60651
dev_network_count:  203
learn step counter:  60701
dev_network_count:  203
learn step counter:  60751
dev_network_count:  203
learn step counter:  60801
dev_network_count:  203
learn step counter:  60851
dev_network_count:  203

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
204  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  60901
dev_network_count:  204
learn step counter:  60951
dev_network_count:  204
EPOCH %d 64
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.0011790184577738603 0.3
2059 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
2060 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
2061 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
2062 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
2063 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
2064 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
2065 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  61001
dev_network_count:  204
learn step counter:  61051
dev_network_count:  204
learn step counter:  61101
dev_network_count:  204
learn step counter:  61151
dev_network_count:  204

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
205  r_total and score:  215.00000000000023 38.680147332529344
Current Bleu score is:  38.680147332529344
learn step counter:  61201
dev_network_count:  205
learn step counter:  61251
dev_network_count:  205
learn step counter:  61301
dev_network_count:  205
learn step counter:  61351
dev_network_count:  205
learn step counter:  61401
dev_network_count:  205
learn step counter:  61451
dev_network_count:  205

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
206  r_total and score:  215.00000000000023 38.680147332529344
Current Bleu score is:  38.680147332529344
learn step counter:  61501
dev_network_count:  206
learn step counter:  61551
dev_network_count:  206
learn step counter:  61601
dev_network_count:  206
learn step counter:  61651
dev_network_count:  206
learn step counter:  61701
dev_network_count:  206
learn step counter:  61751
dev_network_count:  206

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
207  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  61801
dev_network_count:  207
learn step counter:  61851
dev_network_count:  207
learn step counter:  61901
dev_network_count:  207
learn step counter:  61951
dev_network_count:  207
EPOCH %d 65
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001061116611996474 0.3
4552 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4553 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4554 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4555 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4556 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4557 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4558 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  62001
dev_network_count:  207
learn step counter:  62051
dev_network_count:  207

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
208  r_total and score:  215.00000000000023 38.680147332529344
Current Bleu score is:  38.680147332529344
learn step counter:  62101
dev_network_count:  208
learn step counter:  62151
dev_network_count:  208
learn step counter:  62201
dev_network_count:  208
learn step counter:  62251
dev_network_count:  208
learn step counter:  62301
dev_network_count:  208
learn step counter:  62351
dev_network_count:  208

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
209  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  62401
dev_network_count:  209
learn step counter:  62451
dev_network_count:  209
learn step counter:  62501
dev_network_count:  209
learn step counter:  62551
dev_network_count:  209
learn step counter:  62601
dev_network_count:  209
learn step counter:  62651
dev_network_count:  209

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
210  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  62701
dev_network_count:  210
learn step counter:  62751
dev_network_count:  210
learn step counter:  62801
dev_network_count:  210
learn step counter:  62851
dev_network_count:  210
learn step counter:  62901
dev_network_count:  210
learn step counter:  62951
dev_network_count:  210
EPOCH %d 66
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
2045 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
2046 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
2047 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
2048 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
2049 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
2050 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
2051 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
211  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  63001
dev_network_count:  211
learn step counter:  63051
dev_network_count:  211
learn step counter:  63101
dev_network_count:  211
learn step counter:  63151
dev_network_count:  211
learn step counter:  63201
dev_network_count:  211
learn step counter:  63251
dev_network_count:  211

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
212  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  63301
dev_network_count:  212
learn step counter:  63351
dev_network_count:  212
learn step counter:  63401
dev_network_count:  212
learn step counter:  63451
dev_network_count:  212
learn step counter:  63501
dev_network_count:  212
learn step counter:  63551
dev_network_count:  212

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
213  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  63601
dev_network_count:  213
learn step counter:  63651
dev_network_count:  213
learn step counter:  63701
dev_network_count:  213
learn step counter:  63751
dev_network_count:  213
learn step counter:  63801
dev_network_count:  213
learn step counter:  63851
dev_network_count:  213

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
214  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  63901
dev_network_count:  214
learn step counter:  63951
dev_network_count:  214
EPOCH %d 67
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
4538 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4539 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4540 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4541 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4542 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4543 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4544 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  64001
dev_network_count:  214
learn step counter:  64051
dev_network_count:  214
learn step counter:  64101
dev_network_count:  214
learn step counter:  64151
dev_network_count:  214

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
215  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  64201
dev_network_count:  215
learn step counter:  64251
dev_network_count:  215
learn step counter:  64301
dev_network_count:  215
learn step counter:  64351
dev_network_count:  215
learn step counter:  64401
dev_network_count:  215
learn step counter:  64451
dev_network_count:  215

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
216  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  64501
dev_network_count:  216
learn step counter:  64551
dev_network_count:  216
learn step counter:  64601
dev_network_count:  216
learn step counter:  64651
dev_network_count:  216
learn step counter:  64701
dev_network_count:  216
learn step counter:  64751
dev_network_count:  216

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
217  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  64801
dev_network_count:  217
learn step counter:  64851
dev_network_count:  217
learn step counter:  64901
dev_network_count:  217
learn step counter:  64951
dev_network_count:  217
EPOCH %d 68
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
2031 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
2032 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
2033 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
2034 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
2035 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
2036 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
2037 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  65001
dev_network_count:  217
learn step counter:  65051
dev_network_count:  217

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
218  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  65101
dev_network_count:  218
learn step counter:  65151
dev_network_count:  218
learn step counter:  65201
dev_network_count:  218
learn step counter:  65251
dev_network_count:  218
learn step counter:  65301
dev_network_count:  218
learn step counter:  65351
dev_network_count:  218

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
219  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  65401
dev_network_count:  219
learn step counter:  65451
dev_network_count:  219
learn step counter:  65501
dev_network_count:  219
learn step counter:  65551
dev_network_count:  219
learn step counter:  65601
dev_network_count:  219
learn step counter:  65651
dev_network_count:  219

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
220  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  65701
dev_network_count:  220
learn step counter:  65751
dev_network_count:  220
learn step counter:  65801
dev_network_count:  220
learn step counter:  65851
dev_network_count:  220
learn step counter:  65901
dev_network_count:  220
learn step counter:  65951
dev_network_count:  220
EPOCH %d 69
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
4524 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4525 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4526 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4527 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4528 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4529 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4530 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
221  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  66001
dev_network_count:  221
learn step counter:  66051
dev_network_count:  221
learn step counter:  66101
dev_network_count:  221
learn step counter:  66151
dev_network_count:  221
learn step counter:  66201
dev_network_count:  221
learn step counter:  66251
dev_network_count:  221

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
222  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  66301
dev_network_count:  222
learn step counter:  66351
dev_network_count:  222
learn step counter:  66401
dev_network_count:  222
learn step counter:  66451
dev_network_count:  222
learn step counter:  66501
dev_network_count:  222
learn step counter:  66551
dev_network_count:  222

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
223  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  66601
dev_network_count:  223
learn step counter:  66651
dev_network_count:  223
learn step counter:  66701
dev_network_count:  223
learn step counter:  66751
dev_network_count:  223
learn step counter:  66801
dev_network_count:  223
learn step counter:  66851
dev_network_count:  223

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
224  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  66901
dev_network_count:  224
learn step counter:  66951
dev_network_count:  224
EPOCH %d 70
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
2017 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
2018 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
2019 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
2020 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
2021 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
2022 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
2023 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  67001
dev_network_count:  224
learn step counter:  67051
dev_network_count:  224
learn step counter:  67101
dev_network_count:  224
learn step counter:  67151
dev_network_count:  224

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
225  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  67201
dev_network_count:  225
learn step counter:  67251
dev_network_count:  225
learn step counter:  67301
dev_network_count:  225
learn step counter:  67351
dev_network_count:  225
learn step counter:  67401
dev_network_count:  225
learn step counter:  67451
dev_network_count:  225

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
226  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  67501
dev_network_count:  226
learn step counter:  67551
dev_network_count:  226
learn step counter:  67601
dev_network_count:  226
learn step counter:  67651
dev_network_count:  226
learn step counter:  67701
dev_network_count:  226
learn step counter:  67751
dev_network_count:  226

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
227  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  67801
dev_network_count:  227
learn step counter:  67851
dev_network_count:  227
learn step counter:  67901
dev_network_count:  227
learn step counter:  67951
dev_network_count:  227
EPOCH %d 71
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
4510 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4511 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4512 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4513 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4514 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4515 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4516 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  68001
dev_network_count:  227
learn step counter:  68051
dev_network_count:  227

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
228  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  68101
dev_network_count:  228
learn step counter:  68151
dev_network_count:  228
learn step counter:  68201
dev_network_count:  228
learn step counter:  68251
dev_network_count:  228
learn step counter:  68301
dev_network_count:  228
learn step counter:  68351
dev_network_count:  228

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
229  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  68401
dev_network_count:  229
learn step counter:  68451
dev_network_count:  229
learn step counter:  68501
dev_network_count:  229
learn step counter:  68551
dev_network_count:  229
learn step counter:  68601
dev_network_count:  229
learn step counter:  68651
dev_network_count:  229

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
230  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  68701
dev_network_count:  230
learn step counter:  68751
dev_network_count:  230
learn step counter:  68801
dev_network_count:  230
learn step counter:  68851
dev_network_count:  230
learn step counter:  68901
dev_network_count:  230
learn step counter:  68951
dev_network_count:  230
EPOCH %d 72
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
2003 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
2004 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
2005 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
2006 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
2007 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
2008 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
2009 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
231  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  69001
dev_network_count:  231
learn step counter:  69051
dev_network_count:  231
learn step counter:  69101
dev_network_count:  231
learn step counter:  69151
dev_network_count:  231
learn step counter:  69201
dev_network_count:  231
learn step counter:  69251
dev_network_count:  231

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
232  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  69301
dev_network_count:  232
learn step counter:  69351
dev_network_count:  232
learn step counter:  69401
dev_network_count:  232
learn step counter:  69451
dev_network_count:  232
learn step counter:  69501
dev_network_count:  232
learn step counter:  69551
dev_network_count:  232

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
233  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  69601
dev_network_count:  233
learn step counter:  69651
dev_network_count:  233
learn step counter:  69701
dev_network_count:  233
learn step counter:  69751
dev_network_count:  233
learn step counter:  69801
dev_network_count:  233
learn step counter:  69851
dev_network_count:  233

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
234  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  69901
dev_network_count:  234
learn step counter:  69951
dev_network_count:  234
EPOCH %d 73
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
4496 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4497 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4498 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4499 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4500 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4501 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4502 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  70001
dev_network_count:  234
learn step counter:  70051
dev_network_count:  234
learn step counter:  70101
dev_network_count:  234
learn step counter:  70151
dev_network_count:  234

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
235  r_total and score:  218.6000000000002 38.680147332529344
Current Bleu score is:  38.680147332529344
learn step counter:  70201
dev_network_count:  235
learn step counter:  70251
dev_network_count:  235
learn step counter:  70301
dev_network_count:  235
learn step counter:  70351
dev_network_count:  235
learn step counter:  70401
dev_network_count:  235
learn step counter:  70451
dev_network_count:  235

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
236  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  70501
dev_network_count:  236
learn step counter:  70551
dev_network_count:  236
learn step counter:  70601
dev_network_count:  236
learn step counter:  70651
dev_network_count:  236
learn step counter:  70701
dev_network_count:  236
learn step counter:  70751
dev_network_count:  236

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
237  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  70801
dev_network_count:  237
learn step counter:  70851
dev_network_count:  237
learn step counter:  70901
dev_network_count:  237
learn step counter:  70951
dev_network_count:  237
EPOCH %d 74
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1989 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1990 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1991 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1992 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1993 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1994 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1995 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  71001
dev_network_count:  237
learn step counter:  71051
dev_network_count:  237

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
238  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  71101
dev_network_count:  238
learn step counter:  71151
dev_network_count:  238
learn step counter:  71201
dev_network_count:  238
learn step counter:  71251
dev_network_count:  238
learn step counter:  71301
dev_network_count:  238
learn step counter:  71351
dev_network_count:  238

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
239  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  71401
dev_network_count:  239
learn step counter:  71451
dev_network_count:  239
learn step counter:  71501
dev_network_count:  239
learn step counter:  71551
dev_network_count:  239
learn step counter:  71601
dev_network_count:  239
learn step counter:  71651
dev_network_count:  239

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
240  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  71701
dev_network_count:  240
learn step counter:  71751
dev_network_count:  240
learn step counter:  71801
dev_network_count:  240
learn step counter:  71851
dev_network_count:  240
learn step counter:  71901
dev_network_count:  240
learn step counter:  71951
dev_network_count:  240
EPOCH %d 75
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
4482 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4483 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4484 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4485 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4486 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4487 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4488 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
241  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  72001
dev_network_count:  241
learn step counter:  72051
dev_network_count:  241
learn step counter:  72101
dev_network_count:  241
learn step counter:  72151
dev_network_count:  241
learn step counter:  72201
dev_network_count:  241
learn step counter:  72251
dev_network_count:  241

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
242  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  72301
dev_network_count:  242
learn step counter:  72351
dev_network_count:  242
learn step counter:  72401
dev_network_count:  242
learn step counter:  72451
dev_network_count:  242
learn step counter:  72501
dev_network_count:  242
learn step counter:  72551
dev_network_count:  242

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
243  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  72601
dev_network_count:  243
learn step counter:  72651
dev_network_count:  243
learn step counter:  72701
dev_network_count:  243
learn step counter:  72751
dev_network_count:  243
learn step counter:  72801
dev_network_count:  243
learn step counter:  72851
dev_network_count:  243

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
244  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  72901
dev_network_count:  244
learn step counter:  72951
dev_network_count:  244
EPOCH %d 76
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1975 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1976 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1977 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1978 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1979 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1980 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1981 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  73001
dev_network_count:  244
learn step counter:  73051
dev_network_count:  244
learn step counter:  73101
dev_network_count:  244
learn step counter:  73151
dev_network_count:  244

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
245  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  73201
dev_network_count:  245
learn step counter:  73251
dev_network_count:  245
learn step counter:  73301
dev_network_count:  245
learn step counter:  73351
dev_network_count:  245
learn step counter:  73401
dev_network_count:  245
learn step counter:  73451
dev_network_count:  245

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
246  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  73501
dev_network_count:  246
learn step counter:  73551
dev_network_count:  246
learn step counter:  73601
dev_network_count:  246
learn step counter:  73651
dev_network_count:  246
learn step counter:  73701
dev_network_count:  246
learn step counter:  73751
dev_network_count:  246

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
247  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  73801
dev_network_count:  247
learn step counter:  73851
dev_network_count:  247
learn step counter:  73901
dev_network_count:  247
learn step counter:  73951
dev_network_count:  247
EPOCH %d 77
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
4468 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4469 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4470 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4471 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4472 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4473 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4474 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  74001
dev_network_count:  247
learn step counter:  74051
dev_network_count:  247

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
248  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  74101
dev_network_count:  248
learn step counter:  74151
dev_network_count:  248
learn step counter:  74201
dev_network_count:  248
learn step counter:  74251
dev_network_count:  248
learn step counter:  74301
dev_network_count:  248
learn step counter:  74351
dev_network_count:  248

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
249  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  74401
dev_network_count:  249
learn step counter:  74451
dev_network_count:  249
learn step counter:  74501
dev_network_count:  249
learn step counter:  74551
dev_network_count:  249
learn step counter:  74601
dev_network_count:  249
learn step counter:  74651
dev_network_count:  249

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
250  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  74701
dev_network_count:  250
learn step counter:  74751
dev_network_count:  250
learn step counter:  74801
dev_network_count:  250
learn step counter:  74851
dev_network_count:  250
learn step counter:  74901
dev_network_count:  250
learn step counter:  74951
dev_network_count:  250
EPOCH %d 78
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1961 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1962 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1963 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1964 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1965 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1966 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1967 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
251  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  75001
dev_network_count:  251
learn step counter:  75051
dev_network_count:  251
learn step counter:  75101
dev_network_count:  251
learn step counter:  75151
dev_network_count:  251
learn step counter:  75201
dev_network_count:  251
learn step counter:  75251
dev_network_count:  251

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
252  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  75301
dev_network_count:  252
learn step counter:  75351
dev_network_count:  252
learn step counter:  75401
dev_network_count:  252
learn step counter:  75451
dev_network_count:  252
learn step counter:  75501
dev_network_count:  252
learn step counter:  75551
dev_network_count:  252

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
253  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  75601
dev_network_count:  253
learn step counter:  75651
dev_network_count:  253
learn step counter:  75701
dev_network_count:  253
learn step counter:  75751
dev_network_count:  253
learn step counter:  75801
dev_network_count:  253
learn step counter:  75851
dev_network_count:  253

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
254  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  75901
dev_network_count:  254
learn step counter:  75951
dev_network_count:  254
EPOCH %d 79
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
4454 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4455 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4456 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4457 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4458 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4459 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4460 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  76001
dev_network_count:  254
learn step counter:  76051
dev_network_count:  254
learn step counter:  76101
dev_network_count:  254
learn step counter:  76151
dev_network_count:  254

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
255  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  76201
dev_network_count:  255
learn step counter:  76251
dev_network_count:  255
learn step counter:  76301
dev_network_count:  255
learn step counter:  76351
dev_network_count:  255
learn step counter:  76401
dev_network_count:  255
learn step counter:  76451
dev_network_count:  255

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
256  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  76501
dev_network_count:  256
learn step counter:  76551
dev_network_count:  256
learn step counter:  76601
dev_network_count:  256
learn step counter:  76651
dev_network_count:  256
learn step counter:  76701
dev_network_count:  256
learn step counter:  76751
dev_network_count:  256

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
257  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  76801
dev_network_count:  257
learn step counter:  76851
dev_network_count:  257
learn step counter:  76901
dev_network_count:  257
learn step counter:  76951
dev_network_count:  257
EPOCH %d 80
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1947 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1948 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1949 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1950 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1951 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1952 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1953 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  77001
dev_network_count:  257
learn step counter:  77051
dev_network_count:  257

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
258  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  77101
dev_network_count:  258
learn step counter:  77151
dev_network_count:  258
learn step counter:  77201
dev_network_count:  258
learn step counter:  77251
dev_network_count:  258
learn step counter:  77301
dev_network_count:  258
learn step counter:  77351
dev_network_count:  258

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
259  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  77401
dev_network_count:  259
learn step counter:  77451
dev_network_count:  259
learn step counter:  77501
dev_network_count:  259
learn step counter:  77551
dev_network_count:  259
learn step counter:  77601
dev_network_count:  259
learn step counter:  77651
dev_network_count:  259

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
260  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  77701
dev_network_count:  260
learn step counter:  77751
dev_network_count:  260
learn step counter:  77801
dev_network_count:  260
learn step counter:  77851
dev_network_count:  260
learn step counter:  77901
dev_network_count:  260
learn step counter:  77951
dev_network_count:  260
EPOCH %d 81
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
4440 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4441 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4442 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4443 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4444 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4445 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4446 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
261  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  78001
dev_network_count:  261
learn step counter:  78051
dev_network_count:  261
learn step counter:  78101
dev_network_count:  261
learn step counter:  78151
dev_network_count:  261
learn step counter:  78201
dev_network_count:  261
learn step counter:  78251
dev_network_count:  261

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
262  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  78301
dev_network_count:  262
learn step counter:  78351
dev_network_count:  262
learn step counter:  78401
dev_network_count:  262
learn step counter:  78451
dev_network_count:  262
learn step counter:  78501
dev_network_count:  262
learn step counter:  78551
dev_network_count:  262

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
263  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  78601
dev_network_count:  263
learn step counter:  78651
dev_network_count:  263
learn step counter:  78701
dev_network_count:  263
learn step counter:  78751
dev_network_count:  263
learn step counter:  78801
dev_network_count:  263
learn step counter:  78851
dev_network_count:  263

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
264  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  78901
dev_network_count:  264
learn step counter:  78951
dev_network_count:  264
EPOCH %d 82
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1933 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1934 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1935 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1936 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1937 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1938 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1939 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  79001
dev_network_count:  264
learn step counter:  79051
dev_network_count:  264
learn step counter:  79101
dev_network_count:  264
learn step counter:  79151
dev_network_count:  264

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
265  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  79201
dev_network_count:  265
learn step counter:  79251
dev_network_count:  265
learn step counter:  79301
dev_network_count:  265
learn step counter:  79351
dev_network_count:  265
learn step counter:  79401
dev_network_count:  265
learn step counter:  79451
dev_network_count:  265

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
266  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  79501
dev_network_count:  266
learn step counter:  79551
dev_network_count:  266
learn step counter:  79601
dev_network_count:  266
learn step counter:  79651
dev_network_count:  266
learn step counter:  79701
dev_network_count:  266
learn step counter:  79751
dev_network_count:  266

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
267  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  79801
dev_network_count:  267
learn step counter:  79851
dev_network_count:  267
learn step counter:  79901
dev_network_count:  267
learn step counter:  79951
dev_network_count:  267
EPOCH %d 83
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
4426 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4427 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4428 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4429 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4430 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4431 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4432 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  80001
dev_network_count:  267
learn step counter:  80051
dev_network_count:  267

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
268  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  80101
dev_network_count:  268
learn step counter:  80151
dev_network_count:  268
learn step counter:  80201
dev_network_count:  268
learn step counter:  80251
dev_network_count:  268
learn step counter:  80301
dev_network_count:  268
learn step counter:  80351
dev_network_count:  268

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
269  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  80401
dev_network_count:  269
learn step counter:  80451
dev_network_count:  269
learn step counter:  80501
dev_network_count:  269
learn step counter:  80551
dev_network_count:  269
learn step counter:  80601
dev_network_count:  269
learn step counter:  80651
dev_network_count:  269

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
270  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  80701
dev_network_count:  270
learn step counter:  80751
dev_network_count:  270
learn step counter:  80801
dev_network_count:  270
learn step counter:  80851
dev_network_count:  270
learn step counter:  80901
dev_network_count:  270
learn step counter:  80951
dev_network_count:  270
EPOCH %d 84
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1919 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1920 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1921 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1922 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1923 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1924 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1925 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
271  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  81001
dev_network_count:  271
learn step counter:  81051
dev_network_count:  271
learn step counter:  81101
dev_network_count:  271
learn step counter:  81151
dev_network_count:  271
learn step counter:  81201
dev_network_count:  271
learn step counter:  81251
dev_network_count:  271

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
272  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  81301
dev_network_count:  272
learn step counter:  81351
dev_network_count:  272
learn step counter:  81401
dev_network_count:  272
learn step counter:  81451
dev_network_count:  272
learn step counter:  81501
dev_network_count:  272
learn step counter:  81551
dev_network_count:  272

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
273  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  81601
dev_network_count:  273
learn step counter:  81651
dev_network_count:  273
learn step counter:  81701
dev_network_count:  273
learn step counter:  81751
dev_network_count:  273
learn step counter:  81801
dev_network_count:  273
learn step counter:  81851
dev_network_count:  273

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
274  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  81901
dev_network_count:  274
learn step counter:  81951
dev_network_count:  274
EPOCH %d 85
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
4412 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4413 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4414 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4415 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4416 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4417 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4418 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  82001
dev_network_count:  274
learn step counter:  82051
dev_network_count:  274
learn step counter:  82101
dev_network_count:  274
learn step counter:  82151
dev_network_count:  274

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
275  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  82201
dev_network_count:  275
learn step counter:  82251
dev_network_count:  275
learn step counter:  82301
dev_network_count:  275
learn step counter:  82351
dev_network_count:  275
learn step counter:  82401
dev_network_count:  275
learn step counter:  82451
dev_network_count:  275

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
276  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  82501
dev_network_count:  276
learn step counter:  82551
dev_network_count:  276
learn step counter:  82601
dev_network_count:  276
learn step counter:  82651
dev_network_count:  276
learn step counter:  82701
dev_network_count:  276
learn step counter:  82751
dev_network_count:  276

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
277  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  82801
dev_network_count:  277
learn step counter:  82851
dev_network_count:  277
learn step counter:  82901
dev_network_count:  277
learn step counter:  82951
dev_network_count:  277
EPOCH %d 86
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1905 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1906 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1907 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1908 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1909 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1910 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1911 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  83001
dev_network_count:  277
learn step counter:  83051
dev_network_count:  277

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
278  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  83101
dev_network_count:  278
learn step counter:  83151
dev_network_count:  278
learn step counter:  83201
dev_network_count:  278
learn step counter:  83251
dev_network_count:  278
learn step counter:  83301
dev_network_count:  278
learn step counter:  83351
dev_network_count:  278

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
279  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  83401
dev_network_count:  279
learn step counter:  83451
dev_network_count:  279
learn step counter:  83501
dev_network_count:  279
learn step counter:  83551
dev_network_count:  279
learn step counter:  83601
dev_network_count:  279
learn step counter:  83651
dev_network_count:  279

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
280  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  83701
dev_network_count:  280
learn step counter:  83751
dev_network_count:  280
learn step counter:  83801
dev_network_count:  280
learn step counter:  83851
dev_network_count:  280
learn step counter:  83901
dev_network_count:  280
learn step counter:  83951
dev_network_count:  280
EPOCH %d 87
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
4398 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4399 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4400 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4401 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4402 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4403 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4404 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
281  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  84001
dev_network_count:  281
learn step counter:  84051
dev_network_count:  281
learn step counter:  84101
dev_network_count:  281
learn step counter:  84151
dev_network_count:  281
learn step counter:  84201
dev_network_count:  281
learn step counter:  84251
dev_network_count:  281

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
282  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  84301
dev_network_count:  282
learn step counter:  84351
dev_network_count:  282
learn step counter:  84401
dev_network_count:  282
learn step counter:  84451
dev_network_count:  282
learn step counter:  84501
dev_network_count:  282
learn step counter:  84551
dev_network_count:  282

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
283  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  84601
dev_network_count:  283
learn step counter:  84651
dev_network_count:  283
learn step counter:  84701
dev_network_count:  283
learn step counter:  84751
dev_network_count:  283
learn step counter:  84801
dev_network_count:  283
learn step counter:  84851
dev_network_count:  283

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
284  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  84901
dev_network_count:  284
learn step counter:  84951
dev_network_count:  284
EPOCH %d 88
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1891 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1892 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1893 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1894 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1895 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1896 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1897 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  85001
dev_network_count:  284
learn step counter:  85051
dev_network_count:  284
learn step counter:  85101
dev_network_count:  284
learn step counter:  85151
dev_network_count:  284

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
285  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  85201
dev_network_count:  285
learn step counter:  85251
dev_network_count:  285
learn step counter:  85301
dev_network_count:  285
learn step counter:  85351
dev_network_count:  285
learn step counter:  85401
dev_network_count:  285
learn step counter:  85451
dev_network_count:  285

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
286  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  85501
dev_network_count:  286
learn step counter:  85551
dev_network_count:  286
learn step counter:  85601
dev_network_count:  286
learn step counter:  85651
dev_network_count:  286
learn step counter:  85701
dev_network_count:  286
learn step counter:  85751
dev_network_count:  286

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
287  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  85801
dev_network_count:  287
learn step counter:  85851
dev_network_count:  287
learn step counter:  85901
dev_network_count:  287
learn step counter:  85951
dev_network_count:  287
EPOCH %d 89
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
4384 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4385 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4386 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4387 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4388 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4389 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4390 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  86001
dev_network_count:  287
learn step counter:  86051
dev_network_count:  287

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
288  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  86101
dev_network_count:  288
learn step counter:  86151
dev_network_count:  288
learn step counter:  86201
dev_network_count:  288
learn step counter:  86251
dev_network_count:  288
learn step counter:  86301
dev_network_count:  288
learn step counter:  86351
dev_network_count:  288

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
289  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  86401
dev_network_count:  289
learn step counter:  86451
dev_network_count:  289
learn step counter:  86501
dev_network_count:  289
learn step counter:  86551
dev_network_count:  289
learn step counter:  86601
dev_network_count:  289
learn step counter:  86651
dev_network_count:  289

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
290  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  86701
dev_network_count:  290
learn step counter:  86751
dev_network_count:  290
learn step counter:  86801
dev_network_count:  290
learn step counter:  86851
dev_network_count:  290
learn step counter:  86901
dev_network_count:  290
learn step counter:  86951
dev_network_count:  290
EPOCH %d 90
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1877 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1878 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1879 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1880 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1881 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1882 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1883 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
291  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  87001
dev_network_count:  291
learn step counter:  87051
dev_network_count:  291
learn step counter:  87101
dev_network_count:  291
learn step counter:  87151
dev_network_count:  291
learn step counter:  87201
dev_network_count:  291
learn step counter:  87251
dev_network_count:  291

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
292  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  87301
dev_network_count:  292
learn step counter:  87351
dev_network_count:  292
learn step counter:  87401
dev_network_count:  292
learn step counter:  87451
dev_network_count:  292
learn step counter:  87501
dev_network_count:  292
learn step counter:  87551
dev_network_count:  292

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
293  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  87601
dev_network_count:  293
learn step counter:  87651
dev_network_count:  293
learn step counter:  87701
dev_network_count:  293
learn step counter:  87751
dev_network_count:  293
learn step counter:  87801
dev_network_count:  293
learn step counter:  87851
dev_network_count:  293

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
294  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  87901
dev_network_count:  294
learn step counter:  87951
dev_network_count:  294
EPOCH %d 91
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
4370 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4371 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4372 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4373 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4374 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4375 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4376 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  88001
dev_network_count:  294
learn step counter:  88051
dev_network_count:  294
learn step counter:  88101
dev_network_count:  294
learn step counter:  88151
dev_network_count:  294

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
295  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  88201
dev_network_count:  295
learn step counter:  88251
dev_network_count:  295
learn step counter:  88301
dev_network_count:  295
learn step counter:  88351
dev_network_count:  295
learn step counter:  88401
dev_network_count:  295
learn step counter:  88451
dev_network_count:  295

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
296  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  88501
dev_network_count:  296
learn step counter:  88551
dev_network_count:  296
learn step counter:  88601
dev_network_count:  296
learn step counter:  88651
dev_network_count:  296
learn step counter:  88701
dev_network_count:  296
learn step counter:  88751
dev_network_count:  296

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
297  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  88801
dev_network_count:  297
learn step counter:  88851
dev_network_count:  297
learn step counter:  88901
dev_network_count:  297
learn step counter:  88951
dev_network_count:  297
EPOCH %d 92
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1863 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1864 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1865 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1866 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1867 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1868 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1869 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  89001
dev_network_count:  297
learn step counter:  89051
dev_network_count:  297

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
298  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  89101
dev_network_count:  298
learn step counter:  89151
dev_network_count:  298
learn step counter:  89201
dev_network_count:  298
learn step counter:  89251
dev_network_count:  298
learn step counter:  89301
dev_network_count:  298
learn step counter:  89351
dev_network_count:  298

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
299  r_total and score:  212.00000000000026 43.916235662953355
Current Bleu score is:  43.916235662953355
learn step counter:  89401
dev_network_count:  299
learn step counter:  89451
dev_network_count:  299
learn step counter:  89501
dev_network_count:  299
learn step counter:  89551
dev_network_count:  299
learn step counter:  89601
dev_network_count:  299
learn step counter:  89651
dev_network_count:  299

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
300  r_total and score:  210.00000000000026 47.22228823048435
Current Bleu score is:  47.22228823048435
learn step counter:  89701
dev_network_count:  300
learn step counter:  89751
dev_network_count:  300
learn step counter:  89801
dev_network_count:  300
learn step counter:  89851
dev_network_count:  300
learn step counter:  89901
dev_network_count:  300
learn step counter:  89951
dev_network_count:  300
EPOCH %d 93
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
4356 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4357 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4358 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4359 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4360 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4361 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4362 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
301  r_total and score:  210.00000000000026 47.22228823048435
Current Bleu score is:  47.22228823048435
learn step counter:  90001
dev_network_count:  301
learn step counter:  90051
dev_network_count:  301
learn step counter:  90101
dev_network_count:  301
learn step counter:  90151
dev_network_count:  301
learn step counter:  90201
dev_network_count:  301
learn step counter:  90251
dev_network_count:  301

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
302  r_total and score:  210.00000000000026 47.22228823048435
Current Bleu score is:  47.22228823048435
learn step counter:  90301
dev_network_count:  302
learn step counter:  90351
dev_network_count:  302
learn step counter:  90401
dev_network_count:  302
learn step counter:  90451
dev_network_count:  302
learn step counter:  90501
dev_network_count:  302
learn step counter:  90551
dev_network_count:  302

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
303  r_total and score:  210.00000000000026 47.22228823048435
Current Bleu score is:  47.22228823048435
learn step counter:  90601
dev_network_count:  303
learn step counter:  90651
dev_network_count:  303
learn step counter:  90701
dev_network_count:  303
learn step counter:  90751
dev_network_count:  303
learn step counter:  90801
dev_network_count:  303
learn step counter:  90851
dev_network_count:  303

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
304  r_total and score:  212.20000000000024 45.58499079354373
Current Bleu score is:  45.58499079354373
learn step counter:  90901
dev_network_count:  304
learn step counter:  90951
dev_network_count:  304
EPOCH %d 94
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1849 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1850 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1851 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1852 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1853 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1854 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1855 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  91001
dev_network_count:  304
learn step counter:  91051
dev_network_count:  304
learn step counter:  91101
dev_network_count:  304
learn step counter:  91151
dev_network_count:  304

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
305  r_total and score:  210.00000000000026 47.22228823048435
Current Bleu score is:  47.22228823048435
learn step counter:  91201
dev_network_count:  305
learn step counter:  91251
dev_network_count:  305
learn step counter:  91301
dev_network_count:  305
learn step counter:  91351
dev_network_count:  305
learn step counter:  91401
dev_network_count:  305
learn step counter:  91451
dev_network_count:  305

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
306  r_total and score:  210.00000000000026 47.22228823048435
Current Bleu score is:  47.22228823048435
learn step counter:  91501
dev_network_count:  306
learn step counter:  91551
dev_network_count:  306
learn step counter:  91601
dev_network_count:  306
learn step counter:  91651
dev_network_count:  306
learn step counter:  91701
dev_network_count:  306
learn step counter:  91751
dev_network_count:  306

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
307  r_total and score:  210.00000000000026 47.22228823048435
Current Bleu score is:  47.22228823048435
learn step counter:  91801
dev_network_count:  307
learn step counter:  91851
dev_network_count:  307
learn step counter:  91901
dev_network_count:  307
learn step counter:  91951
dev_network_count:  307
EPOCH %d 95
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
4342 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4343 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4344 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4345 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4346 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4347 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4348 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  92001
dev_network_count:  307
learn step counter:  92051
dev_network_count:  307

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
308  r_total and score:  210.00000000000026 47.22228823048435
Current Bleu score is:  47.22228823048435
learn step counter:  92101
dev_network_count:  308
learn step counter:  92151
dev_network_count:  308
learn step counter:  92201
dev_network_count:  308
learn step counter:  92251
dev_network_count:  308
learn step counter:  92301
dev_network_count:  308
learn step counter:  92351
dev_network_count:  308

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
309  r_total and score:  210.00000000000026 47.22228823048435
Current Bleu score is:  47.22228823048435
learn step counter:  92401
dev_network_count:  309
learn step counter:  92451
dev_network_count:  309
learn step counter:  92501
dev_network_count:  309
learn step counter:  92551
dev_network_count:  309
learn step counter:  92601
dev_network_count:  309
learn step counter:  92651
dev_network_count:  309

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
310  r_total and score:  210.00000000000026 47.22228823048435
Current Bleu score is:  47.22228823048435
learn step counter:  92701
dev_network_count:  310
learn step counter:  92751
dev_network_count:  310
learn step counter:  92801
dev_network_count:  310
learn step counter:  92851
dev_network_count:  310
learn step counter:  92901
dev_network_count:  310
learn step counter:  92951
dev_network_count:  310
EPOCH %d 96
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1835 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1836 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1837 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1838 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1839 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1840 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1841 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
311  r_total and score:  210.00000000000026 47.22228823048435
Current Bleu score is:  47.22228823048435
learn step counter:  93001
dev_network_count:  311
learn step counter:  93051
dev_network_count:  311
learn step counter:  93101
dev_network_count:  311
learn step counter:  93151
dev_network_count:  311
learn step counter:  93201
dev_network_count:  311
learn step counter:  93251
dev_network_count:  311

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
312  r_total and score:  210.00000000000026 47.22228823048435
Current Bleu score is:  47.22228823048435
learn step counter:  93301
dev_network_count:  312
learn step counter:  93351
dev_network_count:  312
learn step counter:  93401
dev_network_count:  312
learn step counter:  93451
dev_network_count:  312
learn step counter:  93501
dev_network_count:  312
learn step counter:  93551
dev_network_count:  312

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
313  r_total and score:  210.00000000000026 47.22228823048435
Current Bleu score is:  47.22228823048435
learn step counter:  93601
dev_network_count:  313
learn step counter:  93651
dev_network_count:  313
learn step counter:  93701
dev_network_count:  313
learn step counter:  93751
dev_network_count:  313
learn step counter:  93801
dev_network_count:  313
learn step counter:  93851
dev_network_count:  313

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
314  r_total and score:  210.00000000000026 47.22228823048435
Current Bleu score is:  47.22228823048435
learn step counter:  93901
dev_network_count:  314
learn step counter:  93951
dev_network_count:  314
EPOCH %d 97
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
4328 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4329 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4330 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4331 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4332 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4333 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4334 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  94001
dev_network_count:  314
learn step counter:  94051
dev_network_count:  314
learn step counter:  94101
dev_network_count:  314
learn step counter:  94151
dev_network_count:  314

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
315  r_total and score:  210.00000000000026 47.22228823048435
Current Bleu score is:  47.22228823048435
learn step counter:  94201
dev_network_count:  315
learn step counter:  94251
dev_network_count:  315
learn step counter:  94301
dev_network_count:  315
learn step counter:  94351
dev_network_count:  315
learn step counter:  94401
dev_network_count:  315
learn step counter:  94451
dev_network_count:  315

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
316  r_total and score:  210.00000000000026 47.22228823048435
Current Bleu score is:  47.22228823048435
learn step counter:  94501
dev_network_count:  316
learn step counter:  94551
dev_network_count:  316
learn step counter:  94601
dev_network_count:  316
learn step counter:  94651
dev_network_count:  316
learn step counter:  94701
dev_network_count:  316
learn step counter:  94751
dev_network_count:  316

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
317  r_total and score:  210.00000000000026 47.22228823048435
Current Bleu score is:  47.22228823048435
learn step counter:  94801
dev_network_count:  317
learn step counter:  94851
dev_network_count:  317
learn step counter:  94901
dev_network_count:  317
learn step counter:  94951
dev_network_count:  317
EPOCH %d 98
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1821 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1822 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1823 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1824 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1825 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1826 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1827 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  95001
dev_network_count:  317
learn step counter:  95051
dev_network_count:  317

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
318  r_total and score:  210.00000000000026 47.22228823048435
Current Bleu score is:  47.22228823048435
learn step counter:  95101
dev_network_count:  318
learn step counter:  95151
dev_network_count:  318
learn step counter:  95201
dev_network_count:  318
learn step counter:  95251
dev_network_count:  318
learn step counter:  95301
dev_network_count:  318
learn step counter:  95351
dev_network_count:  318

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
319  r_total and score:  210.00000000000026 47.22228823048435
Current Bleu score is:  47.22228823048435
learn step counter:  95401
dev_network_count:  319
learn step counter:  95451
dev_network_count:  319
learn step counter:  95501
dev_network_count:  319
learn step counter:  95551
dev_network_count:  319
learn step counter:  95601
dev_network_count:  319
learn step counter:  95651
dev_network_count:  319

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
320  r_total and score:  210.00000000000026 47.22228823048435
Current Bleu score is:  47.22228823048435
learn step counter:  95701
dev_network_count:  320
learn step counter:  95751
dev_network_count:  320
learn step counter:  95801
dev_network_count:  320
learn step counter:  95851
dev_network_count:  320
learn step counter:  95901
dev_network_count:  320
learn step counter:  95951
dev_network_count:  320
EPOCH %d 99
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
4314 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4315 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4316 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4317 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4318 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4319 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4320 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
321  r_total and score:  210.00000000000026 47.22228823048435
Current Bleu score is:  47.22228823048435
learn step counter:  96001
dev_network_count:  321
learn step counter:  96051
dev_network_count:  321
learn step counter:  96101
dev_network_count:  321
learn step counter:  96151
dev_network_count:  321
learn step counter:  96201
dev_network_count:  321
learn step counter:  96251
dev_network_count:  321

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
322  r_total and score:  210.00000000000026 47.22228823048435
Current Bleu score is:  47.22228823048435
learn step counter:  96301
dev_network_count:  322
learn step counter:  96351
dev_network_count:  322
learn step counter:  96401
dev_network_count:  322
learn step counter:  96451
dev_network_count:  322
learn step counter:  96501
dev_network_count:  322
learn step counter:  96551
dev_network_count:  322

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
323  r_total and score:  210.00000000000026 47.22228823048435
Current Bleu score is:  47.22228823048435
learn step counter:  96601
dev_network_count:  323
learn step counter:  96651
dev_network_count:  323
learn step counter:  96701
dev_network_count:  323
learn step counter:  96751
dev_network_count:  323
learn step counter:  96801
dev_network_count:  323
learn step counter:  96851
dev_network_count:  323

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
324  r_total and score:  212.20000000000024 45.58499079354373
Current Bleu score is:  45.58499079354373
learn step counter:  96901
dev_network_count:  324
learn step counter:  96951
dev_network_count:  324
EPOCH %d 100
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1807 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1808 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1809 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1810 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1811 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1812 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1813 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  97001
dev_network_count:  324
learn step counter:  97051
dev_network_count:  324
learn step counter:  97101
dev_network_count:  324
learn step counter:  97151
dev_network_count:  324

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
325  r_total and score:  210.00000000000026 47.22228823048435
Current Bleu score is:  47.22228823048435
learn step counter:  97201
dev_network_count:  325
learn step counter:  97251
dev_network_count:  325
learn step counter:  97301
dev_network_count:  325
learn step counter:  97351
dev_network_count:  325
learn step counter:  97401
dev_network_count:  325
learn step counter:  97451
dev_network_count:  325

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
326  r_total and score:  210.00000000000026 47.22228823048435
Current Bleu score is:  47.22228823048435
learn step counter:  97501
dev_network_count:  326
learn step counter:  97551
dev_network_count:  326
learn step counter:  97601
dev_network_count:  326
learn step counter:  97651
dev_network_count:  326
learn step counter:  97701
dev_network_count:  326
learn step counter:  97751
dev_network_count:  326

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
327  r_total and score:  212.20000000000024 45.58499079354373
Current Bleu score is:  45.58499079354373
learn step counter:  97801
dev_network_count:  327
learn step counter:  97851
dev_network_count:  327
learn step counter:  97901
dev_network_count:  327
learn step counter:  97951
dev_network_count:  327
EPOCH %d 101
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
4300 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4301 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4302 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4303 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4304 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4305 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4306 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  98001
dev_network_count:  327
learn step counter:  98051
dev_network_count:  327

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
328  r_total and score:  212.20000000000024 45.58499079354373
Current Bleu score is:  45.58499079354373
learn step counter:  98101
dev_network_count:  328
learn step counter:  98151
dev_network_count:  328
learn step counter:  98201
dev_network_count:  328
learn step counter:  98251
dev_network_count:  328
learn step counter:  98301
dev_network_count:  328
learn step counter:  98351
dev_network_count:  328

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
329  r_total and score:  210.00000000000026 47.22228823048435
Current Bleu score is:  47.22228823048435
learn step counter:  98401
dev_network_count:  329
learn step counter:  98451
dev_network_count:  329
learn step counter:  98501
dev_network_count:  329
learn step counter:  98551
dev_network_count:  329
learn step counter:  98601
dev_network_count:  329
learn step counter:  98651
dev_network_count:  329

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
330  r_total and score:  212.20000000000024 45.58499079354373
Current Bleu score is:  45.58499079354373
learn step counter:  98701
dev_network_count:  330
learn step counter:  98751
dev_network_count:  330
learn step counter:  98801
dev_network_count:  330
learn step counter:  98851
dev_network_count:  330
learn step counter:  98901
dev_network_count:  330
learn step counter:  98951
dev_network_count:  330
EPOCH %d 102
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1793 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1794 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1795 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1796 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1797 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1798 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1799 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
331  r_total and score:  212.20000000000024 45.58499079354373
Current Bleu score is:  45.58499079354373
learn step counter:  99001
dev_network_count:  331
learn step counter:  99051
dev_network_count:  331
learn step counter:  99101
dev_network_count:  331
learn step counter:  99151
dev_network_count:  331
learn step counter:  99201
dev_network_count:  331
learn step counter:  99251
dev_network_count:  331

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
332  r_total and score:  212.20000000000024 45.58499079354373
Current Bleu score is:  45.58499079354373
learn step counter:  99301
dev_network_count:  332
learn step counter:  99351
dev_network_count:  332
learn step counter:  99401
dev_network_count:  332
learn step counter:  99451
dev_network_count:  332
learn step counter:  99501
dev_network_count:  332
learn step counter:  99551
dev_network_count:  332

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
333  r_total and score:  210.00000000000026 47.22228823048435
Current Bleu score is:  47.22228823048435
learn step counter:  99601
dev_network_count:  333
learn step counter:  99651
dev_network_count:  333
learn step counter:  99701
dev_network_count:  333
learn step counter:  99751
dev_network_count:  333
learn step counter:  99801
dev_network_count:  333
learn step counter:  99851
dev_network_count:  333

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
334  r_total and score:  210.00000000000026 47.22228823048435
Current Bleu score is:  47.22228823048435
learn step counter:  99901
dev_network_count:  334
learn step counter:  99951
dev_network_count:  334
EPOCH %d 103
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
4286 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4287 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4288 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4289 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4290 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4291 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4292 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  100001
dev_network_count:  334
learn step counter:  100051
dev_network_count:  334
learn step counter:  100101
dev_network_count:  334
learn step counter:  100151
dev_network_count:  334

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
335  r_total and score:  213.0000000000002 46.487751516645375
Current Bleu score is:  46.487751516645375
learn step counter:  100201
dev_network_count:  335
learn step counter:  100251
dev_network_count:  335
learn step counter:  100301
dev_network_count:  335
learn step counter:  100351
dev_network_count:  335
learn step counter:  100401
dev_network_count:  335
learn step counter:  100451
dev_network_count:  335

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
336  r_total and score:  210.00000000000026 47.22228823048435
Current Bleu score is:  47.22228823048435
learn step counter:  100501
dev_network_count:  336
learn step counter:  100551
dev_network_count:  336
learn step counter:  100601
dev_network_count:  336
learn step counter:  100651
dev_network_count:  336
learn step counter:  100701
dev_network_count:  336
learn step counter:  100751
dev_network_count:  336

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
337  r_total and score:  210.00000000000026 47.22228823048435
Current Bleu score is:  47.22228823048435
learn step counter:  100801
dev_network_count:  337
learn step counter:  100851
dev_network_count:  337
learn step counter:  100901
dev_network_count:  337
learn step counter:  100951
dev_network_count:  337
EPOCH %d 104
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1779 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1780 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1781 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1782 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1783 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1784 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1785 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  101001
dev_network_count:  337
learn step counter:  101051
dev_network_count:  337

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
338  r_total and score:  212.20000000000024 45.58499079354373
Current Bleu score is:  45.58499079354373
learn step counter:  101101
dev_network_count:  338
learn step counter:  101151
dev_network_count:  338
learn step counter:  101201
dev_network_count:  338
learn step counter:  101251
dev_network_count:  338
learn step counter:  101301
dev_network_count:  338
learn step counter:  101351
dev_network_count:  338

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
339  r_total and score:  210.00000000000026 47.22228823048435
Current Bleu score is:  47.22228823048435
learn step counter:  101401
dev_network_count:  339
learn step counter:  101451
dev_network_count:  339
learn step counter:  101501
dev_network_count:  339
learn step counter:  101551
dev_network_count:  339
learn step counter:  101601
dev_network_count:  339
learn step counter:  101651
dev_network_count:  339

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
340  r_total and score:  212.20000000000024 45.58499079354373
Current Bleu score is:  45.58499079354373
learn step counter:  101701
dev_network_count:  340
learn step counter:  101751
dev_network_count:  340
learn step counter:  101801
dev_network_count:  340
learn step counter:  101851
dev_network_count:  340
learn step counter:  101901
dev_network_count:  340
learn step counter:  101951
dev_network_count:  340
EPOCH %d 105
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
4272 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4273 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4274 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4275 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4276 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4277 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4278 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
341  r_total and score:  210.00000000000026 47.22228823048435
Current Bleu score is:  47.22228823048435
learn step counter:  102001
dev_network_count:  341
learn step counter:  102051
dev_network_count:  341
learn step counter:  102101
dev_network_count:  341
learn step counter:  102151
dev_network_count:  341
learn step counter:  102201
dev_network_count:  341
learn step counter:  102251
dev_network_count:  341

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
342  r_total and score:  210.00000000000026 47.22228823048435
Current Bleu score is:  47.22228823048435
learn step counter:  102301
dev_network_count:  342
learn step counter:  102351
dev_network_count:  342
learn step counter:  102401
dev_network_count:  342
learn step counter:  102451
dev_network_count:  342
learn step counter:  102501
dev_network_count:  342
learn step counter:  102551
dev_network_count:  342

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
343  r_total and score:  210.00000000000026 47.22228823048435
Current Bleu score is:  47.22228823048435
learn step counter:  102601
dev_network_count:  343
learn step counter:  102651
dev_network_count:  343
learn step counter:  102701
dev_network_count:  343
learn step counter:  102751
dev_network_count:  343
learn step counter:  102801
dev_network_count:  343
learn step counter:  102851
dev_network_count:  343

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
344  r_total and score:  210.00000000000026 47.22228823048435
Current Bleu score is:  47.22228823048435
learn step counter:  102901
dev_network_count:  344
learn step counter:  102951
dev_network_count:  344
EPOCH %d 106
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1765 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1766 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1767 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1768 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1769 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1770 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1771 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  103001
dev_network_count:  344
learn step counter:  103051
dev_network_count:  344
learn step counter:  103101
dev_network_count:  344
learn step counter:  103151
dev_network_count:  344

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
345  r_total and score:  212.20000000000024 45.58499079354373
Current Bleu score is:  45.58499079354373
learn step counter:  103201
dev_network_count:  345
learn step counter:  103251
dev_network_count:  345
learn step counter:  103301
dev_network_count:  345
learn step counter:  103351
dev_network_count:  345
learn step counter:  103401
dev_network_count:  345
learn step counter:  103451
dev_network_count:  345

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
346  r_total and score:  212.20000000000024 45.58499079354373
Current Bleu score is:  45.58499079354373
learn step counter:  103501
dev_network_count:  346
learn step counter:  103551
dev_network_count:  346
learn step counter:  103601
dev_network_count:  346
learn step counter:  103651
dev_network_count:  346
learn step counter:  103701
dev_network_count:  346
learn step counter:  103751
dev_network_count:  346

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
347  r_total and score:  210.00000000000026 47.22228823048435
Current Bleu score is:  47.22228823048435
learn step counter:  103801
dev_network_count:  347
learn step counter:  103851
dev_network_count:  347
learn step counter:  103901
dev_network_count:  347
learn step counter:  103951
dev_network_count:  347
EPOCH %d 107
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
4258 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4259 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4260 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4261 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4262 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4263 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4264 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  104001
dev_network_count:  347
learn step counter:  104051
dev_network_count:  347

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
348  r_total and score:  212.20000000000024 45.58499079354373
Current Bleu score is:  45.58499079354373
learn step counter:  104101
dev_network_count:  348
learn step counter:  104151
dev_network_count:  348
learn step counter:  104201
dev_network_count:  348
learn step counter:  104251
dev_network_count:  348
learn step counter:  104301
dev_network_count:  348
learn step counter:  104351
dev_network_count:  348

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
349  r_total and score:  212.20000000000024 45.58499079354373
Current Bleu score is:  45.58499079354373
learn step counter:  104401
dev_network_count:  349
learn step counter:  104451
dev_network_count:  349
learn step counter:  104501
dev_network_count:  349
learn step counter:  104551
dev_network_count:  349
learn step counter:  104601
dev_network_count:  349
learn step counter:  104651
dev_network_count:  349

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
350  r_total and score:  212.20000000000024 45.58499079354373
Current Bleu score is:  45.58499079354373
learn step counter:  104701
dev_network_count:  350
learn step counter:  104751
dev_network_count:  350
learn step counter:  104801
dev_network_count:  350
learn step counter:  104851
dev_network_count:  350
learn step counter:  104901
dev_network_count:  350
learn step counter:  104951
dev_network_count:  350
EPOCH %d 108
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1751 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1752 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1753 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1754 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1755 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1756 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1757 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
351  r_total and score:  212.20000000000024 45.58499079354373
Current Bleu score is:  45.58499079354373
learn step counter:  105001
dev_network_count:  351
learn step counter:  105051
dev_network_count:  351
learn step counter:  105101
dev_network_count:  351
learn step counter:  105151
dev_network_count:  351
learn step counter:  105201
dev_network_count:  351
learn step counter:  105251
dev_network_count:  351

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
352  r_total and score:  212.20000000000024 45.58499079354373
Current Bleu score is:  45.58499079354373
learn step counter:  105301
dev_network_count:  352
learn step counter:  105351
dev_network_count:  352
learn step counter:  105401
dev_network_count:  352
learn step counter:  105451
dev_network_count:  352
learn step counter:  105501
dev_network_count:  352
learn step counter:  105551
dev_network_count:  352

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
353  r_total and score:  210.00000000000026 47.22228823048435
Current Bleu score is:  47.22228823048435
learn step counter:  105601
dev_network_count:  353
learn step counter:  105651
dev_network_count:  353
learn step counter:  105701
dev_network_count:  353
learn step counter:  105751
dev_network_count:  353
learn step counter:  105801
dev_network_count:  353
learn step counter:  105851
dev_network_count:  353

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
354  r_total and score:  212.20000000000024 45.58499079354373
Current Bleu score is:  45.58499079354373
learn step counter:  105901
dev_network_count:  354
learn step counter:  105951
dev_network_count:  354
EPOCH %d 109
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
4244 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4245 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4246 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4247 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4248 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4249 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4250 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  106001
dev_network_count:  354
learn step counter:  106051
dev_network_count:  354
learn step counter:  106101
dev_network_count:  354
learn step counter:  106151
dev_network_count:  354

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
355  r_total and score:  212.20000000000024 45.58499079354373
Current Bleu score is:  45.58499079354373
learn step counter:  106201
dev_network_count:  355
learn step counter:  106251
dev_network_count:  355
learn step counter:  106301
dev_network_count:  355
learn step counter:  106351
dev_network_count:  355
learn step counter:  106401
dev_network_count:  355
learn step counter:  106451
dev_network_count:  355

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
356  r_total and score:  210.00000000000026 47.22228823048435
Current Bleu score is:  47.22228823048435
learn step counter:  106501
dev_network_count:  356
learn step counter:  106551
dev_network_count:  356
learn step counter:  106601
dev_network_count:  356
learn step counter:  106651
dev_network_count:  356
learn step counter:  106701
dev_network_count:  356
learn step counter:  106751
dev_network_count:  356

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
357  r_total and score:  212.20000000000024 45.58499079354373
Current Bleu score is:  45.58499079354373
learn step counter:  106801
dev_network_count:  357
learn step counter:  106851
dev_network_count:  357
learn step counter:  106901
dev_network_count:  357
learn step counter:  106951
dev_network_count:  357
EPOCH %d 110
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1737 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1738 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1739 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1740 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1741 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1742 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1743 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  107001
dev_network_count:  357
learn step counter:  107051
dev_network_count:  357

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
358  r_total and score:  212.20000000000024 45.58499079354373
Current Bleu score is:  45.58499079354373
learn step counter:  107101
dev_network_count:  358
learn step counter:  107151
dev_network_count:  358
learn step counter:  107201
dev_network_count:  358
learn step counter:  107251
dev_network_count:  358
learn step counter:  107301
dev_network_count:  358
learn step counter:  107351
dev_network_count:  358

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
359  r_total and score:  212.20000000000024 45.58499079354373
Current Bleu score is:  45.58499079354373
learn step counter:  107401
dev_network_count:  359
learn step counter:  107451
dev_network_count:  359
learn step counter:  107501
dev_network_count:  359
learn step counter:  107551
dev_network_count:  359
learn step counter:  107601
dev_network_count:  359
learn step counter:  107651
dev_network_count:  359

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
360  r_total and score:  212.20000000000024 45.58499079354373
Current Bleu score is:  45.58499079354373
learn step counter:  107701
dev_network_count:  360
learn step counter:  107751
dev_network_count:  360
learn step counter:  107801
dev_network_count:  360
learn step counter:  107851
dev_network_count:  360
learn step counter:  107901
dev_network_count:  360
learn step counter:  107951
dev_network_count:  360
EPOCH %d 111
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
4230 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4231 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4232 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4233 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4234 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4235 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4236 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
361  r_total and score:  212.20000000000024 45.58499079354373
Current Bleu score is:  45.58499079354373
learn step counter:  108001
dev_network_count:  361
learn step counter:  108051
dev_network_count:  361
learn step counter:  108101
dev_network_count:  361
learn step counter:  108151
dev_network_count:  361
learn step counter:  108201
dev_network_count:  361
learn step counter:  108251
dev_network_count:  361

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
362  r_total and score:  212.20000000000024 45.58499079354373
Current Bleu score is:  45.58499079354373
learn step counter:  108301
dev_network_count:  362
learn step counter:  108351
dev_network_count:  362
learn step counter:  108401
dev_network_count:  362
learn step counter:  108451
dev_network_count:  362
learn step counter:  108501
dev_network_count:  362
learn step counter:  108551
dev_network_count:  362

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
363  r_total and score:  212.20000000000024 45.58499079354373
Current Bleu score is:  45.58499079354373
learn step counter:  108601
dev_network_count:  363
learn step counter:  108651
dev_network_count:  363
learn step counter:  108701
dev_network_count:  363
learn step counter:  108751
dev_network_count:  363
learn step counter:  108801
dev_network_count:  363
learn step counter:  108851
dev_network_count:  363

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
364  r_total and score:  212.20000000000024 45.58499079354373
Current Bleu score is:  45.58499079354373
learn step counter:  108901
dev_network_count:  364
learn step counter:  108951
dev_network_count:  364
EPOCH %d 112
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1723 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1724 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1725 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1726 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1727 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1728 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1729 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  109001
dev_network_count:  364
learn step counter:  109051
dev_network_count:  364
learn step counter:  109101
dev_network_count:  364
learn step counter:  109151
dev_network_count:  364

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
365  r_total and score:  212.20000000000024 45.58499079354373
Current Bleu score is:  45.58499079354373
learn step counter:  109201
dev_network_count:  365
learn step counter:  109251
dev_network_count:  365
learn step counter:  109301
dev_network_count:  365
learn step counter:  109351
dev_network_count:  365
learn step counter:  109401
dev_network_count:  365
learn step counter:  109451
dev_network_count:  365

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
366  r_total and score:  212.20000000000024 45.58499079354373
Current Bleu score is:  45.58499079354373
learn step counter:  109501
dev_network_count:  366
learn step counter:  109551
dev_network_count:  366
learn step counter:  109601
dev_network_count:  366
learn step counter:  109651
dev_network_count:  366
learn step counter:  109701
dev_network_count:  366
learn step counter:  109751
dev_network_count:  366

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
367  r_total and score:  212.20000000000024 45.58499079354373
Current Bleu score is:  45.58499079354373
learn step counter:  109801
dev_network_count:  367
learn step counter:  109851
dev_network_count:  367
learn step counter:  109901
dev_network_count:  367
learn step counter:  109951
dev_network_count:  367
EPOCH %d 113
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
4216 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4217 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4218 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4219 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4220 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4221 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4222 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  110001
dev_network_count:  367
learn step counter:  110051
dev_network_count:  367

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
368  r_total and score:  213.0000000000002 46.487751516645375
Current Bleu score is:  46.487751516645375
learn step counter:  110101
dev_network_count:  368
learn step counter:  110151
dev_network_count:  368
learn step counter:  110201
dev_network_count:  368
learn step counter:  110251
dev_network_count:  368
learn step counter:  110301
dev_network_count:  368
learn step counter:  110351
dev_network_count:  368

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
369  r_total and score:  212.20000000000024 45.58499079354373
Current Bleu score is:  45.58499079354373
learn step counter:  110401
dev_network_count:  369
learn step counter:  110451
dev_network_count:  369
learn step counter:  110501
dev_network_count:  369
learn step counter:  110551
dev_network_count:  369
learn step counter:  110601
dev_network_count:  369
learn step counter:  110651
dev_network_count:  369

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
370  r_total and score:  212.20000000000024 45.58499079354373
Current Bleu score is:  45.58499079354373
learn step counter:  110701
dev_network_count:  370
learn step counter:  110751
dev_network_count:  370
learn step counter:  110801
dev_network_count:  370
learn step counter:  110851
dev_network_count:  370
learn step counter:  110901
dev_network_count:  370
learn step counter:  110951
dev_network_count:  370
EPOCH %d 114
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1709 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1710 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1711 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1712 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1713 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1714 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1715 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
371  r_total and score:  212.20000000000024 45.58499079354373
Current Bleu score is:  45.58499079354373
learn step counter:  111001
dev_network_count:  371
learn step counter:  111051
dev_network_count:  371
learn step counter:  111101
dev_network_count:  371
learn step counter:  111151
dev_network_count:  371
learn step counter:  111201
dev_network_count:  371
learn step counter:  111251
dev_network_count:  371

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
372  r_total and score:  212.20000000000024 45.58499079354373
Current Bleu score is:  45.58499079354373
learn step counter:  111301
dev_network_count:  372
learn step counter:  111351
dev_network_count:  372
learn step counter:  111401
dev_network_count:  372
learn step counter:  111451
dev_network_count:  372
learn step counter:  111501
dev_network_count:  372
learn step counter:  111551
dev_network_count:  372

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
373  r_total and score:  212.20000000000024 45.58499079354373
Current Bleu score is:  45.58499079354373
learn step counter:  111601
dev_network_count:  373
learn step counter:  111651
dev_network_count:  373
learn step counter:  111701
dev_network_count:  373
learn step counter:  111751
dev_network_count:  373
learn step counter:  111801
dev_network_count:  373
learn step counter:  111851
dev_network_count:  373

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
374  r_total and score:  212.20000000000024 45.58499079354373
Current Bleu score is:  45.58499079354373
learn step counter:  111901
dev_network_count:  374
learn step counter:  111951
dev_network_count:  374
EPOCH %d 115
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
4202 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4203 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4204 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4205 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4206 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4207 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4208 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  112001
dev_network_count:  374
learn step counter:  112051
dev_network_count:  374
learn step counter:  112101
dev_network_count:  374
learn step counter:  112151
dev_network_count:  374

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
375  r_total and score:  212.20000000000024 45.58499079354373
Current Bleu score is:  45.58499079354373
learn step counter:  112201
dev_network_count:  375
learn step counter:  112251
dev_network_count:  375
learn step counter:  112301
dev_network_count:  375
learn step counter:  112351
dev_network_count:  375
learn step counter:  112401
dev_network_count:  375
learn step counter:  112451
dev_network_count:  375

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
376  r_total and score:  212.20000000000024 45.58499079354373
Current Bleu score is:  45.58499079354373
learn step counter:  112501
dev_network_count:  376
learn step counter:  112551
dev_network_count:  376
learn step counter:  112601
dev_network_count:  376
learn step counter:  112651
dev_network_count:  376
learn step counter:  112701
dev_network_count:  376
learn step counter:  112751
dev_network_count:  376

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
377  r_total and score:  212.20000000000024 45.58499079354373
Current Bleu score is:  45.58499079354373
learn step counter:  112801
dev_network_count:  377
learn step counter:  112851
dev_network_count:  377
learn step counter:  112901
dev_network_count:  377
learn step counter:  112951
dev_network_count:  377
EPOCH %d 116
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1695 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1696 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1697 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1698 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1699 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1700 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1701 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  113001
dev_network_count:  377
learn step counter:  113051
dev_network_count:  377

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
378  r_total and score:  212.20000000000024 45.58499079354373
Current Bleu score is:  45.58499079354373
learn step counter:  113101
dev_network_count:  378
learn step counter:  113151
dev_network_count:  378
learn step counter:  113201
dev_network_count:  378
learn step counter:  113251
dev_network_count:  378
learn step counter:  113301
dev_network_count:  378
learn step counter:  113351
dev_network_count:  378

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
379  r_total and score:  212.20000000000024 45.58499079354373
Current Bleu score is:  45.58499079354373
learn step counter:  113401
dev_network_count:  379
learn step counter:  113451
dev_network_count:  379
learn step counter:  113501
dev_network_count:  379
learn step counter:  113551
dev_network_count:  379
learn step counter:  113601
dev_network_count:  379
learn step counter:  113651
dev_network_count:  379

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
380  r_total and score:  212.20000000000024 45.58499079354373
Current Bleu score is:  45.58499079354373
learn step counter:  113701
dev_network_count:  380
learn step counter:  113751
dev_network_count:  380
learn step counter:  113801
dev_network_count:  380
learn step counter:  113851
dev_network_count:  380
learn step counter:  113901
dev_network_count:  380
learn step counter:  113951
dev_network_count:  380
EPOCH %d 117
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
4188 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4189 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4190 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4191 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4192 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4193 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4194 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
381  r_total and score:  212.0000000000002 48.148303600573065
Current Bleu score is:  48.148303600573065
learn step counter:  114001
dev_network_count:  381
learn step counter:  114051
dev_network_count:  381
learn step counter:  114101
dev_network_count:  381
learn step counter:  114151
dev_network_count:  381
learn step counter:  114201
dev_network_count:  381
learn step counter:  114251
dev_network_count:  381

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
382  r_total and score:  212.20000000000024 45.58499079354373
Current Bleu score is:  45.58499079354373
learn step counter:  114301
dev_network_count:  382
learn step counter:  114351
dev_network_count:  382
learn step counter:  114401
dev_network_count:  382
learn step counter:  114451
dev_network_count:  382
learn step counter:  114501
dev_network_count:  382
learn step counter:  114551
dev_network_count:  382

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
383  r_total and score:  212.20000000000024 45.58499079354373
Current Bleu score is:  45.58499079354373
learn step counter:  114601
dev_network_count:  383
learn step counter:  114651
dev_network_count:  383
learn step counter:  114701
dev_network_count:  383
learn step counter:  114751
dev_network_count:  383
learn step counter:  114801
dev_network_count:  383
learn step counter:  114851
dev_network_count:  383

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
384  r_total and score:  212.20000000000024 45.58499079354373
Current Bleu score is:  45.58499079354373
learn step counter:  114901
dev_network_count:  384
learn step counter:  114951
dev_network_count:  384
EPOCH %d 118
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1681 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1682 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1683 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1684 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1685 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1686 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1687 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  115001
dev_network_count:  384
learn step counter:  115051
dev_network_count:  384
learn step counter:  115101
dev_network_count:  384
learn step counter:  115151
dev_network_count:  384

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
385  r_total and score:  212.20000000000024 45.58499079354373
Current Bleu score is:  45.58499079354373
learn step counter:  115201
dev_network_count:  385
learn step counter:  115251
dev_network_count:  385
learn step counter:  115301
dev_network_count:  385
learn step counter:  115351
dev_network_count:  385
learn step counter:  115401
dev_network_count:  385
learn step counter:  115451
dev_network_count:  385

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
386  r_total and score:  212.20000000000024 45.58499079354373
Current Bleu score is:  45.58499079354373
learn step counter:  115501
dev_network_count:  386
learn step counter:  115551
dev_network_count:  386
learn step counter:  115601
dev_network_count:  386
learn step counter:  115651
dev_network_count:  386
learn step counter:  115701
dev_network_count:  386
learn step counter:  115751
dev_network_count:  386

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
387  r_total and score:  212.20000000000024 45.58499079354373
Current Bleu score is:  45.58499079354373
learn step counter:  115801
dev_network_count:  387
learn step counter:  115851
dev_network_count:  387
learn step counter:  115901
dev_network_count:  387
learn step counter:  115951
dev_network_count:  387
EPOCH %d 119
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
4174 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4175 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4176 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4177 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4178 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4179 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4180 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  116001
dev_network_count:  387
learn step counter:  116051
dev_network_count:  387

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
388  r_total and score:  212.20000000000024 45.58499079354373
Current Bleu score is:  45.58499079354373
learn step counter:  116101
dev_network_count:  388
learn step counter:  116151
dev_network_count:  388
learn step counter:  116201
dev_network_count:  388
learn step counter:  116251
dev_network_count:  388
learn step counter:  116301
dev_network_count:  388
learn step counter:  116351
dev_network_count:  388

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
389  r_total and score:  212.20000000000024 45.58499079354373
Current Bleu score is:  45.58499079354373
learn step counter:  116401
dev_network_count:  389
learn step counter:  116451
dev_network_count:  389
learn step counter:  116501
dev_network_count:  389
learn step counter:  116551
dev_network_count:  389
learn step counter:  116601
dev_network_count:  389
learn step counter:  116651
dev_network_count:  389

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
390  r_total and score:  212.20000000000024 45.58499079354373
Current Bleu score is:  45.58499079354373
learn step counter:  116701
dev_network_count:  390
learn step counter:  116751
dev_network_count:  390
learn step counter:  116801
dev_network_count:  390
learn step counter:  116851
dev_network_count:  390
learn step counter:  116901
dev_network_count:  390
learn step counter:  116951
dev_network_count:  390
EPOCH %d 120
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1667 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1668 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1669 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1670 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1671 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1672 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1673 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
391  r_total and score:  213.0000000000002 46.487751516645375
Current Bleu score is:  46.487751516645375
learn step counter:  117001
dev_network_count:  391
learn step counter:  117051
dev_network_count:  391
learn step counter:  117101
dev_network_count:  391
learn step counter:  117151
dev_network_count:  391
learn step counter:  117201
dev_network_count:  391
learn step counter:  117251
dev_network_count:  391

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
392  r_total and score:  213.0000000000002 46.487751516645375
Current Bleu score is:  46.487751516645375
learn step counter:  117301
dev_network_count:  392
learn step counter:  117351
dev_network_count:  392
learn step counter:  117401
dev_network_count:  392
learn step counter:  117451
dev_network_count:  392
learn step counter:  117501
dev_network_count:  392
learn step counter:  117551
dev_network_count:  392

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
393  r_total and score:  212.20000000000024 45.58499079354373
Current Bleu score is:  45.58499079354373
learn step counter:  117601
dev_network_count:  393
learn step counter:  117651
dev_network_count:  393
learn step counter:  117701
dev_network_count:  393
learn step counter:  117751
dev_network_count:  393
learn step counter:  117801
dev_network_count:  393
learn step counter:  117851
dev_network_count:  393

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
394  r_total and score:  212.20000000000024 45.58499079354373
Current Bleu score is:  45.58499079354373
learn step counter:  117901
dev_network_count:  394
learn step counter:  117951
dev_network_count:  394
EPOCH %d 121
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
4160 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4161 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4162 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4163 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4164 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4165 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4166 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  118001
dev_network_count:  394
learn step counter:  118051
dev_network_count:  394
learn step counter:  118101
dev_network_count:  394
learn step counter:  118151
dev_network_count:  394

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
395  r_total and score:  212.20000000000024 45.58499079354373
Current Bleu score is:  45.58499079354373
learn step counter:  118201
dev_network_count:  395
learn step counter:  118251
dev_network_count:  395
learn step counter:  118301
dev_network_count:  395
learn step counter:  118351
dev_network_count:  395
learn step counter:  118401
dev_network_count:  395
learn step counter:  118451
dev_network_count:  395

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
396  r_total and score:  212.20000000000024 45.58499079354373
Current Bleu score is:  45.58499079354373
learn step counter:  118501
dev_network_count:  396
learn step counter:  118551
dev_network_count:  396
learn step counter:  118601
dev_network_count:  396
learn step counter:  118651
dev_network_count:  396
learn step counter:  118701
dev_network_count:  396
learn step counter:  118751
dev_network_count:  396

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
397  r_total and score:  212.20000000000024 45.58499079354373
Current Bleu score is:  45.58499079354373
learn step counter:  118801
dev_network_count:  397
learn step counter:  118851
dev_network_count:  397
learn step counter:  118901
dev_network_count:  397
learn step counter:  118951
dev_network_count:  397
EPOCH %d 122
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1653 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1654 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1655 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1656 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1657 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1658 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1659 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  119001
dev_network_count:  397
learn step counter:  119051
dev_network_count:  397

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
398  r_total and score:  211.20000000000024 47.22228823048435
Current Bleu score is:  47.22228823048435
learn step counter:  119101
dev_network_count:  398
learn step counter:  119151
dev_network_count:  398
learn step counter:  119201
dev_network_count:  398
learn step counter:  119251
dev_network_count:  398
learn step counter:  119301
dev_network_count:  398
learn step counter:  119351
dev_network_count:  398

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
399  r_total and score:  213.0000000000002 46.487751516645375
Current Bleu score is:  46.487751516645375
learn step counter:  119401
dev_network_count:  399
learn step counter:  119451
dev_network_count:  399
learn step counter:  119501
dev_network_count:  399
learn step counter:  119551
dev_network_count:  399
learn step counter:  119601
dev_network_count:  399
learn step counter:  119651
dev_network_count:  399

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
400  r_total and score:  212.0000000000002 48.148303600573065
Current Bleu score is:  48.148303600573065
learn step counter:  119701
dev_network_count:  400
learn step counter:  119751
dev_network_count:  400
learn step counter:  119801
dev_network_count:  400
learn step counter:  119851
dev_network_count:  400
learn step counter:  119901
dev_network_count:  400
learn step counter:  119951
dev_network_count:  400
EPOCH %d 123
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
4146 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4147 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4148 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4149 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4150 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4151 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4152 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
401  r_total and score:  213.0000000000002 46.487751516645375
Current Bleu score is:  46.487751516645375
learn step counter:  120001
dev_network_count:  401
learn step counter:  120051
dev_network_count:  401
learn step counter:  120101
dev_network_count:  401
learn step counter:  120151
dev_network_count:  401
learn step counter:  120201
dev_network_count:  401
learn step counter:  120251
dev_network_count:  401

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
402  r_total and score:  213.0000000000002 46.487751516645375
Current Bleu score is:  46.487751516645375
learn step counter:  120301
dev_network_count:  402
learn step counter:  120351
dev_network_count:  402
learn step counter:  120401
dev_network_count:  402
learn step counter:  120451
dev_network_count:  402
learn step counter:  120501
dev_network_count:  402
learn step counter:  120551
dev_network_count:  402

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
403  r_total and score:  213.0000000000002 46.487751516645375
Current Bleu score is:  46.487751516645375
learn step counter:  120601
dev_network_count:  403
learn step counter:  120651
dev_network_count:  403
learn step counter:  120701
dev_network_count:  403
learn step counter:  120751
dev_network_count:  403
learn step counter:  120801
dev_network_count:  403
learn step counter:  120851
dev_network_count:  403

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
404  r_total and score:  213.0000000000002 46.487751516645375
Current Bleu score is:  46.487751516645375
learn step counter:  120901
dev_network_count:  404
learn step counter:  120951
dev_network_count:  404
EPOCH %d 124
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1639 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1640 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1641 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1642 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1643 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1644 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1645 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  121001
dev_network_count:  404
learn step counter:  121051
dev_network_count:  404
learn step counter:  121101
dev_network_count:  404
learn step counter:  121151
dev_network_count:  404

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
405  r_total and score:  213.0000000000002 46.487751516645375
Current Bleu score is:  46.487751516645375
learn step counter:  121201
dev_network_count:  405
learn step counter:  121251
dev_network_count:  405
learn step counter:  121301
dev_network_count:  405
learn step counter:  121351
dev_network_count:  405
learn step counter:  121401
dev_network_count:  405
learn step counter:  121451
dev_network_count:  405

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
406  r_total and score:  213.0000000000002 46.487751516645375
Current Bleu score is:  46.487751516645375
learn step counter:  121501
dev_network_count:  406
learn step counter:  121551
dev_network_count:  406
learn step counter:  121601
dev_network_count:  406
learn step counter:  121651
dev_network_count:  406
learn step counter:  121701
dev_network_count:  406
learn step counter:  121751
dev_network_count:  406

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
407  r_total and score:  213.0000000000002 46.487751516645375
Current Bleu score is:  46.487751516645375
learn step counter:  121801
dev_network_count:  407
learn step counter:  121851
dev_network_count:  407
learn step counter:  121901
dev_network_count:  407
learn step counter:  121951
dev_network_count:  407
EPOCH %d 125
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
4132 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4133 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4134 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4135 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4136 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4137 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4138 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  122001
dev_network_count:  407
learn step counter:  122051
dev_network_count:  407

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
408  r_total and score:  213.0000000000002 46.487751516645375
Current Bleu score is:  46.487751516645375
learn step counter:  122101
dev_network_count:  408
learn step counter:  122151
dev_network_count:  408
learn step counter:  122201
dev_network_count:  408
learn step counter:  122251
dev_network_count:  408
learn step counter:  122301
dev_network_count:  408
learn step counter:  122351
dev_network_count:  408

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
409  r_total and score:  212.0000000000002 48.148303600573065
Current Bleu score is:  48.148303600573065
learn step counter:  122401
dev_network_count:  409
learn step counter:  122451
dev_network_count:  409
learn step counter:  122501
dev_network_count:  409
learn step counter:  122551
dev_network_count:  409
learn step counter:  122601
dev_network_count:  409
learn step counter:  122651
dev_network_count:  409

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
410  r_total and score:  213.0000000000002 46.487751516645375
Current Bleu score is:  46.487751516645375
learn step counter:  122701
dev_network_count:  410
learn step counter:  122751
dev_network_count:  410
learn step counter:  122801
dev_network_count:  410
learn step counter:  122851
dev_network_count:  410
learn step counter:  122901
dev_network_count:  410
learn step counter:  122951
dev_network_count:  410
EPOCH %d 126
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1625 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1626 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1627 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1628 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1629 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1630 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1631 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
411  r_total and score:  213.0000000000002 46.487751516645375
Current Bleu score is:  46.487751516645375
learn step counter:  123001
dev_network_count:  411
learn step counter:  123051
dev_network_count:  411
learn step counter:  123101
dev_network_count:  411
learn step counter:  123151
dev_network_count:  411
learn step counter:  123201
dev_network_count:  411
learn step counter:  123251
dev_network_count:  411

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
412  r_total and score:  213.0000000000002 46.487751516645375
Current Bleu score is:  46.487751516645375
learn step counter:  123301
dev_network_count:  412
learn step counter:  123351
dev_network_count:  412
learn step counter:  123401
dev_network_count:  412
learn step counter:  123451
dev_network_count:  412
learn step counter:  123501
dev_network_count:  412
learn step counter:  123551
dev_network_count:  412

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
413  r_total and score:  213.0000000000002 46.487751516645375
Current Bleu score is:  46.487751516645375
learn step counter:  123601
dev_network_count:  413
learn step counter:  123651
dev_network_count:  413
learn step counter:  123701
dev_network_count:  413
learn step counter:  123751
dev_network_count:  413
learn step counter:  123801
dev_network_count:  413
learn step counter:  123851
dev_network_count:  413

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
414  r_total and score:  213.0000000000002 46.487751516645375
Current Bleu score is:  46.487751516645375
learn step counter:  123901
dev_network_count:  414
learn step counter:  123951
dev_network_count:  414
EPOCH %d 127
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
4118 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4119 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4120 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4121 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4122 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4123 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4124 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  124001
dev_network_count:  414
learn step counter:  124051
dev_network_count:  414
learn step counter:  124101
dev_network_count:  414
learn step counter:  124151
dev_network_count:  414

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
415  r_total and score:  212.0000000000002 48.148303600573065
Current Bleu score is:  48.148303600573065
learn step counter:  124201
dev_network_count:  415
learn step counter:  124251
dev_network_count:  415
learn step counter:  124301
dev_network_count:  415
learn step counter:  124351
dev_network_count:  415
learn step counter:  124401
dev_network_count:  415
learn step counter:  124451
dev_network_count:  415

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
416  r_total and score:  213.0000000000002 46.487751516645375
Current Bleu score is:  46.487751516645375
learn step counter:  124501
dev_network_count:  416
learn step counter:  124551
dev_network_count:  416
learn step counter:  124601
dev_network_count:  416
learn step counter:  124651
dev_network_count:  416
learn step counter:  124701
dev_network_count:  416
learn step counter:  124751
dev_network_count:  416

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
417  r_total and score:  213.0000000000002 46.487751516645375
Current Bleu score is:  46.487751516645375
learn step counter:  124801
dev_network_count:  417
learn step counter:  124851
dev_network_count:  417
learn step counter:  124901
dev_network_count:  417
learn step counter:  124951
dev_network_count:  417
EPOCH %d 128
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1611 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1612 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1613 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1614 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1615 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1616 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1617 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  125001
dev_network_count:  417
learn step counter:  125051
dev_network_count:  417

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
418  r_total and score:  213.0000000000002 46.487751516645375
Current Bleu score is:  46.487751516645375
learn step counter:  125101
dev_network_count:  418
learn step counter:  125151
dev_network_count:  418
learn step counter:  125201
dev_network_count:  418
learn step counter:  125251
dev_network_count:  418
learn step counter:  125301
dev_network_count:  418
learn step counter:  125351
dev_network_count:  418

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
419  r_total and score:  213.0000000000002 46.487751516645375
Current Bleu score is:  46.487751516645375
learn step counter:  125401
dev_network_count:  419
learn step counter:  125451
dev_network_count:  419
learn step counter:  125501
dev_network_count:  419
learn step counter:  125551
dev_network_count:  419
learn step counter:  125601
dev_network_count:  419
learn step counter:  125651
dev_network_count:  419

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
420  r_total and score:  213.0000000000002 46.487751516645375
Current Bleu score is:  46.487751516645375
learn step counter:  125701
dev_network_count:  420
learn step counter:  125751
dev_network_count:  420
learn step counter:  125801
dev_network_count:  420
learn step counter:  125851
dev_network_count:  420
learn step counter:  125901
dev_network_count:  420
learn step counter:  125951
dev_network_count:  420
EPOCH %d 129
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
4104 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4105 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4106 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4107 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4108 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4109 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4110 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
421  r_total and score:  213.0000000000002 46.487751516645375
Current Bleu score is:  46.487751516645375
learn step counter:  126001
dev_network_count:  421
learn step counter:  126051
dev_network_count:  421
learn step counter:  126101
dev_network_count:  421
learn step counter:  126151
dev_network_count:  421
learn step counter:  126201
dev_network_count:  421
learn step counter:  126251
dev_network_count:  421

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
422  r_total and score:  213.0000000000002 46.487751516645375
Current Bleu score is:  46.487751516645375
learn step counter:  126301
dev_network_count:  422
learn step counter:  126351
dev_network_count:  422
learn step counter:  126401
dev_network_count:  422
learn step counter:  126451
dev_network_count:  422
learn step counter:  126501
dev_network_count:  422
learn step counter:  126551
dev_network_count:  422

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
423  r_total and score:  206.20000000000022 46.705776048914885
Current Bleu score is:  46.705776048914885
learn step counter:  126601
dev_network_count:  423
learn step counter:  126651
dev_network_count:  423
learn step counter:  126701
dev_network_count:  423
learn step counter:  126751
dev_network_count:  423
learn step counter:  126801
dev_network_count:  423
learn step counter:  126851
dev_network_count:  423

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
424  r_total and score:  206.20000000000022 46.705776048914885
Current Bleu score is:  46.705776048914885
learn step counter:  126901
dev_network_count:  424
learn step counter:  126951
dev_network_count:  424
EPOCH %d 130
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1597 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1598 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1599 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1600 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1601 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1602 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1603 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  127001
dev_network_count:  424
learn step counter:  127051
dev_network_count:  424
learn step counter:  127101
dev_network_count:  424
learn step counter:  127151
dev_network_count:  424

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
425  r_total and score:  206.20000000000022 46.705776048914885
Current Bleu score is:  46.705776048914885
learn step counter:  127201
dev_network_count:  425
learn step counter:  127251
dev_network_count:  425
learn step counter:  127301
dev_network_count:  425
learn step counter:  127351
dev_network_count:  425
learn step counter:  127401
dev_network_count:  425
learn step counter:  127451
dev_network_count:  425

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
426  r_total and score:  206.20000000000022 46.705776048914885
Current Bleu score is:  46.705776048914885
learn step counter:  127501
dev_network_count:  426
learn step counter:  127551
dev_network_count:  426
learn step counter:  127601
dev_network_count:  426
learn step counter:  127651
dev_network_count:  426
learn step counter:  127701
dev_network_count:  426
learn step counter:  127751
dev_network_count:  426

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
427  r_total and score:  206.20000000000022 46.705776048914885
Current Bleu score is:  46.705776048914885
learn step counter:  127801
dev_network_count:  427
learn step counter:  127851
dev_network_count:  427
learn step counter:  127901
dev_network_count:  427
learn step counter:  127951
dev_network_count:  427
EPOCH %d 131
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
4090 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4091 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4092 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4093 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4094 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4095 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4096 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  128001
dev_network_count:  427
learn step counter:  128051
dev_network_count:  427

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-2.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 4', '2', '', '4', '3 3', '4', '3']
428  r_total and score:  205.40000000000023 45.78081760725756
Current Bleu score is:  45.78081760725756
learn step counter:  128101
dev_network_count:  428
learn step counter:  128151
dev_network_count:  428
learn step counter:  128201
dev_network_count:  428
learn step counter:  128251
dev_network_count:  428
learn step counter:  128301
dev_network_count:  428
learn step counter:  128351
dev_network_count:  428

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
429  r_total and score:  206.20000000000022 46.705776048914885
Current Bleu score is:  46.705776048914885
learn step counter:  128401
dev_network_count:  429
learn step counter:  128451
dev_network_count:  429
learn step counter:  128501
dev_network_count:  429
learn step counter:  128551
dev_network_count:  429
learn step counter:  128601
dev_network_count:  429
learn step counter:  128651
dev_network_count:  429

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
430  r_total and score:  206.20000000000022 46.705776048914885
Current Bleu score is:  46.705776048914885
learn step counter:  128701
dev_network_count:  430
learn step counter:  128751
dev_network_count:  430
learn step counter:  128801
dev_network_count:  430
learn step counter:  128851
dev_network_count:  430
learn step counter:  128901
dev_network_count:  430
learn step counter:  128951
dev_network_count:  430
EPOCH %d 132
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1583 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1584 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1585 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1586 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1587 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1588 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1589 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
431  r_total and score:  206.20000000000022 46.705776048914885
Current Bleu score is:  46.705776048914885
learn step counter:  129001
dev_network_count:  431
learn step counter:  129051
dev_network_count:  431
learn step counter:  129101
dev_network_count:  431
learn step counter:  129151
dev_network_count:  431
learn step counter:  129201
dev_network_count:  431
learn step counter:  129251
dev_network_count:  431

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
432  r_total and score:  206.20000000000022 46.465330702480806
Current Bleu score is:  46.465330702480806
learn step counter:  129301
dev_network_count:  432
learn step counter:  129351
dev_network_count:  432
learn step counter:  129401
dev_network_count:  432
learn step counter:  129451
dev_network_count:  432
learn step counter:  129501
dev_network_count:  432
learn step counter:  129551
dev_network_count:  432

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
433  r_total and score:  206.20000000000022 46.465330702480806
Current Bleu score is:  46.465330702480806
learn step counter:  129601
dev_network_count:  433
learn step counter:  129651
dev_network_count:  433
learn step counter:  129701
dev_network_count:  433
learn step counter:  129751
dev_network_count:  433
learn step counter:  129801
dev_network_count:  433
learn step counter:  129851
dev_network_count:  433

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
434  r_total and score:  206.20000000000022 46.465330702480806
Current Bleu score is:  46.465330702480806
learn step counter:  129901
dev_network_count:  434
learn step counter:  129951
dev_network_count:  434
EPOCH %d 133
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
4076 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4077 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4078 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4079 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4080 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4081 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4082 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  130001
dev_network_count:  434
learn step counter:  130051
dev_network_count:  434
learn step counter:  130101
dev_network_count:  434
learn step counter:  130151
dev_network_count:  434

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
435  r_total and score:  206.20000000000022 46.465330702480806
Current Bleu score is:  46.465330702480806
learn step counter:  130201
dev_network_count:  435
learn step counter:  130251
dev_network_count:  435
learn step counter:  130301
dev_network_count:  435
learn step counter:  130351
dev_network_count:  435
learn step counter:  130401
dev_network_count:  435
learn step counter:  130451
dev_network_count:  435

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
436  r_total and score:  206.20000000000022 46.465330702480806
Current Bleu score is:  46.465330702480806
learn step counter:  130501
dev_network_count:  436
learn step counter:  130551
dev_network_count:  436
learn step counter:  130601
dev_network_count:  436
learn step counter:  130651
dev_network_count:  436
learn step counter:  130701
dev_network_count:  436
learn step counter:  130751
dev_network_count:  436

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
437  r_total and score:  205.20000000000022 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  130801
dev_network_count:  437
learn step counter:  130851
dev_network_count:  437
learn step counter:  130901
dev_network_count:  437
learn step counter:  130951
dev_network_count:  437
EPOCH %d 134
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1569 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1570 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1571 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1572 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1573 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1574 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1575 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  131001
dev_network_count:  437
learn step counter:  131051
dev_network_count:  437

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
438  r_total and score:  206.20000000000022 46.465330702480806
Current Bleu score is:  46.465330702480806
learn step counter:  131101
dev_network_count:  438
learn step counter:  131151
dev_network_count:  438
learn step counter:  131201
dev_network_count:  438
learn step counter:  131251
dev_network_count:  438
learn step counter:  131301
dev_network_count:  438
learn step counter:  131351
dev_network_count:  438

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
439  r_total and score:  205.20000000000022 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  131401
dev_network_count:  439
learn step counter:  131451
dev_network_count:  439
learn step counter:  131501
dev_network_count:  439
learn step counter:  131551
dev_network_count:  439
learn step counter:  131601
dev_network_count:  439
learn step counter:  131651
dev_network_count:  439

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
440  r_total and score:  206.20000000000022 46.465330702480806
Current Bleu score is:  46.465330702480806
learn step counter:  131701
dev_network_count:  440
learn step counter:  131751
dev_network_count:  440
learn step counter:  131801
dev_network_count:  440
learn step counter:  131851
dev_network_count:  440
learn step counter:  131901
dev_network_count:  440
learn step counter:  131951
dev_network_count:  440
EPOCH %d 135
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
4062 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4063 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4064 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4065 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4066 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4067 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4068 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
441  r_total and score:  205.20000000000022 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  132001
dev_network_count:  441
learn step counter:  132051
dev_network_count:  441
learn step counter:  132101
dev_network_count:  441
learn step counter:  132151
dev_network_count:  441
learn step counter:  132201
dev_network_count:  441
learn step counter:  132251
dev_network_count:  441

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
442  r_total and score:  206.20000000000022 46.465330702480806
Current Bleu score is:  46.465330702480806
learn step counter:  132301
dev_network_count:  442
learn step counter:  132351
dev_network_count:  442
learn step counter:  132401
dev_network_count:  442
learn step counter:  132451
dev_network_count:  442
learn step counter:  132501
dev_network_count:  442
learn step counter:  132551
dev_network_count:  442

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
443  r_total and score:  206.20000000000022 46.465330702480806
Current Bleu score is:  46.465330702480806
learn step counter:  132601
dev_network_count:  443
learn step counter:  132651
dev_network_count:  443
learn step counter:  132701
dev_network_count:  443
learn step counter:  132751
dev_network_count:  443
learn step counter:  132801
dev_network_count:  443
learn step counter:  132851
dev_network_count:  443

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
444  r_total and score:  206.20000000000022 46.465330702480806
Current Bleu score is:  46.465330702480806
learn step counter:  132901
dev_network_count:  444
learn step counter:  132951
dev_network_count:  444
EPOCH %d 136
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1555 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1556 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1557 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1558 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1559 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1560 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1561 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  133001
dev_network_count:  444
learn step counter:  133051
dev_network_count:  444
learn step counter:  133101
dev_network_count:  444
learn step counter:  133151
dev_network_count:  444

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
445  r_total and score:  206.20000000000022 46.465330702480806
Current Bleu score is:  46.465330702480806
learn step counter:  133201
dev_network_count:  445
learn step counter:  133251
dev_network_count:  445
learn step counter:  133301
dev_network_count:  445
learn step counter:  133351
dev_network_count:  445
learn step counter:  133401
dev_network_count:  445
learn step counter:  133451
dev_network_count:  445

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
446  r_total and score:  206.20000000000022 46.465330702480806
Current Bleu score is:  46.465330702480806
learn step counter:  133501
dev_network_count:  446
learn step counter:  133551
dev_network_count:  446
learn step counter:  133601
dev_network_count:  446
learn step counter:  133651
dev_network_count:  446
learn step counter:  133701
dev_network_count:  446
learn step counter:  133751
dev_network_count:  446

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
447  r_total and score:  205.20000000000022 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  133801
dev_network_count:  447
learn step counter:  133851
dev_network_count:  447
learn step counter:  133901
dev_network_count:  447
learn step counter:  133951
dev_network_count:  447
EPOCH %d 137
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
4048 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4049 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4050 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4051 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4052 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4053 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4054 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  134001
dev_network_count:  447
learn step counter:  134051
dev_network_count:  447

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
448  r_total and score:  206.20000000000022 46.465330702480806
Current Bleu score is:  46.465330702480806
learn step counter:  134101
dev_network_count:  448
learn step counter:  134151
dev_network_count:  448
learn step counter:  134201
dev_network_count:  448
learn step counter:  134251
dev_network_count:  448
learn step counter:  134301
dev_network_count:  448
learn step counter:  134351
dev_network_count:  448

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
449  r_total and score:  205.20000000000022 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  134401
dev_network_count:  449
learn step counter:  134451
dev_network_count:  449
learn step counter:  134501
dev_network_count:  449
learn step counter:  134551
dev_network_count:  449
learn step counter:  134601
dev_network_count:  449
learn step counter:  134651
dev_network_count:  449

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
450  r_total and score:  206.20000000000022 46.465330702480806
Current Bleu score is:  46.465330702480806
learn step counter:  134701
dev_network_count:  450
learn step counter:  134751
dev_network_count:  450
learn step counter:  134801
dev_network_count:  450
learn step counter:  134851
dev_network_count:  450
learn step counter:  134901
dev_network_count:  450
learn step counter:  134951
dev_network_count:  450
EPOCH %d 138
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1541 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1542 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1543 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1544 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1545 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1546 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1547 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
451  r_total and score:  205.20000000000022 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  135001
dev_network_count:  451
learn step counter:  135051
dev_network_count:  451
learn step counter:  135101
dev_network_count:  451
learn step counter:  135151
dev_network_count:  451
learn step counter:  135201
dev_network_count:  451
learn step counter:  135251
dev_network_count:  451

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
452  r_total and score:  206.20000000000022 46.465330702480806
Current Bleu score is:  46.465330702480806
learn step counter:  135301
dev_network_count:  452
learn step counter:  135351
dev_network_count:  452
learn step counter:  135401
dev_network_count:  452
learn step counter:  135451
dev_network_count:  452
learn step counter:  135501
dev_network_count:  452
learn step counter:  135551
dev_network_count:  452

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
453  r_total and score:  206.20000000000022 46.465330702480806
Current Bleu score is:  46.465330702480806
learn step counter:  135601
dev_network_count:  453
learn step counter:  135651
dev_network_count:  453
learn step counter:  135701
dev_network_count:  453
learn step counter:  135751
dev_network_count:  453
learn step counter:  135801
dev_network_count:  453
learn step counter:  135851
dev_network_count:  453

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
454  r_total and score:  205.20000000000022 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  135901
dev_network_count:  454
learn step counter:  135951
dev_network_count:  454
EPOCH %d 139
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
4034 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4035 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4036 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4037 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4038 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4039 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4040 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  136001
dev_network_count:  454
learn step counter:  136051
dev_network_count:  454
learn step counter:  136101
dev_network_count:  454
learn step counter:  136151
dev_network_count:  454

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
455  r_total and score:  206.20000000000022 46.465330702480806
Current Bleu score is:  46.465330702480806
learn step counter:  136201
dev_network_count:  455
learn step counter:  136251
dev_network_count:  455
learn step counter:  136301
dev_network_count:  455
learn step counter:  136351
dev_network_count:  455
learn step counter:  136401
dev_network_count:  455
learn step counter:  136451
dev_network_count:  455

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
456  r_total and score:  205.20000000000022 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  136501
dev_network_count:  456
learn step counter:  136551
dev_network_count:  456
learn step counter:  136601
dev_network_count:  456
learn step counter:  136651
dev_network_count:  456
learn step counter:  136701
dev_network_count:  456
learn step counter:  136751
dev_network_count:  456

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
457  r_total and score:  205.20000000000022 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  136801
dev_network_count:  457
learn step counter:  136851
dev_network_count:  457
learn step counter:  136901
dev_network_count:  457
learn step counter:  136951
dev_network_count:  457
EPOCH %d 140
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1527 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1528 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1529 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1530 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1531 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1532 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1533 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  137001
dev_network_count:  457
learn step counter:  137051
dev_network_count:  457

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
458  r_total and score:  205.20000000000022 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  137101
dev_network_count:  458
learn step counter:  137151
dev_network_count:  458
learn step counter:  137201
dev_network_count:  458
learn step counter:  137251
dev_network_count:  458
learn step counter:  137301
dev_network_count:  458
learn step counter:  137351
dev_network_count:  458

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
459  r_total and score:  205.20000000000022 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  137401
dev_network_count:  459
learn step counter:  137451
dev_network_count:  459
learn step counter:  137501
dev_network_count:  459
learn step counter:  137551
dev_network_count:  459
learn step counter:  137601
dev_network_count:  459
learn step counter:  137651
dev_network_count:  459

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
460  r_total and score:  206.20000000000022 46.465330702480806
Current Bleu score is:  46.465330702480806
learn step counter:  137701
dev_network_count:  460
learn step counter:  137751
dev_network_count:  460
learn step counter:  137801
dev_network_count:  460
learn step counter:  137851
dev_network_count:  460
learn step counter:  137901
dev_network_count:  460
learn step counter:  137951
dev_network_count:  460
EPOCH %d 141
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
4020 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4021 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4022 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4023 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4024 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4025 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4026 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
461  r_total and score:  205.20000000000022 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  138001
dev_network_count:  461
learn step counter:  138051
dev_network_count:  461
learn step counter:  138101
dev_network_count:  461
learn step counter:  138151
dev_network_count:  461
learn step counter:  138201
dev_network_count:  461
learn step counter:  138251
dev_network_count:  461

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
462  r_total and score:  205.20000000000022 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  138301
dev_network_count:  462
learn step counter:  138351
dev_network_count:  462
learn step counter:  138401
dev_network_count:  462
learn step counter:  138451
dev_network_count:  462
learn step counter:  138501
dev_network_count:  462
learn step counter:  138551
dev_network_count:  462

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
463  r_total and score:  205.20000000000022 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  138601
dev_network_count:  463
learn step counter:  138651
dev_network_count:  463
learn step counter:  138701
dev_network_count:  463
learn step counter:  138751
dev_network_count:  463
learn step counter:  138801
dev_network_count:  463
learn step counter:  138851
dev_network_count:  463

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
464  r_total and score:  206.20000000000022 46.465330702480806
Current Bleu score is:  46.465330702480806
learn step counter:  138901
dev_network_count:  464
learn step counter:  138951
dev_network_count:  464
EPOCH %d 142
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1513 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1514 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1515 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1516 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1517 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1518 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1519 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  139001
dev_network_count:  464
learn step counter:  139051
dev_network_count:  464
learn step counter:  139101
dev_network_count:  464
learn step counter:  139151
dev_network_count:  464

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
465  r_total and score:  206.20000000000022 46.465330702480806
Current Bleu score is:  46.465330702480806
learn step counter:  139201
dev_network_count:  465
learn step counter:  139251
dev_network_count:  465
learn step counter:  139301
dev_network_count:  465
learn step counter:  139351
dev_network_count:  465
learn step counter:  139401
dev_network_count:  465
learn step counter:  139451
dev_network_count:  465

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
466  r_total and score:  205.20000000000022 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  139501
dev_network_count:  466
learn step counter:  139551
dev_network_count:  466
learn step counter:  139601
dev_network_count:  466
learn step counter:  139651
dev_network_count:  466
learn step counter:  139701
dev_network_count:  466
learn step counter:  139751
dev_network_count:  466

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
467  r_total and score:  205.20000000000022 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  139801
dev_network_count:  467
learn step counter:  139851
dev_network_count:  467
learn step counter:  139901
dev_network_count:  467
learn step counter:  139951
dev_network_count:  467
EPOCH %d 143
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
4006 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4007 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4008 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4009 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4010 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4011 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4012 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  140001
dev_network_count:  467
learn step counter:  140051
dev_network_count:  467

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
468  r_total and score:  205.20000000000022 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  140101
dev_network_count:  468
learn step counter:  140151
dev_network_count:  468
learn step counter:  140201
dev_network_count:  468
learn step counter:  140251
dev_network_count:  468
learn step counter:  140301
dev_network_count:  468
learn step counter:  140351
dev_network_count:  468

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
469  r_total and score:  205.20000000000022 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  140401
dev_network_count:  469
learn step counter:  140451
dev_network_count:  469
learn step counter:  140501
dev_network_count:  469
learn step counter:  140551
dev_network_count:  469
learn step counter:  140601
dev_network_count:  469
learn step counter:  140651
dev_network_count:  469

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
470  r_total and score:  205.20000000000022 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  140701
dev_network_count:  470
learn step counter:  140751
dev_network_count:  470
learn step counter:  140801
dev_network_count:  470
learn step counter:  140851
dev_network_count:  470
learn step counter:  140901
dev_network_count:  470
learn step counter:  140951
dev_network_count:  470
EPOCH %d 144
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1499 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1500 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1501 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1502 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1503 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1504 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1505 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
471  r_total and score:  205.20000000000022 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  141001
dev_network_count:  471
learn step counter:  141051
dev_network_count:  471
learn step counter:  141101
dev_network_count:  471
learn step counter:  141151
dev_network_count:  471
learn step counter:  141201
dev_network_count:  471
learn step counter:  141251
dev_network_count:  471

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
472  r_total and score:  205.20000000000022 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  141301
dev_network_count:  472
learn step counter:  141351
dev_network_count:  472
learn step counter:  141401
dev_network_count:  472
learn step counter:  141451
dev_network_count:  472
learn step counter:  141501
dev_network_count:  472
learn step counter:  141551
dev_network_count:  472

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
473  r_total and score:  205.20000000000022 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  141601
dev_network_count:  473
learn step counter:  141651
dev_network_count:  473
learn step counter:  141701
dev_network_count:  473
learn step counter:  141751
dev_network_count:  473
learn step counter:  141801
dev_network_count:  473
learn step counter:  141851
dev_network_count:  473

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
474  r_total and score:  205.20000000000022 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  141901
dev_network_count:  474
learn step counter:  141951
dev_network_count:  474
EPOCH %d 145
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
3992 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
3993 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
3994 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
3995 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
3996 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
3997 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
3998 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  142001
dev_network_count:  474
learn step counter:  142051
dev_network_count:  474
learn step counter:  142101
dev_network_count:  474
learn step counter:  142151
dev_network_count:  474

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
475  r_total and score:  205.20000000000022 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  142201
dev_network_count:  475
learn step counter:  142251
dev_network_count:  475
learn step counter:  142301
dev_network_count:  475
learn step counter:  142351
dev_network_count:  475
learn step counter:  142401
dev_network_count:  475
learn step counter:  142451
dev_network_count:  475

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
476  r_total and score:  205.20000000000022 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  142501
dev_network_count:  476
learn step counter:  142551
dev_network_count:  476
learn step counter:  142601
dev_network_count:  476
learn step counter:  142651
dev_network_count:  476
learn step counter:  142701
dev_network_count:  476
learn step counter:  142751
dev_network_count:  476

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
477  r_total and score:  205.20000000000022 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  142801
dev_network_count:  477
learn step counter:  142851
dev_network_count:  477
learn step counter:  142901
dev_network_count:  477
learn step counter:  142951
dev_network_count:  477
EPOCH %d 146
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1485 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1486 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1487 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1488 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1489 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1490 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1491 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  143001
dev_network_count:  477
learn step counter:  143051
dev_network_count:  477

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
478  r_total and score:  205.20000000000022 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  143101
dev_network_count:  478
learn step counter:  143151
dev_network_count:  478
learn step counter:  143201
dev_network_count:  478
learn step counter:  143251
dev_network_count:  478
learn step counter:  143301
dev_network_count:  478
learn step counter:  143351
dev_network_count:  478

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
479  r_total and score:  205.20000000000022 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  143401
dev_network_count:  479
learn step counter:  143451
dev_network_count:  479
learn step counter:  143501
dev_network_count:  479
learn step counter:  143551
dev_network_count:  479
learn step counter:  143601
dev_network_count:  479
learn step counter:  143651
dev_network_count:  479

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
480  r_total and score:  205.20000000000022 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  143701
dev_network_count:  480
learn step counter:  143751
dev_network_count:  480
learn step counter:  143801
dev_network_count:  480
learn step counter:  143851
dev_network_count:  480
learn step counter:  143901
dev_network_count:  480
learn step counter:  143951
dev_network_count:  480
EPOCH %d 147
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
3978 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
3979 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
3980 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
3981 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
3982 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
3983 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
3984 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
481  r_total and score:  205.20000000000022 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  144001
dev_network_count:  481
learn step counter:  144051
dev_network_count:  481
learn step counter:  144101
dev_network_count:  481
learn step counter:  144151
dev_network_count:  481
learn step counter:  144201
dev_network_count:  481
learn step counter:  144251
dev_network_count:  481

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
482  r_total and score:  205.20000000000022 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  144301
dev_network_count:  482
learn step counter:  144351
dev_network_count:  482
learn step counter:  144401
dev_network_count:  482
learn step counter:  144451
dev_network_count:  482
learn step counter:  144501
dev_network_count:  482
learn step counter:  144551
dev_network_count:  482

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
483  r_total and score:  206.20000000000022 46.465330702480806
Current Bleu score is:  46.465330702480806
learn step counter:  144601
dev_network_count:  483
learn step counter:  144651
dev_network_count:  483
learn step counter:  144701
dev_network_count:  483
learn step counter:  144751
dev_network_count:  483
learn step counter:  144801
dev_network_count:  483
learn step counter:  144851
dev_network_count:  483

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
484  r_total and score:  205.20000000000022 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  144901
dev_network_count:  484
learn step counter:  144951
dev_network_count:  484
EPOCH %d 148
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1471 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1472 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1473 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1474 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1475 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1476 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1477 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  145001
dev_network_count:  484
learn step counter:  145051
dev_network_count:  484
learn step counter:  145101
dev_network_count:  484
learn step counter:  145151
dev_network_count:  484

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
485  r_total and score:  205.20000000000022 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  145201
dev_network_count:  485
learn step counter:  145251
dev_network_count:  485
learn step counter:  145301
dev_network_count:  485
learn step counter:  145351
dev_network_count:  485
learn step counter:  145401
dev_network_count:  485
learn step counter:  145451
dev_network_count:  485

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
486  r_total and score:  205.20000000000022 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  145501
dev_network_count:  486
learn step counter:  145551
dev_network_count:  486
learn step counter:  145601
dev_network_count:  486
learn step counter:  145651
dev_network_count:  486
learn step counter:  145701
dev_network_count:  486
learn step counter:  145751
dev_network_count:  486

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
487  r_total and score:  205.20000000000022 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  145801
dev_network_count:  487
learn step counter:  145851
dev_network_count:  487
learn step counter:  145901
dev_network_count:  487
learn step counter:  145951
dev_network_count:  487
EPOCH %d 149
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
3964 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
3965 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
3966 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
3967 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
3968 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
3969 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
3970 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  146001
dev_network_count:  487
learn step counter:  146051
dev_network_count:  487

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
488  r_total and score:  205.20000000000022 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  146101
dev_network_count:  488
learn step counter:  146151
dev_network_count:  488
learn step counter:  146201
dev_network_count:  488
learn step counter:  146251
dev_network_count:  488
learn step counter:  146301
dev_network_count:  488
learn step counter:  146351
dev_network_count:  488

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
489  r_total and score:  205.20000000000022 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  146401
dev_network_count:  489
learn step counter:  146451
dev_network_count:  489
learn step counter:  146501
dev_network_count:  489
learn step counter:  146551
dev_network_count:  489
learn step counter:  146601
dev_network_count:  489
learn step counter:  146651
dev_network_count:  489

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
490  r_total and score:  205.20000000000022 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  146701
dev_network_count:  490
learn step counter:  146751
dev_network_count:  490
learn step counter:  146801
dev_network_count:  490
learn step counter:  146851
dev_network_count:  490
learn step counter:  146901
dev_network_count:  490
learn step counter:  146951
dev_network_count:  490
EPOCH %d 150
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1457 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1458 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1459 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1460 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1461 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1462 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1463 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
491  r_total and score:  205.20000000000022 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  147001
dev_network_count:  491
learn step counter:  147051
dev_network_count:  491
learn step counter:  147101
dev_network_count:  491
learn step counter:  147151
dev_network_count:  491
learn step counter:  147201
dev_network_count:  491
learn step counter:  147251
dev_network_count:  491

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
492  r_total and score:  205.20000000000022 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  147301
dev_network_count:  492
learn step counter:  147351
dev_network_count:  492
learn step counter:  147401
dev_network_count:  492
learn step counter:  147451
dev_network_count:  492
learn step counter:  147501
dev_network_count:  492
learn step counter:  147551
dev_network_count:  492

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
493  r_total and score:  205.20000000000022 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  147601
dev_network_count:  493
learn step counter:  147651
dev_network_count:  493
learn step counter:  147701
dev_network_count:  493
learn step counter:  147751
dev_network_count:  493
learn step counter:  147801
dev_network_count:  493
learn step counter:  147851
dev_network_count:  493

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
494  r_total and score:  205.20000000000022 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  147901
dev_network_count:  494
learn step counter:  147951
dev_network_count:  494
EPOCH %d 151
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
3950 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
3951 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
3952 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
3953 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
3954 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
3955 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
3956 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  148001
dev_network_count:  494
learn step counter:  148051
dev_network_count:  494
learn step counter:  148101
dev_network_count:  494
learn step counter:  148151
dev_network_count:  494

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
495  r_total and score:  205.20000000000022 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  148201
dev_network_count:  495
learn step counter:  148251
dev_network_count:  495
learn step counter:  148301
dev_network_count:  495
learn step counter:  148351
dev_network_count:  495
learn step counter:  148401
dev_network_count:  495
learn step counter:  148451
dev_network_count:  495

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
496  r_total and score:  205.20000000000022 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  148501
dev_network_count:  496
learn step counter:  148551
dev_network_count:  496
learn step counter:  148601
dev_network_count:  496
learn step counter:  148651
dev_network_count:  496
learn step counter:  148701
dev_network_count:  496
learn step counter:  148751
dev_network_count:  496

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
497  r_total and score:  205.20000000000022 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  148801
dev_network_count:  497
learn step counter:  148851
dev_network_count:  497
learn step counter:  148901
dev_network_count:  497
learn step counter:  148951
dev_network_count:  497
EPOCH %d 152
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1443 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1444 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1445 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1446 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1447 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1448 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1449 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  149001
dev_network_count:  497
learn step counter:  149051
dev_network_count:  497

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
498  r_total and score:  205.20000000000022 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  149101
dev_network_count:  498
learn step counter:  149151
dev_network_count:  498
learn step counter:  149201
dev_network_count:  498
learn step counter:  149251
dev_network_count:  498
learn step counter:  149301
dev_network_count:  498
learn step counter:  149351
dev_network_count:  498

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
499  r_total and score:  205.20000000000022 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  149401
dev_network_count:  499
learn step counter:  149451
dev_network_count:  499
learn step counter:  149501
dev_network_count:  499
learn step counter:  149551
dev_network_count:  499
learn step counter:  149601
dev_network_count:  499
learn step counter:  149651
dev_network_count:  499

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
500  r_total and score:  205.20000000000022 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  149701
dev_network_count:  500
learn step counter:  149751
dev_network_count:  500
learn step counter:  149801
dev_network_count:  500
learn step counter:  149851
dev_network_count:  500
learn step counter:  149901
dev_network_count:  500
learn step counter:  149951
dev_network_count:  500
EPOCH %d 153
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
3936 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
3937 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
3938 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
3939 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
3940 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
3941 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
3942 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
501  r_total and score:  206.20000000000022 46.465330702480806
Current Bleu score is:  46.465330702480806
learn step counter:  150001
dev_network_count:  501
learn step counter:  150051
dev_network_count:  501
learn step counter:  150101
dev_network_count:  501
learn step counter:  150151
dev_network_count:  501
learn step counter:  150201
dev_network_count:  501
learn step counter:  150251
dev_network_count:  501

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
502  r_total and score:  206.20000000000022 46.465330702480806
Current Bleu score is:  46.465330702480806
learn step counter:  150301
dev_network_count:  502
learn step counter:  150351
dev_network_count:  502
learn step counter:  150401
dev_network_count:  502
learn step counter:  150451
dev_network_count:  502
learn step counter:  150501
dev_network_count:  502
learn step counter:  150551
dev_network_count:  502

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
503  r_total and score:  205.20000000000022 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  150601
dev_network_count:  503
learn step counter:  150651
dev_network_count:  503
learn step counter:  150701
dev_network_count:  503
learn step counter:  150751
dev_network_count:  503
learn step counter:  150801
dev_network_count:  503
learn step counter:  150851
dev_network_count:  503

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
504  r_total and score:  205.20000000000022 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  150901
dev_network_count:  504
learn step counter:  150951
dev_network_count:  504
EPOCH %d 154
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1429 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1430 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1431 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1432 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1433 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1434 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1435 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  151001
dev_network_count:  504
learn step counter:  151051
dev_network_count:  504
learn step counter:  151101
dev_network_count:  504
learn step counter:  151151
dev_network_count:  504

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
505  r_total and score:  205.20000000000022 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  151201
dev_network_count:  505
learn step counter:  151251
dev_network_count:  505
learn step counter:  151301
dev_network_count:  505
learn step counter:  151351
dev_network_count:  505
learn step counter:  151401
dev_network_count:  505
learn step counter:  151451
dev_network_count:  505

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
506  r_total and score:  205.20000000000022 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  151501
dev_network_count:  506
learn step counter:  151551
dev_network_count:  506
learn step counter:  151601
dev_network_count:  506
learn step counter:  151651
dev_network_count:  506
learn step counter:  151701
dev_network_count:  506
learn step counter:  151751
dev_network_count:  506

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
507  r_total and score:  205.20000000000022 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  151801
dev_network_count:  507
learn step counter:  151851
dev_network_count:  507
learn step counter:  151901
dev_network_count:  507
learn step counter:  151951
dev_network_count:  507
EPOCH %d 155
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
3922 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
3923 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
3924 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
3925 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
3926 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
3927 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
3928 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  152001
dev_network_count:  507
learn step counter:  152051
dev_network_count:  507

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
508  r_total and score:  205.20000000000022 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  152101
dev_network_count:  508
learn step counter:  152151
dev_network_count:  508
learn step counter:  152201
dev_network_count:  508
learn step counter:  152251
dev_network_count:  508
learn step counter:  152301
dev_network_count:  508
learn step counter:  152351
dev_network_count:  508

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
509  r_total and score:  205.20000000000022 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  152401
dev_network_count:  509
learn step counter:  152451
dev_network_count:  509
learn step counter:  152501
dev_network_count:  509
learn step counter:  152551
dev_network_count:  509
learn step counter:  152601
dev_network_count:  509
learn step counter:  152651
dev_network_count:  509

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
510  r_total and score:  206.20000000000022 46.465330702480806
Current Bleu score is:  46.465330702480806
learn step counter:  152701
dev_network_count:  510
learn step counter:  152751
dev_network_count:  510
learn step counter:  152801
dev_network_count:  510
learn step counter:  152851
dev_network_count:  510
learn step counter:  152901
dev_network_count:  510
learn step counter:  152951
dev_network_count:  510
EPOCH %d 156
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1415 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1416 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1417 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1418 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1419 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1420 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1421 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
511  r_total and score:  205.20000000000022 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  153001
dev_network_count:  511
learn step counter:  153051
dev_network_count:  511
learn step counter:  153101
dev_network_count:  511
learn step counter:  153151
dev_network_count:  511
learn step counter:  153201
dev_network_count:  511
learn step counter:  153251
dev_network_count:  511

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
512  r_total and score:  205.20000000000022 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  153301
dev_network_count:  512
learn step counter:  153351
dev_network_count:  512
learn step counter:  153401
dev_network_count:  512
learn step counter:  153451
dev_network_count:  512
learn step counter:  153501
dev_network_count:  512
learn step counter:  153551
dev_network_count:  512

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
513  r_total and score:  205.20000000000022 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  153601
dev_network_count:  513
learn step counter:  153651
dev_network_count:  513
learn step counter:  153701
dev_network_count:  513
learn step counter:  153751
dev_network_count:  513
learn step counter:  153801
dev_network_count:  513
learn step counter:  153851
dev_network_count:  513

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
514  r_total and score:  205.20000000000022 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  153901
dev_network_count:  514
learn step counter:  153951
dev_network_count:  514
EPOCH %d 157
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
3908 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
3909 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
3910 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
3911 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
3912 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
3913 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
3914 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  154001
dev_network_count:  514
learn step counter:  154051
dev_network_count:  514
learn step counter:  154101
dev_network_count:  514
learn step counter:  154151
dev_network_count:  514

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
515  r_total and score:  206.20000000000022 46.465330702480806
Current Bleu score is:  46.465330702480806
learn step counter:  154201
dev_network_count:  515
learn step counter:  154251
dev_network_count:  515
learn step counter:  154301
dev_network_count:  515
learn step counter:  154351
dev_network_count:  515
learn step counter:  154401
dev_network_count:  515
learn step counter:  154451
dev_network_count:  515

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
516  r_total and score:  205.20000000000022 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  154501
dev_network_count:  516
learn step counter:  154551
dev_network_count:  516
learn step counter:  154601
dev_network_count:  516
learn step counter:  154651
dev_network_count:  516
learn step counter:  154701
dev_network_count:  516
learn step counter:  154751
dev_network_count:  516

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
517  r_total and score:  205.20000000000022 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  154801
dev_network_count:  517
learn step counter:  154851
dev_network_count:  517
learn step counter:  154901
dev_network_count:  517
learn step counter:  154951
dev_network_count:  517
EPOCH %d 158
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1401 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1402 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1403 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1404 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1405 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1406 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1407 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  155001
dev_network_count:  517
learn step counter:  155051
dev_network_count:  517

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
518  r_total and score:  205.20000000000022 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  155101
dev_network_count:  518
learn step counter:  155151
dev_network_count:  518
learn step counter:  155201
dev_network_count:  518
learn step counter:  155251
dev_network_count:  518
learn step counter:  155301
dev_network_count:  518
learn step counter:  155351
dev_network_count:  518

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
519  r_total and score:  205.20000000000022 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  155401
dev_network_count:  519
learn step counter:  155451
dev_network_count:  519
learn step counter:  155501
dev_network_count:  519
learn step counter:  155551
dev_network_count:  519
learn step counter:  155601
dev_network_count:  519
learn step counter:  155651
dev_network_count:  519

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
520  r_total and score:  205.20000000000022 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  155701
dev_network_count:  520
learn step counter:  155751
dev_network_count:  520
learn step counter:  155801
dev_network_count:  520
learn step counter:  155851
dev_network_count:  520
learn step counter:  155901
dev_network_count:  520
learn step counter:  155951
dev_network_count:  520
EPOCH %d 159
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
3894 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
3895 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
3896 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
3897 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
3898 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
3899 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
3900 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
521  r_total and score:  205.20000000000022 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  156001
dev_network_count:  521
learn step counter:  156051
dev_network_count:  521
learn step counter:  156101
dev_network_count:  521
learn step counter:  156151
dev_network_count:  521
learn step counter:  156201
dev_network_count:  521
learn step counter:  156251
dev_network_count:  521

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
522  r_total and score:  205.20000000000022 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  156301
dev_network_count:  522
learn step counter:  156351
dev_network_count:  522
learn step counter:  156401
dev_network_count:  522
learn step counter:  156451
dev_network_count:  522
learn step counter:  156501
dev_network_count:  522
learn step counter:  156551
dev_network_count:  522

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
523  r_total and score:  205.6000000000002 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  156601
dev_network_count:  523
learn step counter:  156651
dev_network_count:  523
learn step counter:  156701
dev_network_count:  523
learn step counter:  156751
dev_network_count:  523
learn step counter:  156801
dev_network_count:  523
learn step counter:  156851
dev_network_count:  523

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
524  r_total and score:  205.6000000000002 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  156901
dev_network_count:  524
learn step counter:  156951
dev_network_count:  524
EPOCH %d 160
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1387 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1388 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1389 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1390 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1391 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1392 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1393 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  157001
dev_network_count:  524
learn step counter:  157051
dev_network_count:  524
learn step counter:  157101
dev_network_count:  524
learn step counter:  157151
dev_network_count:  524

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
525  r_total and score:  205.6000000000002 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  157201
dev_network_count:  525
learn step counter:  157251
dev_network_count:  525
learn step counter:  157301
dev_network_count:  525
learn step counter:  157351
dev_network_count:  525
learn step counter:  157401
dev_network_count:  525
learn step counter:  157451
dev_network_count:  525

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
526  r_total and score:  205.6000000000002 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  157501
dev_network_count:  526
learn step counter:  157551
dev_network_count:  526
learn step counter:  157601
dev_network_count:  526
learn step counter:  157651
dev_network_count:  526
learn step counter:  157701
dev_network_count:  526
learn step counter:  157751
dev_network_count:  526

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
527  r_total and score:  205.6000000000002 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  157801
dev_network_count:  527
learn step counter:  157851
dev_network_count:  527
learn step counter:  157901
dev_network_count:  527
learn step counter:  157951
dev_network_count:  527
EPOCH %d 161
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
3880 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
3881 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
3882 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
3883 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
3884 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
3885 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
3886 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  158001
dev_network_count:  527
learn step counter:  158051
dev_network_count:  527

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
528  r_total and score:  205.6000000000002 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  158101
dev_network_count:  528
learn step counter:  158151
dev_network_count:  528
learn step counter:  158201
dev_network_count:  528
learn step counter:  158251
dev_network_count:  528
learn step counter:  158301
dev_network_count:  528
learn step counter:  158351
dev_network_count:  528

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
529  r_total and score:  205.6000000000002 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  158401
dev_network_count:  529
learn step counter:  158451
dev_network_count:  529
learn step counter:  158501
dev_network_count:  529
learn step counter:  158551
dev_network_count:  529
learn step counter:  158601
dev_network_count:  529
learn step counter:  158651
dev_network_count:  529

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
530  r_total and score:  205.6000000000002 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  158701
dev_network_count:  530
learn step counter:  158751
dev_network_count:  530
learn step counter:  158801
dev_network_count:  530
learn step counter:  158851
dev_network_count:  530
learn step counter:  158901
dev_network_count:  530
learn step counter:  158951
dev_network_count:  530
EPOCH %d 162
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1373 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1374 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1375 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1376 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1377 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1378 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1379 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
531  r_total and score:  205.6000000000002 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  159001
dev_network_count:  531
learn step counter:  159051
dev_network_count:  531
learn step counter:  159101
dev_network_count:  531
learn step counter:  159151
dev_network_count:  531
learn step counter:  159201
dev_network_count:  531
learn step counter:  159251
dev_network_count:  531

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
532  r_total and score:  205.6000000000002 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  159301
dev_network_count:  532
learn step counter:  159351
dev_network_count:  532
learn step counter:  159401
dev_network_count:  532
learn step counter:  159451
dev_network_count:  532
learn step counter:  159501
dev_network_count:  532
learn step counter:  159551
dev_network_count:  532

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
533  r_total and score:  205.6000000000002 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  159601
dev_network_count:  533
learn step counter:  159651
dev_network_count:  533
learn step counter:  159701
dev_network_count:  533
learn step counter:  159751
dev_network_count:  533
learn step counter:  159801
dev_network_count:  533
learn step counter:  159851
dev_network_count:  533

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
534  r_total and score:  206.20000000000022 46.465330702480806
Current Bleu score is:  46.465330702480806
learn step counter:  159901
dev_network_count:  534
learn step counter:  159951
dev_network_count:  534
EPOCH %d 163
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
3866 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
3867 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
3868 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
3869 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
3870 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
3871 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
3872 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  160001
dev_network_count:  534
learn step counter:  160051
dev_network_count:  534
learn step counter:  160101
dev_network_count:  534
learn step counter:  160151
dev_network_count:  534

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
535  r_total and score:  205.6000000000002 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  160201
dev_network_count:  535
learn step counter:  160251
dev_network_count:  535
learn step counter:  160301
dev_network_count:  535
learn step counter:  160351
dev_network_count:  535
learn step counter:  160401
dev_network_count:  535
learn step counter:  160451
dev_network_count:  535

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
536  r_total and score:  206.6000000000002 46.465330702480806
Current Bleu score is:  46.465330702480806
learn step counter:  160501
dev_network_count:  536
learn step counter:  160551
dev_network_count:  536
learn step counter:  160601
dev_network_count:  536
learn step counter:  160651
dev_network_count:  536
learn step counter:  160701
dev_network_count:  536
learn step counter:  160751
dev_network_count:  536

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
537  r_total and score:  205.6000000000002 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  160801
dev_network_count:  537
learn step counter:  160851
dev_network_count:  537
learn step counter:  160901
dev_network_count:  537
learn step counter:  160951
dev_network_count:  537
EPOCH %d 164
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1359 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1360 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1361 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1362 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1363 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1364 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1365 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  161001
dev_network_count:  537
learn step counter:  161051
dev_network_count:  537

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
538  r_total and score:  205.6000000000002 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  161101
dev_network_count:  538
learn step counter:  161151
dev_network_count:  538
learn step counter:  161201
dev_network_count:  538
learn step counter:  161251
dev_network_count:  538
learn step counter:  161301
dev_network_count:  538
learn step counter:  161351
dev_network_count:  538

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
539  r_total and score:  205.6000000000002 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  161401
dev_network_count:  539
learn step counter:  161451
dev_network_count:  539
learn step counter:  161501
dev_network_count:  539
learn step counter:  161551
dev_network_count:  539
learn step counter:  161601
dev_network_count:  539
learn step counter:  161651
dev_network_count:  539

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
540  r_total and score:  205.6000000000002 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  161701
dev_network_count:  540
learn step counter:  161751
dev_network_count:  540
learn step counter:  161801
dev_network_count:  540
learn step counter:  161851
dev_network_count:  540
learn step counter:  161901
dev_network_count:  540
learn step counter:  161951
dev_network_count:  540
EPOCH %d 165
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
3852 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
3853 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
3854 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
3855 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
3856 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
3857 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
3858 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
541  r_total and score:  205.6000000000002 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  162001
dev_network_count:  541
learn step counter:  162051
dev_network_count:  541
learn step counter:  162101
dev_network_count:  541
learn step counter:  162151
dev_network_count:  541
learn step counter:  162201
dev_network_count:  541
learn step counter:  162251
dev_network_count:  541

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
542  r_total and score:  205.6000000000002 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  162301
dev_network_count:  542
learn step counter:  162351
dev_network_count:  542
learn step counter:  162401
dev_network_count:  542
learn step counter:  162451
dev_network_count:  542
learn step counter:  162501
dev_network_count:  542
learn step counter:  162551
dev_network_count:  542

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
543  r_total and score:  205.6000000000002 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  162601
dev_network_count:  543
learn step counter:  162651
dev_network_count:  543
learn step counter:  162701
dev_network_count:  543
learn step counter:  162751
dev_network_count:  543
learn step counter:  162801
dev_network_count:  543
learn step counter:  162851
dev_network_count:  543

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
544  r_total and score:  205.6000000000002 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  162901
dev_network_count:  544
learn step counter:  162951
dev_network_count:  544
EPOCH %d 166
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1345 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1346 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1347 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1348 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1349 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1350 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1351 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  163001
dev_network_count:  544
learn step counter:  163051
dev_network_count:  544
learn step counter:  163101
dev_network_count:  544
learn step counter:  163151
dev_network_count:  544

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
545  r_total and score:  205.6000000000002 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  163201
dev_network_count:  545
learn step counter:  163251
dev_network_count:  545
learn step counter:  163301
dev_network_count:  545
learn step counter:  163351
dev_network_count:  545
learn step counter:  163401
dev_network_count:  545
learn step counter:  163451
dev_network_count:  545

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
546  r_total and score:  205.6000000000002 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  163501
dev_network_count:  546
learn step counter:  163551
dev_network_count:  546
learn step counter:  163601
dev_network_count:  546
learn step counter:  163651
dev_network_count:  546
learn step counter:  163701
dev_network_count:  546
learn step counter:  163751
dev_network_count:  546

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
547  r_total and score:  205.6000000000002 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  163801
dev_network_count:  547
learn step counter:  163851
dev_network_count:  547
learn step counter:  163901
dev_network_count:  547
learn step counter:  163951
dev_network_count:  547
EPOCH %d 167
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
3838 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
3839 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
3840 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
3841 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
3842 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
3843 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
3844 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  164001
dev_network_count:  547
learn step counter:  164051
dev_network_count:  547

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
548  r_total and score:  205.6000000000002 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  164101
dev_network_count:  548
learn step counter:  164151
dev_network_count:  548
learn step counter:  164201
dev_network_count:  548
learn step counter:  164251
dev_network_count:  548
learn step counter:  164301
dev_network_count:  548
learn step counter:  164351
dev_network_count:  548

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
549  r_total and score:  205.6000000000002 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  164401
dev_network_count:  549
learn step counter:  164451
dev_network_count:  549
learn step counter:  164501
dev_network_count:  549
learn step counter:  164551
dev_network_count:  549
learn step counter:  164601
dev_network_count:  549
learn step counter:  164651
dev_network_count:  549

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
550  r_total and score:  205.6000000000002 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  164701
dev_network_count:  550
learn step counter:  164751
dev_network_count:  550
learn step counter:  164801
dev_network_count:  550
learn step counter:  164851
dev_network_count:  550
learn step counter:  164901
dev_network_count:  550
learn step counter:  164951
dev_network_count:  550
EPOCH %d 168
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1331 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1332 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1333 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1334 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1335 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1336 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1337 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
551  r_total and score:  205.6000000000002 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  165001
dev_network_count:  551
learn step counter:  165051
dev_network_count:  551
learn step counter:  165101
dev_network_count:  551
learn step counter:  165151
dev_network_count:  551
learn step counter:  165201
dev_network_count:  551
learn step counter:  165251
dev_network_count:  551

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
552  r_total and score:  205.6000000000002 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  165301
dev_network_count:  552
learn step counter:  165351
dev_network_count:  552
learn step counter:  165401
dev_network_count:  552
learn step counter:  165451
dev_network_count:  552
learn step counter:  165501
dev_network_count:  552
learn step counter:  165551
dev_network_count:  552

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
553  r_total and score:  205.6000000000002 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  165601
dev_network_count:  553
learn step counter:  165651
dev_network_count:  553
learn step counter:  165701
dev_network_count:  553
learn step counter:  165751
dev_network_count:  553
learn step counter:  165801
dev_network_count:  553
learn step counter:  165851
dev_network_count:  553

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
554  r_total and score:  205.6000000000002 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  165901
dev_network_count:  554
learn step counter:  165951
dev_network_count:  554
EPOCH %d 169
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
3824 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
3825 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
3826 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
3827 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
3828 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
3829 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
3830 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  166001
dev_network_count:  554
learn step counter:  166051
dev_network_count:  554
learn step counter:  166101
dev_network_count:  554
learn step counter:  166151
dev_network_count:  554

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
555  r_total and score:  205.6000000000002 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  166201
dev_network_count:  555
learn step counter:  166251
dev_network_count:  555
learn step counter:  166301
dev_network_count:  555
learn step counter:  166351
dev_network_count:  555
learn step counter:  166401
dev_network_count:  555
learn step counter:  166451
dev_network_count:  555

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
556  r_total and score:  205.6000000000002 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  166501
dev_network_count:  556
learn step counter:  166551
dev_network_count:  556
learn step counter:  166601
dev_network_count:  556
learn step counter:  166651
dev_network_count:  556
learn step counter:  166701
dev_network_count:  556
learn step counter:  166751
dev_network_count:  556

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
557  r_total and score:  206.20000000000022 46.465330702480806
Current Bleu score is:  46.465330702480806
learn step counter:  166801
dev_network_count:  557
learn step counter:  166851
dev_network_count:  557
learn step counter:  166901
dev_network_count:  557
learn step counter:  166951
dev_network_count:  557
EPOCH %d 170
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1317 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1318 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1319 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1320 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1321 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1322 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1323 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  167001
dev_network_count:  557
learn step counter:  167051
dev_network_count:  557

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
558  r_total and score:  205.6000000000002 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  167101
dev_network_count:  558
learn step counter:  167151
dev_network_count:  558
learn step counter:  167201
dev_network_count:  558
learn step counter:  167251
dev_network_count:  558
learn step counter:  167301
dev_network_count:  558
learn step counter:  167351
dev_network_count:  558

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
559  r_total and score:  205.6000000000002 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  167401
dev_network_count:  559
learn step counter:  167451
dev_network_count:  559
learn step counter:  167501
dev_network_count:  559
learn step counter:  167551
dev_network_count:  559
learn step counter:  167601
dev_network_count:  559
learn step counter:  167651
dev_network_count:  559

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
560  r_total and score:  205.6000000000002 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  167701
dev_network_count:  560
learn step counter:  167751
dev_network_count:  560
learn step counter:  167801
dev_network_count:  560
learn step counter:  167851
dev_network_count:  560
learn step counter:  167901
dev_network_count:  560
learn step counter:  167951
dev_network_count:  560
EPOCH %d 171
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
3810 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
3811 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
3812 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
3813 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
3814 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
3815 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
3816 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
561  r_total and score:  205.6000000000002 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  168001
dev_network_count:  561
learn step counter:  168051
dev_network_count:  561
learn step counter:  168101
dev_network_count:  561
learn step counter:  168151
dev_network_count:  561
learn step counter:  168201
dev_network_count:  561
learn step counter:  168251
dev_network_count:  561

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
562  r_total and score:  205.6000000000002 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  168301
dev_network_count:  562
learn step counter:  168351
dev_network_count:  562
learn step counter:  168401
dev_network_count:  562
learn step counter:  168451
dev_network_count:  562
learn step counter:  168501
dev_network_count:  562
learn step counter:  168551
dev_network_count:  562

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
563  r_total and score:  205.6000000000002 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  168601
dev_network_count:  563
learn step counter:  168651
dev_network_count:  563
learn step counter:  168701
dev_network_count:  563
learn step counter:  168751
dev_network_count:  563
learn step counter:  168801
dev_network_count:  563
learn step counter:  168851
dev_network_count:  563

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
564  r_total and score:  205.6000000000002 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  168901
dev_network_count:  564
learn step counter:  168951
dev_network_count:  564
EPOCH %d 172
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1303 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1304 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1305 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1306 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1307 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1308 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1309 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  169001
dev_network_count:  564
learn step counter:  169051
dev_network_count:  564
learn step counter:  169101
dev_network_count:  564
learn step counter:  169151
dev_network_count:  564

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
565  r_total and score:  205.6000000000002 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  169201
dev_network_count:  565
learn step counter:  169251
dev_network_count:  565
learn step counter:  169301
dev_network_count:  565
learn step counter:  169351
dev_network_count:  565
learn step counter:  169401
dev_network_count:  565
learn step counter:  169451
dev_network_count:  565

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
566  r_total and score:  205.6000000000002 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  169501
dev_network_count:  566
learn step counter:  169551
dev_network_count:  566
learn step counter:  169601
dev_network_count:  566
learn step counter:  169651
dev_network_count:  566
learn step counter:  169701
dev_network_count:  566
learn step counter:  169751
dev_network_count:  566

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
567  r_total and score:  205.6000000000002 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  169801
dev_network_count:  567
learn step counter:  169851
dev_network_count:  567
learn step counter:  169901
dev_network_count:  567
learn step counter:  169951
dev_network_count:  567
EPOCH %d 173
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
3796 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
3797 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
3798 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
3799 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
3800 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
3801 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
3802 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  170001
dev_network_count:  567
learn step counter:  170051
dev_network_count:  567

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
568  r_total and score:  205.6000000000002 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  170101
dev_network_count:  568
learn step counter:  170151
dev_network_count:  568
learn step counter:  170201
dev_network_count:  568
learn step counter:  170251
dev_network_count:  568
learn step counter:  170301
dev_network_count:  568
learn step counter:  170351
dev_network_count:  568

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
569  r_total and score:  205.6000000000002 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  170401
dev_network_count:  569
learn step counter:  170451
dev_network_count:  569
learn step counter:  170501
dev_network_count:  569
learn step counter:  170551
dev_network_count:  569
learn step counter:  170601
dev_network_count:  569
learn step counter:  170651
dev_network_count:  569

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
570  r_total and score:  204.6000000000002 49.764755817731036
Current Bleu score is:  49.764755817731036
learn step counter:  170701
dev_network_count:  570
learn step counter:  170751
dev_network_count:  570
learn step counter:  170801
dev_network_count:  570
learn step counter:  170851
dev_network_count:  570
learn step counter:  170901
dev_network_count:  570
learn step counter:  170951
dev_network_count:  570
EPOCH %d 174
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1289 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1290 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1291 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1292 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1293 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1294 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1295 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 6 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '4', '3']
571  r_total and score:  205.6000000000002 48.129575172237274
Current Bleu score is:  48.129575172237274
learn step counter:  171001
dev_network_count:  571
learn step counter:  171051
dev_network_count:  571
learn step counter:  171101
dev_network_count:  571
learn step counter:  171151
dev_network_count:  571
learn step counter:  171201
dev_network_count:  571
learn step counter:  171251
dev_network_count:  571

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
572  r_total and score:  203.6000000000002 51.37364794625721
Current Bleu score is:  51.37364794625721
learn step counter:  171301
dev_network_count:  572
learn step counter:  171351
dev_network_count:  572
learn step counter:  171401
dev_network_count:  572
learn step counter:  171451
dev_network_count:  572
learn step counter:  171501
dev_network_count:  572
learn step counter:  171551
dev_network_count:  572

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
573  r_total and score:  203.6000000000002 51.37364794625721
Current Bleu score is:  51.37364794625721
learn step counter:  171601
dev_network_count:  573
learn step counter:  171651
dev_network_count:  573
learn step counter:  171701
dev_network_count:  573
learn step counter:  171751
dev_network_count:  573
learn step counter:  171801
dev_network_count:  573
learn step counter:  171851
dev_network_count:  573

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
574  r_total and score:  203.6000000000002 51.37364794625721
Current Bleu score is:  51.37364794625721
learn step counter:  171901
dev_network_count:  574
learn step counter:  171951
dev_network_count:  574
EPOCH %d 175
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
3782 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
3783 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
3784 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
3785 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
3786 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
3787 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
3788 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  172001
dev_network_count:  574
learn step counter:  172051
dev_network_count:  574
learn step counter:  172101
dev_network_count:  574
learn step counter:  172151
dev_network_count:  574

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
575  r_total and score:  203.6000000000002 51.37364794625721
Current Bleu score is:  51.37364794625721
learn step counter:  172201
dev_network_count:  575
learn step counter:  172251
dev_network_count:  575
learn step counter:  172301
dev_network_count:  575
learn step counter:  172351
dev_network_count:  575
learn step counter:  172401
dev_network_count:  575
learn step counter:  172451
dev_network_count:  575

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
576  r_total and score:  203.6000000000002 51.37364794625721
Current Bleu score is:  51.37364794625721
learn step counter:  172501
dev_network_count:  576
learn step counter:  172551
dev_network_count:  576
learn step counter:  172601
dev_network_count:  576
learn step counter:  172651
dev_network_count:  576
learn step counter:  172701
dev_network_count:  576
learn step counter:  172751
dev_network_count:  576

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
577  r_total and score:  203.6000000000002 51.37364794625721
Current Bleu score is:  51.37364794625721
learn step counter:  172801
dev_network_count:  577
learn step counter:  172851
dev_network_count:  577
learn step counter:  172901
dev_network_count:  577
learn step counter:  172951
dev_network_count:  577
EPOCH %d 176
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1275 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1276 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1277 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1278 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1279 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1280 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1281 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  173001
dev_network_count:  577
learn step counter:  173051
dev_network_count:  577

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
578  r_total and score:  203.6000000000002 51.37364794625721
Current Bleu score is:  51.37364794625721
learn step counter:  173101
dev_network_count:  578
learn step counter:  173151
dev_network_count:  578
learn step counter:  173201
dev_network_count:  578
learn step counter:  173251
dev_network_count:  578
learn step counter:  173301
dev_network_count:  578
learn step counter:  173351
dev_network_count:  578

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
579  r_total and score:  203.6000000000002 51.37364794625721
Current Bleu score is:  51.37364794625721
learn step counter:  173401
dev_network_count:  579
learn step counter:  173451
dev_network_count:  579
learn step counter:  173501
dev_network_count:  579
learn step counter:  173551
dev_network_count:  579
learn step counter:  173601
dev_network_count:  579
learn step counter:  173651
dev_network_count:  579

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
580  r_total and score:  203.6000000000002 51.37364794625721
Current Bleu score is:  51.37364794625721
learn step counter:  173701
dev_network_count:  580
learn step counter:  173751
dev_network_count:  580
learn step counter:  173801
dev_network_count:  580
learn step counter:  173851
dev_network_count:  580
learn step counter:  173901
dev_network_count:  580
learn step counter:  173951
dev_network_count:  580
EPOCH %d 177
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
3768 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
3769 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
3770 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
3771 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
3772 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
3773 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
3774 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
581  r_total and score:  203.6000000000002 51.37364794625721
Current Bleu score is:  51.37364794625721
learn step counter:  174001
dev_network_count:  581
learn step counter:  174051
dev_network_count:  581
learn step counter:  174101
dev_network_count:  581
learn step counter:  174151
dev_network_count:  581
learn step counter:  174201
dev_network_count:  581
learn step counter:  174251
dev_network_count:  581

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
582  r_total and score:  203.6000000000002 51.37364794625721
Current Bleu score is:  51.37364794625721
learn step counter:  174301
dev_network_count:  582
learn step counter:  174351
dev_network_count:  582
learn step counter:  174401
dev_network_count:  582
learn step counter:  174451
dev_network_count:  582
learn step counter:  174501
dev_network_count:  582
learn step counter:  174551
dev_network_count:  582

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
583  r_total and score:  203.6000000000002 51.37364794625721
Current Bleu score is:  51.37364794625721
learn step counter:  174601
dev_network_count:  583
learn step counter:  174651
dev_network_count:  583
learn step counter:  174701
dev_network_count:  583
learn step counter:  174751
dev_network_count:  583
learn step counter:  174801
dev_network_count:  583
learn step counter:  174851
dev_network_count:  583

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
584  r_total and score:  203.6000000000002 51.37364794625721
Current Bleu score is:  51.37364794625721
learn step counter:  174901
dev_network_count:  584
learn step counter:  174951
dev_network_count:  584
EPOCH %d 178
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1261 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1262 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1263 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1264 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1265 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1266 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1267 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  175001
dev_network_count:  584
learn step counter:  175051
dev_network_count:  584
learn step counter:  175101
dev_network_count:  584
learn step counter:  175151
dev_network_count:  584

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
585  r_total and score:  203.6000000000002 51.37364794625721
Current Bleu score is:  51.37364794625721
learn step counter:  175201
dev_network_count:  585
learn step counter:  175251
dev_network_count:  585
learn step counter:  175301
dev_network_count:  585
learn step counter:  175351
dev_network_count:  585
learn step counter:  175401
dev_network_count:  585
learn step counter:  175451
dev_network_count:  585

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
586  r_total and score:  203.6000000000002 51.37364794625721
Current Bleu score is:  51.37364794625721
learn step counter:  175501
dev_network_count:  586
learn step counter:  175551
dev_network_count:  586
learn step counter:  175601
dev_network_count:  586
learn step counter:  175651
dev_network_count:  586
learn step counter:  175701
dev_network_count:  586
learn step counter:  175751
dev_network_count:  586

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
587  r_total and score:  203.6000000000002 51.37364794625721
Current Bleu score is:  51.37364794625721
learn step counter:  175801
dev_network_count:  587
learn step counter:  175851
dev_network_count:  587
learn step counter:  175901
dev_network_count:  587
learn step counter:  175951
dev_network_count:  587
EPOCH %d 179
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
3754 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
3755 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
3756 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
3757 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
3758 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
3759 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
3760 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  176001
dev_network_count:  587
learn step counter:  176051
dev_network_count:  587

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
588  r_total and score:  203.6000000000002 51.37364794625721
Current Bleu score is:  51.37364794625721
learn step counter:  176101
dev_network_count:  588
learn step counter:  176151
dev_network_count:  588
learn step counter:  176201
dev_network_count:  588
learn step counter:  176251
dev_network_count:  588
learn step counter:  176301
dev_network_count:  588
learn step counter:  176351
dev_network_count:  588

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
589  r_total and score:  203.6000000000002 51.37364794625721
Current Bleu score is:  51.37364794625721
learn step counter:  176401
dev_network_count:  589
learn step counter:  176451
dev_network_count:  589
learn step counter:  176501
dev_network_count:  589
learn step counter:  176551
dev_network_count:  589
learn step counter:  176601
dev_network_count:  589
learn step counter:  176651
dev_network_count:  589

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
590  r_total and score:  203.6000000000002 51.37364794625721
Current Bleu score is:  51.37364794625721
learn step counter:  176701
dev_network_count:  590
learn step counter:  176751
dev_network_count:  590
learn step counter:  176801
dev_network_count:  590
learn step counter:  176851
dev_network_count:  590
learn step counter:  176901
dev_network_count:  590
learn step counter:  176951
dev_network_count:  590
EPOCH %d 180
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1247 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1248 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1249 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1250 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1251 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1252 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1253 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
591  r_total and score:  203.6000000000002 51.37364794625721
Current Bleu score is:  51.37364794625721
learn step counter:  177001
dev_network_count:  591
learn step counter:  177051
dev_network_count:  591
learn step counter:  177101
dev_network_count:  591
learn step counter:  177151
dev_network_count:  591
learn step counter:  177201
dev_network_count:  591
learn step counter:  177251
dev_network_count:  591

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
592  r_total and score:  203.6000000000002 51.37364794625721
Current Bleu score is:  51.37364794625721
learn step counter:  177301
dev_network_count:  592
learn step counter:  177351
dev_network_count:  592
learn step counter:  177401
dev_network_count:  592
learn step counter:  177451
dev_network_count:  592
learn step counter:  177501
dev_network_count:  592
learn step counter:  177551
dev_network_count:  592

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
593  r_total and score:  203.6000000000002 51.37364794625721
Current Bleu score is:  51.37364794625721
learn step counter:  177601
dev_network_count:  593
learn step counter:  177651
dev_network_count:  593
learn step counter:  177701
dev_network_count:  593
learn step counter:  177751
dev_network_count:  593
learn step counter:  177801
dev_network_count:  593
learn step counter:  177851
dev_network_count:  593

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
594  r_total and score:  203.6000000000002 51.37364794625721
Current Bleu score is:  51.37364794625721
learn step counter:  177901
dev_network_count:  594
learn step counter:  177951
dev_network_count:  594
EPOCH %d 181
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
3740 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
3741 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
3742 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
3743 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
3744 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
3745 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
3746 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  178001
dev_network_count:  594
learn step counter:  178051
dev_network_count:  594
learn step counter:  178101
dev_network_count:  594
learn step counter:  178151
dev_network_count:  594

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
595  r_total and score:  203.6000000000002 51.37364794625721
Current Bleu score is:  51.37364794625721
learn step counter:  178201
dev_network_count:  595
learn step counter:  178251
dev_network_count:  595
learn step counter:  178301
dev_network_count:  595
learn step counter:  178351
dev_network_count:  595
learn step counter:  178401
dev_network_count:  595
learn step counter:  178451
dev_network_count:  595

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
596  r_total and score:  203.6000000000002 51.37364794625721
Current Bleu score is:  51.37364794625721
learn step counter:  178501
dev_network_count:  596
learn step counter:  178551
dev_network_count:  596
learn step counter:  178601
dev_network_count:  596
learn step counter:  178651
dev_network_count:  596
learn step counter:  178701
dev_network_count:  596
learn step counter:  178751
dev_network_count:  596

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
597  r_total and score:  203.6000000000002 51.37364794625721
Current Bleu score is:  51.37364794625721
learn step counter:  178801
dev_network_count:  597
learn step counter:  178851
dev_network_count:  597
learn step counter:  178901
dev_network_count:  597
learn step counter:  178951
dev_network_count:  597
EPOCH %d 182
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1233 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1234 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1235 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1236 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1237 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1238 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1239 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  179001
dev_network_count:  597
learn step counter:  179051
dev_network_count:  597

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
598  r_total and score:  204.6000000000002 49.764755817731036
Current Bleu score is:  49.764755817731036
learn step counter:  179101
dev_network_count:  598
learn step counter:  179151
dev_network_count:  598
learn step counter:  179201
dev_network_count:  598
learn step counter:  179251
dev_network_count:  598
learn step counter:  179301
dev_network_count:  598
learn step counter:  179351
dev_network_count:  598

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
599  r_total and score:  203.6000000000002 51.37364794625721
Current Bleu score is:  51.37364794625721
learn step counter:  179401
dev_network_count:  599
learn step counter:  179451
dev_network_count:  599
learn step counter:  179501
dev_network_count:  599
learn step counter:  179551
dev_network_count:  599
learn step counter:  179601
dev_network_count:  599
learn step counter:  179651
dev_network_count:  599

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
600  r_total and score:  203.6000000000002 51.37364794625721
Current Bleu score is:  51.37364794625721
learn step counter:  179701
dev_network_count:  600
learn step counter:  179751
dev_network_count:  600
learn step counter:  179801
dev_network_count:  600
learn step counter:  179851
dev_network_count:  600
learn step counter:  179901
dev_network_count:  600
learn step counter:  179951
dev_network_count:  600
EPOCH %d 183
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
3726 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
3727 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
3728 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
3729 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
3730 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
3731 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
3732 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
601  r_total and score:  203.6000000000002 51.37364794625721
Current Bleu score is:  51.37364794625721
learn step counter:  180001
dev_network_count:  601
learn step counter:  180051
dev_network_count:  601
learn step counter:  180101
dev_network_count:  601
learn step counter:  180151
dev_network_count:  601
learn step counter:  180201
dev_network_count:  601
learn step counter:  180251
dev_network_count:  601

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
602  r_total and score:  203.6000000000002 51.37364794625721
Current Bleu score is:  51.37364794625721
learn step counter:  180301
dev_network_count:  602
learn step counter:  180351
dev_network_count:  602
learn step counter:  180401
dev_network_count:  602
learn step counter:  180451
dev_network_count:  602
learn step counter:  180501
dev_network_count:  602
learn step counter:  180551
dev_network_count:  602

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
603  r_total and score:  203.6000000000002 51.37364794625721
Current Bleu score is:  51.37364794625721
learn step counter:  180601
dev_network_count:  603
learn step counter:  180651
dev_network_count:  603
learn step counter:  180701
dev_network_count:  603
learn step counter:  180751
dev_network_count:  603
learn step counter:  180801
dev_network_count:  603
learn step counter:  180851
dev_network_count:  603

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
604  r_total and score:  203.6000000000002 51.37364794625721
Current Bleu score is:  51.37364794625721
learn step counter:  180901
dev_network_count:  604
learn step counter:  180951
dev_network_count:  604
EPOCH %d 184
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1219 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1220 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1221 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1222 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1223 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1224 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1225 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  181001
dev_network_count:  604
learn step counter:  181051
dev_network_count:  604
learn step counter:  181101
dev_network_count:  604
learn step counter:  181151
dev_network_count:  604

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
605  r_total and score:  203.6000000000002 51.37364794625721
Current Bleu score is:  51.37364794625721
learn step counter:  181201
dev_network_count:  605
learn step counter:  181251
dev_network_count:  605
learn step counter:  181301
dev_network_count:  605
learn step counter:  181351
dev_network_count:  605
learn step counter:  181401
dev_network_count:  605
learn step counter:  181451
dev_network_count:  605

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
606  r_total and score:  203.6000000000002 51.37364794625721
Current Bleu score is:  51.37364794625721
learn step counter:  181501
dev_network_count:  606
learn step counter:  181551
dev_network_count:  606
learn step counter:  181601
dev_network_count:  606
learn step counter:  181651
dev_network_count:  606
learn step counter:  181701
dev_network_count:  606
learn step counter:  181751
dev_network_count:  606

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
607  r_total and score:  203.6000000000002 51.37364794625721
Current Bleu score is:  51.37364794625721
learn step counter:  181801
dev_network_count:  607
learn step counter:  181851
dev_network_count:  607
learn step counter:  181901
dev_network_count:  607
learn step counter:  181951
dev_network_count:  607
EPOCH %d 185
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
3712 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
3713 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
3714 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
3715 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
3716 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
3717 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
3718 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  182001
dev_network_count:  607
learn step counter:  182051
dev_network_count:  607

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
608  r_total and score:  203.6000000000002 51.37364794625721
Current Bleu score is:  51.37364794625721
learn step counter:  182101
dev_network_count:  608
learn step counter:  182151
dev_network_count:  608
learn step counter:  182201
dev_network_count:  608
learn step counter:  182251
dev_network_count:  608
learn step counter:  182301
dev_network_count:  608
learn step counter:  182351
dev_network_count:  608

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
609  r_total and score:  203.6000000000002 51.37364794625721
Current Bleu score is:  51.37364794625721
learn step counter:  182401
dev_network_count:  609
learn step counter:  182451
dev_network_count:  609
learn step counter:  182501
dev_network_count:  609
learn step counter:  182551
dev_network_count:  609
learn step counter:  182601
dev_network_count:  609
learn step counter:  182651
dev_network_count:  609

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
610  r_total and score:  203.6000000000002 51.37364794625721
Current Bleu score is:  51.37364794625721
learn step counter:  182701
dev_network_count:  610
learn step counter:  182751
dev_network_count:  610
learn step counter:  182801
dev_network_count:  610
learn step counter:  182851
dev_network_count:  610
learn step counter:  182901
dev_network_count:  610
learn step counter:  182951
dev_network_count:  610
EPOCH %d 186
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1205 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1206 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1207 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1208 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1209 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1210 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1211 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
611  r_total and score:  203.6000000000002 51.37364794625721
Current Bleu score is:  51.37364794625721
learn step counter:  183001
dev_network_count:  611
learn step counter:  183051
dev_network_count:  611
learn step counter:  183101
dev_network_count:  611
learn step counter:  183151
dev_network_count:  611
learn step counter:  183201
dev_network_count:  611
learn step counter:  183251
dev_network_count:  611

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
612  r_total and score:  203.6000000000002 51.37364794625721
Current Bleu score is:  51.37364794625721
learn step counter:  183301
dev_network_count:  612
learn step counter:  183351
dev_network_count:  612
learn step counter:  183401
dev_network_count:  612
learn step counter:  183451
dev_network_count:  612
learn step counter:  183501
dev_network_count:  612
learn step counter:  183551
dev_network_count:  612

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
613  r_total and score:  203.6000000000002 51.37364794625721
Current Bleu score is:  51.37364794625721
learn step counter:  183601
dev_network_count:  613
learn step counter:  183651
dev_network_count:  613
learn step counter:  183701
dev_network_count:  613
learn step counter:  183751
dev_network_count:  613
learn step counter:  183801
dev_network_count:  613
learn step counter:  183851
dev_network_count:  613

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
614  r_total and score:  203.6000000000002 51.37364794625721
Current Bleu score is:  51.37364794625721
learn step counter:  183901
dev_network_count:  614
learn step counter:  183951
dev_network_count:  614
EPOCH %d 187
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
3698 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
3699 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
3700 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
3701 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
3702 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
3703 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
3704 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  184001
dev_network_count:  614
learn step counter:  184051
dev_network_count:  614
learn step counter:  184101
dev_network_count:  614
learn step counter:  184151
dev_network_count:  614

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
615  r_total and score:  203.6000000000002 51.37364794625721
Current Bleu score is:  51.37364794625721
learn step counter:  184201
dev_network_count:  615
learn step counter:  184251
dev_network_count:  615
learn step counter:  184301
dev_network_count:  615
learn step counter:  184351
dev_network_count:  615
learn step counter:  184401
dev_network_count:  615
learn step counter:  184451
dev_network_count:  615

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
616  r_total and score:  203.6000000000002 51.37364794625721
Current Bleu score is:  51.37364794625721
learn step counter:  184501
dev_network_count:  616
learn step counter:  184551
dev_network_count:  616
learn step counter:  184601
dev_network_count:  616
learn step counter:  184651
dev_network_count:  616
learn step counter:  184701
dev_network_count:  616
learn step counter:  184751
dev_network_count:  616

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
617  r_total and score:  203.6000000000002 51.37364794625721
Current Bleu score is:  51.37364794625721
learn step counter:  184801
dev_network_count:  617
learn step counter:  184851
dev_network_count:  617
learn step counter:  184901
dev_network_count:  617
learn step counter:  184951
dev_network_count:  617
EPOCH %d 188
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1191 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1192 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1193 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1194 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1195 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1196 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1197 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  185001
dev_network_count:  617
learn step counter:  185051
dev_network_count:  617

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
618  r_total and score:  203.6000000000002 51.37364794625721
Current Bleu score is:  51.37364794625721
learn step counter:  185101
dev_network_count:  618
learn step counter:  185151
dev_network_count:  618
learn step counter:  185201
dev_network_count:  618
learn step counter:  185251
dev_network_count:  618
learn step counter:  185301
dev_network_count:  618
learn step counter:  185351
dev_network_count:  618

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
619  r_total and score:  203.6000000000002 51.37364794625721
Current Bleu score is:  51.37364794625721
learn step counter:  185401
dev_network_count:  619
learn step counter:  185451
dev_network_count:  619
learn step counter:  185501
dev_network_count:  619
learn step counter:  185551
dev_network_count:  619
learn step counter:  185601
dev_network_count:  619
learn step counter:  185651
dev_network_count:  619

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
620  r_total and score:  203.6000000000002 51.37364794625721
Current Bleu score is:  51.37364794625721
learn step counter:  185701
dev_network_count:  620
learn step counter:  185751
dev_network_count:  620
learn step counter:  185801
dev_network_count:  620
learn step counter:  185851
dev_network_count:  620
learn step counter:  185901
dev_network_count:  620
learn step counter:  185951
dev_network_count:  620
EPOCH %d 189
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
3684 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
3685 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
3686 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
3687 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
3688 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
3689 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
3690 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
621  r_total and score:  203.6000000000002 51.37364794625721
Current Bleu score is:  51.37364794625721
learn step counter:  186001
dev_network_count:  621
learn step counter:  186051
dev_network_count:  621
learn step counter:  186101
dev_network_count:  621
learn step counter:  186151
dev_network_count:  621
learn step counter:  186201
dev_network_count:  621
learn step counter:  186251
dev_network_count:  621

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
622  r_total and score:  203.6000000000002 51.37364794625721
Current Bleu score is:  51.37364794625721
learn step counter:  186301
dev_network_count:  622
learn step counter:  186351
dev_network_count:  622
learn step counter:  186401
dev_network_count:  622
learn step counter:  186451
dev_network_count:  622
learn step counter:  186501
dev_network_count:  622
learn step counter:  186551
dev_network_count:  622

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
623  r_total and score:  203.6000000000002 51.37364794625721
Current Bleu score is:  51.37364794625721
learn step counter:  186601
dev_network_count:  623
learn step counter:  186651
dev_network_count:  623
learn step counter:  186701
dev_network_count:  623
learn step counter:  186751
dev_network_count:  623
learn step counter:  186801
dev_network_count:  623
learn step counter:  186851
dev_network_count:  623

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
624  r_total and score:  203.6000000000002 51.37364794625721
Current Bleu score is:  51.37364794625721
learn step counter:  186901
dev_network_count:  624
learn step counter:  186951
dev_network_count:  624
EPOCH %d 190
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1177 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1178 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1179 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1180 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1181 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1182 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1183 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  187001
dev_network_count:  624
learn step counter:  187051
dev_network_count:  624
learn step counter:  187101
dev_network_count:  624
learn step counter:  187151
dev_network_count:  624

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
625  r_total and score:  203.6000000000002 51.37364794625721
Current Bleu score is:  51.37364794625721
learn step counter:  187201
dev_network_count:  625
learn step counter:  187251
dev_network_count:  625
learn step counter:  187301
dev_network_count:  625
learn step counter:  187351
dev_network_count:  625
learn step counter:  187401
dev_network_count:  625
learn step counter:  187451
dev_network_count:  625

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
626  r_total and score:  203.6000000000002 51.37364794625721
Current Bleu score is:  51.37364794625721
learn step counter:  187501
dev_network_count:  626
learn step counter:  187551
dev_network_count:  626
learn step counter:  187601
dev_network_count:  626
learn step counter:  187651
dev_network_count:  626
learn step counter:  187701
dev_network_count:  626
learn step counter:  187751
dev_network_count:  626

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
627  r_total and score:  203.6000000000002 51.37364794625721
Current Bleu score is:  51.37364794625721
learn step counter:  187801
dev_network_count:  627
learn step counter:  187851
dev_network_count:  627
learn step counter:  187901
dev_network_count:  627
learn step counter:  187951
dev_network_count:  627
EPOCH %d 191
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
3670 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
3671 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
3672 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
3673 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
3674 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
3675 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
3676 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  188001
dev_network_count:  627
learn step counter:  188051
dev_network_count:  627

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
628  r_total and score:  203.6000000000002 51.37364794625721
Current Bleu score is:  51.37364794625721
learn step counter:  188101
dev_network_count:  628
learn step counter:  188151
dev_network_count:  628
learn step counter:  188201
dev_network_count:  628
learn step counter:  188251
dev_network_count:  628
learn step counter:  188301
dev_network_count:  628
learn step counter:  188351
dev_network_count:  628

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
629  r_total and score:  203.6000000000002 51.37364794625721
Current Bleu score is:  51.37364794625721
learn step counter:  188401
dev_network_count:  629
learn step counter:  188451
dev_network_count:  629
learn step counter:  188501
dev_network_count:  629
learn step counter:  188551
dev_network_count:  629
learn step counter:  188601
dev_network_count:  629
learn step counter:  188651
dev_network_count:  629

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
630  r_total and score:  203.6000000000002 51.37364794625721
Current Bleu score is:  51.37364794625721
learn step counter:  188701
dev_network_count:  630
learn step counter:  188751
dev_network_count:  630
learn step counter:  188801
dev_network_count:  630
learn step counter:  188851
dev_network_count:  630
learn step counter:  188901
dev_network_count:  630
learn step counter:  188951
dev_network_count:  630
EPOCH %d 192
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1163 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1164 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1165 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1166 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1167 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1168 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1169 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
631  r_total and score:  203.6000000000002 51.37364794625721
Current Bleu score is:  51.37364794625721
learn step counter:  189001
dev_network_count:  631
learn step counter:  189051
dev_network_count:  631
learn step counter:  189101
dev_network_count:  631
learn step counter:  189151
dev_network_count:  631
learn step counter:  189201
dev_network_count:  631
learn step counter:  189251
dev_network_count:  631

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
632  r_total and score:  203.6000000000002 51.37364794625721
Current Bleu score is:  51.37364794625721
learn step counter:  189301
dev_network_count:  632
learn step counter:  189351
dev_network_count:  632
learn step counter:  189401
dev_network_count:  632
learn step counter:  189451
dev_network_count:  632
learn step counter:  189501
dev_network_count:  632
learn step counter:  189551
dev_network_count:  632

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
633  r_total and score:  203.6000000000002 51.37364794625721
Current Bleu score is:  51.37364794625721
learn step counter:  189601
dev_network_count:  633
learn step counter:  189651
dev_network_count:  633
learn step counter:  189701
dev_network_count:  633
learn step counter:  189751
dev_network_count:  633
learn step counter:  189801
dev_network_count:  633
learn step counter:  189851
dev_network_count:  633

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
634  r_total and score:  203.6000000000002 51.37364794625721
Current Bleu score is:  51.37364794625721
learn step counter:  189901
dev_network_count:  634
learn step counter:  189951
dev_network_count:  634
EPOCH %d 193
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
3656 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
3657 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
3658 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
3659 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
3660 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
3661 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
3662 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  190001
dev_network_count:  634
learn step counter:  190051
dev_network_count:  634
learn step counter:  190101
dev_network_count:  634
learn step counter:  190151
dev_network_count:  634

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
635  r_total and score:  203.6000000000002 51.37364794625721
Current Bleu score is:  51.37364794625721
learn step counter:  190201
dev_network_count:  635
learn step counter:  190251
dev_network_count:  635
learn step counter:  190301
dev_network_count:  635
learn step counter:  190351
dev_network_count:  635
learn step counter:  190401
dev_network_count:  635
learn step counter:  190451
dev_network_count:  635

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
636  r_total and score:  203.6000000000002 51.37364794625721
Current Bleu score is:  51.37364794625721
learn step counter:  190501
dev_network_count:  636
learn step counter:  190551
dev_network_count:  636
learn step counter:  190601
dev_network_count:  636
learn step counter:  190651
dev_network_count:  636
learn step counter:  190701
dev_network_count:  636
learn step counter:  190751
dev_network_count:  636

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
637  r_total and score:  203.6000000000002 51.37364794625721
Current Bleu score is:  51.37364794625721
learn step counter:  190801
dev_network_count:  637
learn step counter:  190851
dev_network_count:  637
learn step counter:  190901
dev_network_count:  637
learn step counter:  190951
dev_network_count:  637
EPOCH %d 194
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1149 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1150 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1151 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1152 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1153 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1154 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1155 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  191001
dev_network_count:  637
learn step counter:  191051
dev_network_count:  637

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
638  r_total and score:  203.6000000000002 51.37364794625721
Current Bleu score is:  51.37364794625721
learn step counter:  191101
dev_network_count:  638
learn step counter:  191151
dev_network_count:  638
learn step counter:  191201
dev_network_count:  638
learn step counter:  191251
dev_network_count:  638
learn step counter:  191301
dev_network_count:  638
learn step counter:  191351
dev_network_count:  638

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
639  r_total and score:  203.6000000000002 51.37364794625721
Current Bleu score is:  51.37364794625721
learn step counter:  191401
dev_network_count:  639
learn step counter:  191451
dev_network_count:  639
learn step counter:  191501
dev_network_count:  639
learn step counter:  191551
dev_network_count:  639
learn step counter:  191601
dev_network_count:  639
learn step counter:  191651
dev_network_count:  639

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
640  r_total and score:  203.6000000000002 51.37364794625721
Current Bleu score is:  51.37364794625721
learn step counter:  191701
dev_network_count:  640
learn step counter:  191751
dev_network_count:  640
learn step counter:  191801
dev_network_count:  640
learn step counter:  191851
dev_network_count:  640
learn step counter:  191901
dev_network_count:  640
learn step counter:  191951
dev_network_count:  640
EPOCH %d 195
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
3642 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
3643 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
3644 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
3645 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
3646 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
3647 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
3648 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
641  r_total and score:  203.6000000000002 51.37364794625721
Current Bleu score is:  51.37364794625721
learn step counter:  192001
dev_network_count:  641
learn step counter:  192051
dev_network_count:  641
learn step counter:  192101
dev_network_count:  641
learn step counter:  192151
dev_network_count:  641
learn step counter:  192201
dev_network_count:  641
learn step counter:  192251
dev_network_count:  641

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
642  r_total and score:  203.6000000000002 51.37364794625721
Current Bleu score is:  51.37364794625721
learn step counter:  192301
dev_network_count:  642
learn step counter:  192351
dev_network_count:  642
learn step counter:  192401
dev_network_count:  642
learn step counter:  192451
dev_network_count:  642
learn step counter:  192501
dev_network_count:  642
learn step counter:  192551
dev_network_count:  642

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
643  r_total and score:  203.6000000000002 51.37364794625721
Current Bleu score is:  51.37364794625721
learn step counter:  192601
dev_network_count:  643
learn step counter:  192651
dev_network_count:  643
learn step counter:  192701
dev_network_count:  643
learn step counter:  192751
dev_network_count:  643
learn step counter:  192801
dev_network_count:  643
learn step counter:  192851
dev_network_count:  643

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
644  r_total and score:  203.6000000000002 51.37364794625721
Current Bleu score is:  51.37364794625721
learn step counter:  192901
dev_network_count:  644
learn step counter:  192951
dev_network_count:  644
EPOCH %d 196
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1135 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1136 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1137 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1138 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1139 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1140 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1141 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  193001
dev_network_count:  644
learn step counter:  193051
dev_network_count:  644
learn step counter:  193101
dev_network_count:  644
learn step counter:  193151
dev_network_count:  644

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
645  r_total and score:  203.6000000000002 51.37364794625721
Current Bleu score is:  51.37364794625721
learn step counter:  193201
dev_network_count:  645
learn step counter:  193251
dev_network_count:  645
learn step counter:  193301
dev_network_count:  645
learn step counter:  193351
dev_network_count:  645
learn step counter:  193401
dev_network_count:  645
learn step counter:  193451
dev_network_count:  645

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
646  r_total and score:  203.6000000000002 51.37364794625721
Current Bleu score is:  51.37364794625721
learn step counter:  193501
dev_network_count:  646
learn step counter:  193551
dev_network_count:  646
learn step counter:  193601
dev_network_count:  646
learn step counter:  193651
dev_network_count:  646
learn step counter:  193701
dev_network_count:  646
learn step counter:  193751
dev_network_count:  646

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
647  r_total and score:  203.6000000000002 51.37364794625721
Current Bleu score is:  51.37364794625721
learn step counter:  193801
dev_network_count:  647
learn step counter:  193851
dev_network_count:  647
learn step counter:  193901
dev_network_count:  647
learn step counter:  193951
dev_network_count:  647
EPOCH %d 197
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
3628 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
3629 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
3630 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
3631 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
3632 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
3633 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
3634 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  194001
dev_network_count:  647
learn step counter:  194051
dev_network_count:  647

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
648  r_total and score:  203.6000000000002 51.37364794625721
Current Bleu score is:  51.37364794625721
learn step counter:  194101
dev_network_count:  648
learn step counter:  194151
dev_network_count:  648
learn step counter:  194201
dev_network_count:  648
learn step counter:  194251
dev_network_count:  648
learn step counter:  194301
dev_network_count:  648
learn step counter:  194351
dev_network_count:  648

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
649  r_total and score:  203.6000000000002 51.37364794625721
Current Bleu score is:  51.37364794625721
learn step counter:  194401
dev_network_count:  649
learn step counter:  194451
dev_network_count:  649
learn step counter:  194501
dev_network_count:  649
learn step counter:  194551
dev_network_count:  649
learn step counter:  194601
dev_network_count:  649
learn step counter:  194651
dev_network_count:  649

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
650  r_total and score:  203.6000000000002 51.37364794625721
Current Bleu score is:  51.37364794625721
learn step counter:  194701
dev_network_count:  650
learn step counter:  194751
dev_network_count:  650
learn step counter:  194801
dev_network_count:  650
learn step counter:  194851
dev_network_count:  650
learn step counter:  194901
dev_network_count:  650
learn step counter:  194951
dev_network_count:  650
EPOCH %d 198
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1121 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1122 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1123 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1124 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1125 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1126 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1127 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
651  r_total and score:  203.6000000000002 51.37364794625721
Current Bleu score is:  51.37364794625721
learn step counter:  195001
dev_network_count:  651
learn step counter:  195051
dev_network_count:  651
learn step counter:  195101
dev_network_count:  651
learn step counter:  195151
dev_network_count:  651
learn step counter:  195201
dev_network_count:  651
learn step counter:  195251
dev_network_count:  651

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
652  r_total and score:  203.6000000000002 51.37364794625721
Current Bleu score is:  51.37364794625721
learn step counter:  195301
dev_network_count:  652
learn step counter:  195351
dev_network_count:  652
learn step counter:  195401
dev_network_count:  652
learn step counter:  195451
dev_network_count:  652
learn step counter:  195501
dev_network_count:  652
learn step counter:  195551
dev_network_count:  652

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
653  r_total and score:  203.6000000000002 51.37364794625721
Current Bleu score is:  51.37364794625721
learn step counter:  195601
dev_network_count:  653
learn step counter:  195651
dev_network_count:  653
learn step counter:  195701
dev_network_count:  653
learn step counter:  195751
dev_network_count:  653
learn step counter:  195801
dev_network_count:  653
learn step counter:  195851
dev_network_count:  653

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
654  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  195901
dev_network_count:  654
learn step counter:  195951
dev_network_count:  654
EPOCH %d 199
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
3614 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
3615 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
3616 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
3617 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
3618 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
3619 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
3620 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  196001
dev_network_count:  654
learn step counter:  196051
dev_network_count:  654
learn step counter:  196101
dev_network_count:  654
learn step counter:  196151
dev_network_count:  654

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
655  r_total and score:  194.40000000000015 49.31856086770742
Current Bleu score is:  49.31856086770742
learn step counter:  196201
dev_network_count:  655
learn step counter:  196251
dev_network_count:  655
learn step counter:  196301
dev_network_count:  655
learn step counter:  196351
dev_network_count:  655
learn step counter:  196401
dev_network_count:  655
learn step counter:  196451
dev_network_count:  655

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
656  r_total and score:  203.6000000000002 51.37364794625721
Current Bleu score is:  51.37364794625721
learn step counter:  196501
dev_network_count:  656
learn step counter:  196551
dev_network_count:  656
learn step counter:  196601
dev_network_count:  656
learn step counter:  196651
dev_network_count:  656
learn step counter:  196701
dev_network_count:  656
learn step counter:  196751
dev_network_count:  656

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
657  r_total and score:  194.40000000000015 49.31856086770742
Current Bleu score is:  49.31856086770742
learn step counter:  196801
dev_network_count:  657
learn step counter:  196851
dev_network_count:  657
learn step counter:  196901
dev_network_count:  657
learn step counter:  196951
dev_network_count:  657
EPOCH %d 200
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1107 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1108 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1109 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1110 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1111 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1112 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1113 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  197001
dev_network_count:  657
learn step counter:  197051
dev_network_count:  657

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
658  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  197101
dev_network_count:  658
learn step counter:  197151
dev_network_count:  658
learn step counter:  197201
dev_network_count:  658
learn step counter:  197251
dev_network_count:  658
learn step counter:  197301
dev_network_count:  658
learn step counter:  197351
dev_network_count:  658

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
659  r_total and score:  203.6000000000002 51.37364794625721
Current Bleu score is:  51.37364794625721
learn step counter:  197401
dev_network_count:  659
learn step counter:  197451
dev_network_count:  659
learn step counter:  197501
dev_network_count:  659
learn step counter:  197551
dev_network_count:  659
learn step counter:  197601
dev_network_count:  659
learn step counter:  197651
dev_network_count:  659

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
660  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  197701
dev_network_count:  660
learn step counter:  197751
dev_network_count:  660
learn step counter:  197801
dev_network_count:  660
learn step counter:  197851
dev_network_count:  660
learn step counter:  197901
dev_network_count:  660
learn step counter:  197951
dev_network_count:  660
EPOCH %d 201
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
3600 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
3601 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
3602 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
3603 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
3604 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
3605 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
3606 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
661  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  198001
dev_network_count:  661
learn step counter:  198051
dev_network_count:  661
learn step counter:  198101
dev_network_count:  661
learn step counter:  198151
dev_network_count:  661
learn step counter:  198201
dev_network_count:  661
learn step counter:  198251
dev_network_count:  661

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
662  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  198301
dev_network_count:  662
learn step counter:  198351
dev_network_count:  662
learn step counter:  198401
dev_network_count:  662
learn step counter:  198451
dev_network_count:  662
learn step counter:  198501
dev_network_count:  662
learn step counter:  198551
dev_network_count:  662

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
663  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  198601
dev_network_count:  663
learn step counter:  198651
dev_network_count:  663
learn step counter:  198701
dev_network_count:  663
learn step counter:  198751
dev_network_count:  663
learn step counter:  198801
dev_network_count:  663
learn step counter:  198851
dev_network_count:  663

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
664  r_total and score:  194.40000000000015 49.31856086770742
Current Bleu score is:  49.31856086770742
learn step counter:  198901
dev_network_count:  664
learn step counter:  198951
dev_network_count:  664
EPOCH %d 202
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1093 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1094 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1095 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1096 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1097 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1098 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1099 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  199001
dev_network_count:  664
learn step counter:  199051
dev_network_count:  664
learn step counter:  199101
dev_network_count:  664
learn step counter:  199151
dev_network_count:  664

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
665  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  199201
dev_network_count:  665
learn step counter:  199251
dev_network_count:  665
learn step counter:  199301
dev_network_count:  665
learn step counter:  199351
dev_network_count:  665
learn step counter:  199401
dev_network_count:  665
learn step counter:  199451
dev_network_count:  665

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
666  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  199501
dev_network_count:  666
learn step counter:  199551
dev_network_count:  666
learn step counter:  199601
dev_network_count:  666
learn step counter:  199651
dev_network_count:  666
learn step counter:  199701
dev_network_count:  666
learn step counter:  199751
dev_network_count:  666

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
667  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  199801
dev_network_count:  667
learn step counter:  199851
dev_network_count:  667
learn step counter:  199901
dev_network_count:  667
learn step counter:  199951
dev_network_count:  667
EPOCH %d 203
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
3586 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
3587 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
3588 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
3589 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
3590 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
3591 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
3592 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  200001
dev_network_count:  667
learn step counter:  200051
dev_network_count:  667

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
668  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  200101
dev_network_count:  668
learn step counter:  200151
dev_network_count:  668
learn step counter:  200201
dev_network_count:  668
learn step counter:  200251
dev_network_count:  668
learn step counter:  200301
dev_network_count:  668
learn step counter:  200351
dev_network_count:  668

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
669  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  200401
dev_network_count:  669
learn step counter:  200451
dev_network_count:  669
learn step counter:  200501
dev_network_count:  669
learn step counter:  200551
dev_network_count:  669
learn step counter:  200601
dev_network_count:  669
learn step counter:  200651
dev_network_count:  669

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
670  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  200701
dev_network_count:  670
learn step counter:  200751
dev_network_count:  670
learn step counter:  200801
dev_network_count:  670
learn step counter:  200851
dev_network_count:  670
learn step counter:  200901
dev_network_count:  670
learn step counter:  200951
dev_network_count:  670
EPOCH %d 204
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1079 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1080 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1081 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1082 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1083 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1084 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1085 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
671  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  201001
dev_network_count:  671
learn step counter:  201051
dev_network_count:  671
learn step counter:  201101
dev_network_count:  671
learn step counter:  201151
dev_network_count:  671
learn step counter:  201201
dev_network_count:  671
learn step counter:  201251
dev_network_count:  671

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
672  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  201301
dev_network_count:  672
learn step counter:  201351
dev_network_count:  672
learn step counter:  201401
dev_network_count:  672
learn step counter:  201451
dev_network_count:  672
learn step counter:  201501
dev_network_count:  672
learn step counter:  201551
dev_network_count:  672

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
673  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  201601
dev_network_count:  673
learn step counter:  201651
dev_network_count:  673
learn step counter:  201701
dev_network_count:  673
learn step counter:  201751
dev_network_count:  673
learn step counter:  201801
dev_network_count:  673
learn step counter:  201851
dev_network_count:  673

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
674  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  201901
dev_network_count:  674
learn step counter:  201951
dev_network_count:  674
EPOCH %d 205
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
3572 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
3573 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
3574 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
3575 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
3576 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
3577 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
3578 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  202001
dev_network_count:  674
learn step counter:  202051
dev_network_count:  674
learn step counter:  202101
dev_network_count:  674
learn step counter:  202151
dev_network_count:  674

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
675  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  202201
dev_network_count:  675
learn step counter:  202251
dev_network_count:  675
learn step counter:  202301
dev_network_count:  675
learn step counter:  202351
dev_network_count:  675
learn step counter:  202401
dev_network_count:  675
learn step counter:  202451
dev_network_count:  675

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
676  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  202501
dev_network_count:  676
learn step counter:  202551
dev_network_count:  676
learn step counter:  202601
dev_network_count:  676
learn step counter:  202651
dev_network_count:  676
learn step counter:  202701
dev_network_count:  676
learn step counter:  202751
dev_network_count:  676

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
677  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  202801
dev_network_count:  677
learn step counter:  202851
dev_network_count:  677
learn step counter:  202901
dev_network_count:  677
learn step counter:  202951
dev_network_count:  677
EPOCH %d 206
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1065 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1066 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1067 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1068 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1069 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1070 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1071 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  203001
dev_network_count:  677
learn step counter:  203051
dev_network_count:  677

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
678  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  203101
dev_network_count:  678
learn step counter:  203151
dev_network_count:  678
learn step counter:  203201
dev_network_count:  678
learn step counter:  203251
dev_network_count:  678
learn step counter:  203301
dev_network_count:  678
learn step counter:  203351
dev_network_count:  678

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
679  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  203401
dev_network_count:  679
learn step counter:  203451
dev_network_count:  679
learn step counter:  203501
dev_network_count:  679
learn step counter:  203551
dev_network_count:  679
learn step counter:  203601
dev_network_count:  679
learn step counter:  203651
dev_network_count:  679

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
680  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  203701
dev_network_count:  680
learn step counter:  203751
dev_network_count:  680
learn step counter:  203801
dev_network_count:  680
learn step counter:  203851
dev_network_count:  680
learn step counter:  203901
dev_network_count:  680
learn step counter:  203951
dev_network_count:  680
EPOCH %d 207
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
3558 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
3559 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
3560 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
3561 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
3562 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
3563 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
3564 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
681  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  204001
dev_network_count:  681
learn step counter:  204051
dev_network_count:  681
learn step counter:  204101
dev_network_count:  681
learn step counter:  204151
dev_network_count:  681
learn step counter:  204201
dev_network_count:  681
learn step counter:  204251
dev_network_count:  681

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
682  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  204301
dev_network_count:  682
learn step counter:  204351
dev_network_count:  682
learn step counter:  204401
dev_network_count:  682
learn step counter:  204451
dev_network_count:  682
learn step counter:  204501
dev_network_count:  682
learn step counter:  204551
dev_network_count:  682

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
683  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  204601
dev_network_count:  683
learn step counter:  204651
dev_network_count:  683
learn step counter:  204701
dev_network_count:  683
learn step counter:  204751
dev_network_count:  683
learn step counter:  204801
dev_network_count:  683
learn step counter:  204851
dev_network_count:  683

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
684  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  204901
dev_network_count:  684
learn step counter:  204951
dev_network_count:  684
EPOCH %d 208
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1051 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1052 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1053 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1054 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1055 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1056 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1057 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  205001
dev_network_count:  684
learn step counter:  205051
dev_network_count:  684
learn step counter:  205101
dev_network_count:  684
learn step counter:  205151
dev_network_count:  684

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
685  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  205201
dev_network_count:  685
learn step counter:  205251
dev_network_count:  685
learn step counter:  205301
dev_network_count:  685
learn step counter:  205351
dev_network_count:  685
learn step counter:  205401
dev_network_count:  685
learn step counter:  205451
dev_network_count:  685

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
686  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  205501
dev_network_count:  686
learn step counter:  205551
dev_network_count:  686
learn step counter:  205601
dev_network_count:  686
learn step counter:  205651
dev_network_count:  686
learn step counter:  205701
dev_network_count:  686
learn step counter:  205751
dev_network_count:  686

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
687  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  205801
dev_network_count:  687
learn step counter:  205851
dev_network_count:  687
learn step counter:  205901
dev_network_count:  687
learn step counter:  205951
dev_network_count:  687
EPOCH %d 209
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
3544 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
3545 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
3546 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
3547 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
3548 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
3549 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
3550 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  206001
dev_network_count:  687
learn step counter:  206051
dev_network_count:  687

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
688  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  206101
dev_network_count:  688
learn step counter:  206151
dev_network_count:  688
learn step counter:  206201
dev_network_count:  688
learn step counter:  206251
dev_network_count:  688
learn step counter:  206301
dev_network_count:  688
learn step counter:  206351
dev_network_count:  688

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
689  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  206401
dev_network_count:  689
learn step counter:  206451
dev_network_count:  689
learn step counter:  206501
dev_network_count:  689
learn step counter:  206551
dev_network_count:  689
learn step counter:  206601
dev_network_count:  689
learn step counter:  206651
dev_network_count:  689

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
690  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  206701
dev_network_count:  690
learn step counter:  206751
dev_network_count:  690
learn step counter:  206801
dev_network_count:  690
learn step counter:  206851
dev_network_count:  690
learn step counter:  206901
dev_network_count:  690
learn step counter:  206951
dev_network_count:  690
EPOCH %d 210
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1037 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1038 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1039 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1040 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1041 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1042 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1043 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
691  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  207001
dev_network_count:  691
learn step counter:  207051
dev_network_count:  691
learn step counter:  207101
dev_network_count:  691
learn step counter:  207151
dev_network_count:  691
learn step counter:  207201
dev_network_count:  691
learn step counter:  207251
dev_network_count:  691

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
692  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  207301
dev_network_count:  692
learn step counter:  207351
dev_network_count:  692
learn step counter:  207401
dev_network_count:  692
learn step counter:  207451
dev_network_count:  692
learn step counter:  207501
dev_network_count:  692
learn step counter:  207551
dev_network_count:  692

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
693  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  207601
dev_network_count:  693
learn step counter:  207651
dev_network_count:  693
learn step counter:  207701
dev_network_count:  693
learn step counter:  207751
dev_network_count:  693
learn step counter:  207801
dev_network_count:  693
learn step counter:  207851
dev_network_count:  693

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
694  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  207901
dev_network_count:  694
learn step counter:  207951
dev_network_count:  694
EPOCH %d 211
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
3530 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
3531 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
3532 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
3533 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
3534 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
3535 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
3536 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  208001
dev_network_count:  694
learn step counter:  208051
dev_network_count:  694
learn step counter:  208101
dev_network_count:  694
learn step counter:  208151
dev_network_count:  694

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
695  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  208201
dev_network_count:  695
learn step counter:  208251
dev_network_count:  695
learn step counter:  208301
dev_network_count:  695
learn step counter:  208351
dev_network_count:  695
learn step counter:  208401
dev_network_count:  695
learn step counter:  208451
dev_network_count:  695

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
696  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  208501
dev_network_count:  696
learn step counter:  208551
dev_network_count:  696
learn step counter:  208601
dev_network_count:  696
learn step counter:  208651
dev_network_count:  696
learn step counter:  208701
dev_network_count:  696
learn step counter:  208751
dev_network_count:  696

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
697  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  208801
dev_network_count:  697
learn step counter:  208851
dev_network_count:  697
learn step counter:  208901
dev_network_count:  697
learn step counter:  208951
dev_network_count:  697
EPOCH %d 212
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1023 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1024 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1025 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1026 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1027 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1028 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1029 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  209001
dev_network_count:  697
learn step counter:  209051
dev_network_count:  697

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
698  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  209101
dev_network_count:  698
learn step counter:  209151
dev_network_count:  698
learn step counter:  209201
dev_network_count:  698
learn step counter:  209251
dev_network_count:  698
learn step counter:  209301
dev_network_count:  698
learn step counter:  209351
dev_network_count:  698

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
699  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  209401
dev_network_count:  699
learn step counter:  209451
dev_network_count:  699
learn step counter:  209501
dev_network_count:  699
learn step counter:  209551
dev_network_count:  699
learn step counter:  209601
dev_network_count:  699
learn step counter:  209651
dev_network_count:  699

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
700  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  209701
dev_network_count:  700
learn step counter:  209751
dev_network_count:  700
learn step counter:  209801
dev_network_count:  700
learn step counter:  209851
dev_network_count:  700
learn step counter:  209901
dev_network_count:  700
learn step counter:  209951
dev_network_count:  700
EPOCH %d 213
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
3516 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
3517 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
3518 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
3519 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
3520 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
3521 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
3522 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
701  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  210001
dev_network_count:  701
learn step counter:  210051
dev_network_count:  701
learn step counter:  210101
dev_network_count:  701
learn step counter:  210151
dev_network_count:  701
learn step counter:  210201
dev_network_count:  701
learn step counter:  210251
dev_network_count:  701

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
702  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  210301
dev_network_count:  702
learn step counter:  210351
dev_network_count:  702
learn step counter:  210401
dev_network_count:  702
learn step counter:  210451
dev_network_count:  702
learn step counter:  210501
dev_network_count:  702
learn step counter:  210551
dev_network_count:  702

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
703  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  210601
dev_network_count:  703
learn step counter:  210651
dev_network_count:  703
learn step counter:  210701
dev_network_count:  703
learn step counter:  210751
dev_network_count:  703
learn step counter:  210801
dev_network_count:  703
learn step counter:  210851
dev_network_count:  703

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
704  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  210901
dev_network_count:  704
learn step counter:  210951
dev_network_count:  704
EPOCH %d 214
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1009 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1010 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1011 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1012 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1013 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1014 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1015 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  211001
dev_network_count:  704
learn step counter:  211051
dev_network_count:  704
learn step counter:  211101
dev_network_count:  704
learn step counter:  211151
dev_network_count:  704

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
705  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  211201
dev_network_count:  705
learn step counter:  211251
dev_network_count:  705
learn step counter:  211301
dev_network_count:  705
learn step counter:  211351
dev_network_count:  705
learn step counter:  211401
dev_network_count:  705
learn step counter:  211451
dev_network_count:  705

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
706  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  211501
dev_network_count:  706
learn step counter:  211551
dev_network_count:  706
learn step counter:  211601
dev_network_count:  706
learn step counter:  211651
dev_network_count:  706
learn step counter:  211701
dev_network_count:  706
learn step counter:  211751
dev_network_count:  706

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
707  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  211801
dev_network_count:  707
learn step counter:  211851
dev_network_count:  707
learn step counter:  211901
dev_network_count:  707
learn step counter:  211951
dev_network_count:  707
EPOCH %d 215
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
3502 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
3503 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
3504 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
3505 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
3506 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
3507 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
3508 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  212001
dev_network_count:  707
learn step counter:  212051
dev_network_count:  707

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
708  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  212101
dev_network_count:  708
learn step counter:  212151
dev_network_count:  708
learn step counter:  212201
dev_network_count:  708
learn step counter:  212251
dev_network_count:  708
learn step counter:  212301
dev_network_count:  708
learn step counter:  212351
dev_network_count:  708

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
709  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  212401
dev_network_count:  709
learn step counter:  212451
dev_network_count:  709
learn step counter:  212501
dev_network_count:  709
learn step counter:  212551
dev_network_count:  709
learn step counter:  212601
dev_network_count:  709
learn step counter:  212651
dev_network_count:  709

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
710  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  212701
dev_network_count:  710
learn step counter:  212751
dev_network_count:  710
learn step counter:  212801
dev_network_count:  710
learn step counter:  212851
dev_network_count:  710
learn step counter:  212901
dev_network_count:  710
learn step counter:  212951
dev_network_count:  710
EPOCH %d 216
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
995 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
996 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
997 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
998 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
999 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1000 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1001 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
711  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  213001
dev_network_count:  711
learn step counter:  213051
dev_network_count:  711
learn step counter:  213101
dev_network_count:  711
learn step counter:  213151
dev_network_count:  711
learn step counter:  213201
dev_network_count:  711
learn step counter:  213251
dev_network_count:  711

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
712  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  213301
dev_network_count:  712
learn step counter:  213351
dev_network_count:  712
learn step counter:  213401
dev_network_count:  712
learn step counter:  213451
dev_network_count:  712
learn step counter:  213501
dev_network_count:  712
learn step counter:  213551
dev_network_count:  712

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
713  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  213601
dev_network_count:  713
learn step counter:  213651
dev_network_count:  713
learn step counter:  213701
dev_network_count:  713
learn step counter:  213751
dev_network_count:  713
learn step counter:  213801
dev_network_count:  713
learn step counter:  213851
dev_network_count:  713

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
714  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  213901
dev_network_count:  714
learn step counter:  213951
dev_network_count:  714
EPOCH %d 217
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
3488 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
3489 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
3490 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
3491 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
3492 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
3493 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
3494 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  214001
dev_network_count:  714
learn step counter:  214051
dev_network_count:  714
learn step counter:  214101
dev_network_count:  714
learn step counter:  214151
dev_network_count:  714

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
715  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  214201
dev_network_count:  715
learn step counter:  214251
dev_network_count:  715
learn step counter:  214301
dev_network_count:  715
learn step counter:  214351
dev_network_count:  715
learn step counter:  214401
dev_network_count:  715
learn step counter:  214451
dev_network_count:  715

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
716  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  214501
dev_network_count:  716
learn step counter:  214551
dev_network_count:  716
learn step counter:  214601
dev_network_count:  716
learn step counter:  214651
dev_network_count:  716
learn step counter:  214701
dev_network_count:  716
learn step counter:  214751
dev_network_count:  716

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
717  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  214801
dev_network_count:  717
learn step counter:  214851
dev_network_count:  717
learn step counter:  214901
dev_network_count:  717
learn step counter:  214951
dev_network_count:  717
EPOCH %d 218
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
981 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
982 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
983 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
984 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
985 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
986 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
987 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  215001
dev_network_count:  717
learn step counter:  215051
dev_network_count:  717

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
718  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  215101
dev_network_count:  718
learn step counter:  215151
dev_network_count:  718
learn step counter:  215201
dev_network_count:  718
learn step counter:  215251
dev_network_count:  718
learn step counter:  215301
dev_network_count:  718
learn step counter:  215351
dev_network_count:  718

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
719  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  215401
dev_network_count:  719
learn step counter:  215451
dev_network_count:  719
learn step counter:  215501
dev_network_count:  719
learn step counter:  215551
dev_network_count:  719
learn step counter:  215601
dev_network_count:  719
learn step counter:  215651
dev_network_count:  719

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
720  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  215701
dev_network_count:  720
learn step counter:  215751
dev_network_count:  720
learn step counter:  215801
dev_network_count:  720
learn step counter:  215851
dev_network_count:  720
learn step counter:  215901
dev_network_count:  720
learn step counter:  215951
dev_network_count:  720
EPOCH %d 219
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
3474 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
3475 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
3476 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
3477 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
3478 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
3479 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
3480 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
721  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  216001
dev_network_count:  721
learn step counter:  216051
dev_network_count:  721
learn step counter:  216101
dev_network_count:  721
learn step counter:  216151
dev_network_count:  721
learn step counter:  216201
dev_network_count:  721
learn step counter:  216251
dev_network_count:  721

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
722  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  216301
dev_network_count:  722
learn step counter:  216351
dev_network_count:  722
learn step counter:  216401
dev_network_count:  722
learn step counter:  216451
dev_network_count:  722
learn step counter:  216501
dev_network_count:  722
learn step counter:  216551
dev_network_count:  722

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
723  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  216601
dev_network_count:  723
learn step counter:  216651
dev_network_count:  723
learn step counter:  216701
dev_network_count:  723
learn step counter:  216751
dev_network_count:  723
learn step counter:  216801
dev_network_count:  723
learn step counter:  216851
dev_network_count:  723

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
724  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  216901
dev_network_count:  724
learn step counter:  216951
dev_network_count:  724
EPOCH %d 220
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
967 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
968 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
969 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
970 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
971 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
972 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
973 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  217001
dev_network_count:  724
learn step counter:  217051
dev_network_count:  724
learn step counter:  217101
dev_network_count:  724
learn step counter:  217151
dev_network_count:  724

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
725  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  217201
dev_network_count:  725
learn step counter:  217251
dev_network_count:  725
learn step counter:  217301
dev_network_count:  725
learn step counter:  217351
dev_network_count:  725
learn step counter:  217401
dev_network_count:  725
learn step counter:  217451
dev_network_count:  725

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
726  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  217501
dev_network_count:  726
learn step counter:  217551
dev_network_count:  726
learn step counter:  217601
dev_network_count:  726
learn step counter:  217651
dev_network_count:  726
learn step counter:  217701
dev_network_count:  726
learn step counter:  217751
dev_network_count:  726

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
727  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  217801
dev_network_count:  727
learn step counter:  217851
dev_network_count:  727
learn step counter:  217901
dev_network_count:  727
learn step counter:  217951
dev_network_count:  727
EPOCH %d 221
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
3460 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
3461 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
3462 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
3463 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
3464 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
3465 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
3466 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  218001
dev_network_count:  727
learn step counter:  218051
dev_network_count:  727

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
728  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  218101
dev_network_count:  728
learn step counter:  218151
dev_network_count:  728
learn step counter:  218201
dev_network_count:  728
learn step counter:  218251
dev_network_count:  728
learn step counter:  218301
dev_network_count:  728
learn step counter:  218351
dev_network_count:  728

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
729  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  218401
dev_network_count:  729
learn step counter:  218451
dev_network_count:  729
learn step counter:  218501
dev_network_count:  729
learn step counter:  218551
dev_network_count:  729
learn step counter:  218601
dev_network_count:  729
learn step counter:  218651
dev_network_count:  729

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
730  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  218701
dev_network_count:  730
learn step counter:  218751
dev_network_count:  730
learn step counter:  218801
dev_network_count:  730
learn step counter:  218851
dev_network_count:  730
learn step counter:  218901
dev_network_count:  730
learn step counter:  218951
dev_network_count:  730
EPOCH %d 222
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
953 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
954 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
955 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
956 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
957 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
958 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
959 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
731  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  219001
dev_network_count:  731
learn step counter:  219051
dev_network_count:  731
learn step counter:  219101
dev_network_count:  731
learn step counter:  219151
dev_network_count:  731
learn step counter:  219201
dev_network_count:  731
learn step counter:  219251
dev_network_count:  731

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
732  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  219301
dev_network_count:  732
learn step counter:  219351
dev_network_count:  732
learn step counter:  219401
dev_network_count:  732
learn step counter:  219451
dev_network_count:  732
learn step counter:  219501
dev_network_count:  732
learn step counter:  219551
dev_network_count:  732

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
733  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  219601
dev_network_count:  733
learn step counter:  219651
dev_network_count:  733
learn step counter:  219701
dev_network_count:  733
learn step counter:  219751
dev_network_count:  733
learn step counter:  219801
dev_network_count:  733
learn step counter:  219851
dev_network_count:  733

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
734  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  219901
dev_network_count:  734
learn step counter:  219951
dev_network_count:  734
EPOCH %d 223
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
3446 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
3447 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
3448 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
3449 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
3450 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
3451 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
3452 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  220001
dev_network_count:  734
learn step counter:  220051
dev_network_count:  734
learn step counter:  220101
dev_network_count:  734
learn step counter:  220151
dev_network_count:  734

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
735  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  220201
dev_network_count:  735
learn step counter:  220251
dev_network_count:  735
learn step counter:  220301
dev_network_count:  735
learn step counter:  220351
dev_network_count:  735
learn step counter:  220401
dev_network_count:  735
learn step counter:  220451
dev_network_count:  735

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
736  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  220501
dev_network_count:  736
learn step counter:  220551
dev_network_count:  736
learn step counter:  220601
dev_network_count:  736
learn step counter:  220651
dev_network_count:  736
learn step counter:  220701
dev_network_count:  736
learn step counter:  220751
dev_network_count:  736

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
737  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  220801
dev_network_count:  737
learn step counter:  220851
dev_network_count:  737
learn step counter:  220901
dev_network_count:  737
learn step counter:  220951
dev_network_count:  737
EPOCH %d 224
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
939 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
940 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
941 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
942 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
943 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
944 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
945 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  221001
dev_network_count:  737
learn step counter:  221051
dev_network_count:  737

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
738  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  221101
dev_network_count:  738
learn step counter:  221151
dev_network_count:  738
learn step counter:  221201
dev_network_count:  738
learn step counter:  221251
dev_network_count:  738
learn step counter:  221301
dev_network_count:  738
learn step counter:  221351
dev_network_count:  738

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
739  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  221401
dev_network_count:  739
learn step counter:  221451
dev_network_count:  739
learn step counter:  221501
dev_network_count:  739
learn step counter:  221551
dev_network_count:  739
learn step counter:  221601
dev_network_count:  739
learn step counter:  221651
dev_network_count:  739

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
740  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  221701
dev_network_count:  740
learn step counter:  221751
dev_network_count:  740
learn step counter:  221801
dev_network_count:  740
learn step counter:  221851
dev_network_count:  740
learn step counter:  221901
dev_network_count:  740
learn step counter:  221951
dev_network_count:  740
EPOCH %d 225
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
3432 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
3433 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
3434 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
3435 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
3436 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
3437 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
3438 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
741  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  222001
dev_network_count:  741
learn step counter:  222051
dev_network_count:  741
learn step counter:  222101
dev_network_count:  741
learn step counter:  222151
dev_network_count:  741
learn step counter:  222201
dev_network_count:  741
learn step counter:  222251
dev_network_count:  741

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
742  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  222301
dev_network_count:  742
learn step counter:  222351
dev_network_count:  742
learn step counter:  222401
dev_network_count:  742
learn step counter:  222451
dev_network_count:  742
learn step counter:  222501
dev_network_count:  742
learn step counter:  222551
dev_network_count:  742

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
743  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  222601
dev_network_count:  743
learn step counter:  222651
dev_network_count:  743
learn step counter:  222701
dev_network_count:  743
learn step counter:  222751
dev_network_count:  743
learn step counter:  222801
dev_network_count:  743
learn step counter:  222851
dev_network_count:  743

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
744  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  222901
dev_network_count:  744
learn step counter:  222951
dev_network_count:  744
EPOCH %d 226
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
925 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
926 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
927 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
928 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
929 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
930 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
931 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  223001
dev_network_count:  744
learn step counter:  223051
dev_network_count:  744
learn step counter:  223101
dev_network_count:  744
learn step counter:  223151
dev_network_count:  744

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
745  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  223201
dev_network_count:  745
learn step counter:  223251
dev_network_count:  745
learn step counter:  223301
dev_network_count:  745
learn step counter:  223351
dev_network_count:  745
learn step counter:  223401
dev_network_count:  745
learn step counter:  223451
dev_network_count:  745

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
746  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  223501
dev_network_count:  746
learn step counter:  223551
dev_network_count:  746
learn step counter:  223601
dev_network_count:  746
learn step counter:  223651
dev_network_count:  746
learn step counter:  223701
dev_network_count:  746
learn step counter:  223751
dev_network_count:  746

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
747  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  223801
dev_network_count:  747
learn step counter:  223851
dev_network_count:  747
learn step counter:  223901
dev_network_count:  747
learn step counter:  223951
dev_network_count:  747
EPOCH %d 227
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
3418 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
3419 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
3420 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
3421 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
3422 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
3423 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
3424 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  224001
dev_network_count:  747
learn step counter:  224051
dev_network_count:  747

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
748  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  224101
dev_network_count:  748
learn step counter:  224151
dev_network_count:  748
learn step counter:  224201
dev_network_count:  748
learn step counter:  224251
dev_network_count:  748
learn step counter:  224301
dev_network_count:  748
learn step counter:  224351
dev_network_count:  748

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
749  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  224401
dev_network_count:  749
learn step counter:  224451
dev_network_count:  749
learn step counter:  224501
dev_network_count:  749
learn step counter:  224551
dev_network_count:  749
learn step counter:  224601
dev_network_count:  749
learn step counter:  224651
dev_network_count:  749

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
750  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  224701
dev_network_count:  750
learn step counter:  224751
dev_network_count:  750
learn step counter:  224801
dev_network_count:  750
learn step counter:  224851
dev_network_count:  750
learn step counter:  224901
dev_network_count:  750
learn step counter:  224951
dev_network_count:  750
EPOCH %d 228
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
911 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
912 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
913 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
914 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
915 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
916 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
917 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
751  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  225001
dev_network_count:  751
learn step counter:  225051
dev_network_count:  751
learn step counter:  225101
dev_network_count:  751
learn step counter:  225151
dev_network_count:  751
learn step counter:  225201
dev_network_count:  751
learn step counter:  225251
dev_network_count:  751

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
752  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  225301
dev_network_count:  752
learn step counter:  225351
dev_network_count:  752
learn step counter:  225401
dev_network_count:  752
learn step counter:  225451
dev_network_count:  752
learn step counter:  225501
dev_network_count:  752
learn step counter:  225551
dev_network_count:  752

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
753  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  225601
dev_network_count:  753
learn step counter:  225651
dev_network_count:  753
learn step counter:  225701
dev_network_count:  753
learn step counter:  225751
dev_network_count:  753
learn step counter:  225801
dev_network_count:  753
learn step counter:  225851
dev_network_count:  753

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
754  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  225901
dev_network_count:  754
learn step counter:  225951
dev_network_count:  754
EPOCH %d 229
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
3404 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
3405 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
3406 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
3407 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
3408 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
3409 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
3410 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  226001
dev_network_count:  754
learn step counter:  226051
dev_network_count:  754
learn step counter:  226101
dev_network_count:  754
learn step counter:  226151
dev_network_count:  754

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
755  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  226201
dev_network_count:  755
learn step counter:  226251
dev_network_count:  755
learn step counter:  226301
dev_network_count:  755
learn step counter:  226351
dev_network_count:  755
learn step counter:  226401
dev_network_count:  755
learn step counter:  226451
dev_network_count:  755

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
756  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  226501
dev_network_count:  756
learn step counter:  226551
dev_network_count:  756
learn step counter:  226601
dev_network_count:  756
learn step counter:  226651
dev_network_count:  756
learn step counter:  226701
dev_network_count:  756
learn step counter:  226751
dev_network_count:  756

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
757  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  226801
dev_network_count:  757
learn step counter:  226851
dev_network_count:  757
learn step counter:  226901
dev_network_count:  757
learn step counter:  226951
dev_network_count:  757
EPOCH %d 230
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
897 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
898 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
899 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
900 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
901 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
902 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
903 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  227001
dev_network_count:  757
learn step counter:  227051
dev_network_count:  757

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
758  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  227101
dev_network_count:  758
learn step counter:  227151
dev_network_count:  758
learn step counter:  227201
dev_network_count:  758
learn step counter:  227251
dev_network_count:  758
learn step counter:  227301
dev_network_count:  758
learn step counter:  227351
dev_network_count:  758

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
759  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  227401
dev_network_count:  759
learn step counter:  227451
dev_network_count:  759
learn step counter:  227501
dev_network_count:  759
learn step counter:  227551
dev_network_count:  759
learn step counter:  227601
dev_network_count:  759
learn step counter:  227651
dev_network_count:  759

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
760  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  227701
dev_network_count:  760
learn step counter:  227751
dev_network_count:  760
learn step counter:  227801
dev_network_count:  760
learn step counter:  227851
dev_network_count:  760
learn step counter:  227901
dev_network_count:  760
learn step counter:  227951
dev_network_count:  760
EPOCH %d 231
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
3390 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
3391 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
3392 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
3393 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
3394 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
3395 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
3396 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
761  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  228001
dev_network_count:  761
learn step counter:  228051
dev_network_count:  761
learn step counter:  228101
dev_network_count:  761
learn step counter:  228151
dev_network_count:  761
learn step counter:  228201
dev_network_count:  761
learn step counter:  228251
dev_network_count:  761

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
762  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  228301
dev_network_count:  762
learn step counter:  228351
dev_network_count:  762
learn step counter:  228401
dev_network_count:  762
learn step counter:  228451
dev_network_count:  762
learn step counter:  228501
dev_network_count:  762
learn step counter:  228551
dev_network_count:  762

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
763  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  228601
dev_network_count:  763
learn step counter:  228651
dev_network_count:  763
learn step counter:  228701
dev_network_count:  763
learn step counter:  228751
dev_network_count:  763
learn step counter:  228801
dev_network_count:  763
learn step counter:  228851
dev_network_count:  763

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
764  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  228901
dev_network_count:  764
learn step counter:  228951
dev_network_count:  764
EPOCH %d 232
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
883 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
884 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
885 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
886 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
887 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
888 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
889 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  229001
dev_network_count:  764
learn step counter:  229051
dev_network_count:  764
learn step counter:  229101
dev_network_count:  764
learn step counter:  229151
dev_network_count:  764

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
765  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  229201
dev_network_count:  765
learn step counter:  229251
dev_network_count:  765
learn step counter:  229301
dev_network_count:  765
learn step counter:  229351
dev_network_count:  765
learn step counter:  229401
dev_network_count:  765
learn step counter:  229451
dev_network_count:  765

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
766  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  229501
dev_network_count:  766
learn step counter:  229551
dev_network_count:  766
learn step counter:  229601
dev_network_count:  766
learn step counter:  229651
dev_network_count:  766
learn step counter:  229701
dev_network_count:  766
learn step counter:  229751
dev_network_count:  766

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
767  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  229801
dev_network_count:  767
learn step counter:  229851
dev_network_count:  767
learn step counter:  229901
dev_network_count:  767
learn step counter:  229951
dev_network_count:  767
EPOCH %d 233
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
3376 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
3377 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
3378 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
3379 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
3380 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
3381 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
3382 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  230001
dev_network_count:  767
learn step counter:  230051
dev_network_count:  767

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
768  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  230101
dev_network_count:  768
learn step counter:  230151
dev_network_count:  768
learn step counter:  230201
dev_network_count:  768
learn step counter:  230251
dev_network_count:  768
learn step counter:  230301
dev_network_count:  768
learn step counter:  230351
dev_network_count:  768

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
769  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  230401
dev_network_count:  769
learn step counter:  230451
dev_network_count:  769
learn step counter:  230501
dev_network_count:  769
learn step counter:  230551
dev_network_count:  769
learn step counter:  230601
dev_network_count:  769
learn step counter:  230651
dev_network_count:  769

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
770  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  230701
dev_network_count:  770
learn step counter:  230751
dev_network_count:  770
learn step counter:  230801
dev_network_count:  770
learn step counter:  230851
dev_network_count:  770
learn step counter:  230901
dev_network_count:  770
learn step counter:  230951
dev_network_count:  770
EPOCH %d 234
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
869 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
870 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
871 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
872 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
873 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
874 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
875 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
771  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  231001
dev_network_count:  771
learn step counter:  231051
dev_network_count:  771
learn step counter:  231101
dev_network_count:  771
learn step counter:  231151
dev_network_count:  771
learn step counter:  231201
dev_network_count:  771
learn step counter:  231251
dev_network_count:  771

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
772  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  231301
dev_network_count:  772
learn step counter:  231351
dev_network_count:  772
learn step counter:  231401
dev_network_count:  772
learn step counter:  231451
dev_network_count:  772
learn step counter:  231501
dev_network_count:  772
learn step counter:  231551
dev_network_count:  772

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
773  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  231601
dev_network_count:  773
learn step counter:  231651
dev_network_count:  773
learn step counter:  231701
dev_network_count:  773
learn step counter:  231751
dev_network_count:  773
learn step counter:  231801
dev_network_count:  773
learn step counter:  231851
dev_network_count:  773

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
774  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  231901
dev_network_count:  774
learn step counter:  231951
dev_network_count:  774
EPOCH %d 235
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
3362 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
3363 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
3364 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
3365 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
3366 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
3367 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
3368 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  232001
dev_network_count:  774
learn step counter:  232051
dev_network_count:  774
learn step counter:  232101
dev_network_count:  774
learn step counter:  232151
dev_network_count:  774

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
775  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  232201
dev_network_count:  775
learn step counter:  232251
dev_network_count:  775
learn step counter:  232301
dev_network_count:  775
learn step counter:  232351
dev_network_count:  775
learn step counter:  232401
dev_network_count:  775
learn step counter:  232451
dev_network_count:  775

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
776  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  232501
dev_network_count:  776
learn step counter:  232551
dev_network_count:  776
learn step counter:  232601
dev_network_count:  776
learn step counter:  232651
dev_network_count:  776
learn step counter:  232701
dev_network_count:  776
learn step counter:  232751
dev_network_count:  776

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
777  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  232801
dev_network_count:  777
learn step counter:  232851
dev_network_count:  777
learn step counter:  232901
dev_network_count:  777
learn step counter:  232951
dev_network_count:  777
EPOCH %d 236
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
855 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
856 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
857 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
858 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
859 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
860 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
861 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  233001
dev_network_count:  777
learn step counter:  233051
dev_network_count:  777

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
778  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  233101
dev_network_count:  778
learn step counter:  233151
dev_network_count:  778
learn step counter:  233201
dev_network_count:  778
learn step counter:  233251
dev_network_count:  778
learn step counter:  233301
dev_network_count:  778
learn step counter:  233351
dev_network_count:  778

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
779  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  233401
dev_network_count:  779
learn step counter:  233451
dev_network_count:  779
learn step counter:  233501
dev_network_count:  779
learn step counter:  233551
dev_network_count:  779
learn step counter:  233601
dev_network_count:  779
learn step counter:  233651
dev_network_count:  779

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
780  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  233701
dev_network_count:  780
learn step counter:  233751
dev_network_count:  780
learn step counter:  233801
dev_network_count:  780
learn step counter:  233851
dev_network_count:  780
learn step counter:  233901
dev_network_count:  780
learn step counter:  233951
dev_network_count:  780
EPOCH %d 237
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
3348 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
3349 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
3350 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
3351 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
3352 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
3353 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
3354 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
781  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  234001
dev_network_count:  781
learn step counter:  234051
dev_network_count:  781
learn step counter:  234101
dev_network_count:  781
learn step counter:  234151
dev_network_count:  781
learn step counter:  234201
dev_network_count:  781
learn step counter:  234251
dev_network_count:  781

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
782  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  234301
dev_network_count:  782
learn step counter:  234351
dev_network_count:  782
learn step counter:  234401
dev_network_count:  782
learn step counter:  234451
dev_network_count:  782
learn step counter:  234501
dev_network_count:  782
learn step counter:  234551
dev_network_count:  782

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
783  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  234601
dev_network_count:  783
learn step counter:  234651
dev_network_count:  783
learn step counter:  234701
dev_network_count:  783
learn step counter:  234751
dev_network_count:  783
learn step counter:  234801
dev_network_count:  783
learn step counter:  234851
dev_network_count:  783

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
784  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  234901
dev_network_count:  784
learn step counter:  234951
dev_network_count:  784
EPOCH %d 238
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
841 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
842 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
843 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
844 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
845 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
846 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
847 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  235001
dev_network_count:  784
learn step counter:  235051
dev_network_count:  784
learn step counter:  235101
dev_network_count:  784
learn step counter:  235151
dev_network_count:  784

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
785  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  235201
dev_network_count:  785
learn step counter:  235251
dev_network_count:  785
learn step counter:  235301
dev_network_count:  785
learn step counter:  235351
dev_network_count:  785
learn step counter:  235401
dev_network_count:  785
learn step counter:  235451
dev_network_count:  785

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
786  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  235501
dev_network_count:  786
learn step counter:  235551
dev_network_count:  786
learn step counter:  235601
dev_network_count:  786
learn step counter:  235651
dev_network_count:  786
learn step counter:  235701
dev_network_count:  786
learn step counter:  235751
dev_network_count:  786

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
787  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  235801
dev_network_count:  787
learn step counter:  235851
dev_network_count:  787
learn step counter:  235901
dev_network_count:  787
learn step counter:  235951
dev_network_count:  787
EPOCH %d 239
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
3334 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
3335 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
3336 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
3337 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
3338 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
3339 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
3340 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  236001
dev_network_count:  787
learn step counter:  236051
dev_network_count:  787

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
788  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  236101
dev_network_count:  788
learn step counter:  236151
dev_network_count:  788
learn step counter:  236201
dev_network_count:  788
learn step counter:  236251
dev_network_count:  788
learn step counter:  236301
dev_network_count:  788
learn step counter:  236351
dev_network_count:  788

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
789  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  236401
dev_network_count:  789
learn step counter:  236451
dev_network_count:  789
learn step counter:  236501
dev_network_count:  789
learn step counter:  236551
dev_network_count:  789
learn step counter:  236601
dev_network_count:  789
learn step counter:  236651
dev_network_count:  789

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
790  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  236701
dev_network_count:  790
learn step counter:  236751
dev_network_count:  790
learn step counter:  236801
dev_network_count:  790
learn step counter:  236851
dev_network_count:  790
learn step counter:  236901
dev_network_count:  790
learn step counter:  236951
dev_network_count:  790
EPOCH %d 240
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
827 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
828 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
829 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
830 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
831 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
832 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
833 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
791  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  237001
dev_network_count:  791
learn step counter:  237051
dev_network_count:  791
learn step counter:  237101
dev_network_count:  791
learn step counter:  237151
dev_network_count:  791
learn step counter:  237201
dev_network_count:  791
learn step counter:  237251
dev_network_count:  791

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
792  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  237301
dev_network_count:  792
learn step counter:  237351
dev_network_count:  792
learn step counter:  237401
dev_network_count:  792
learn step counter:  237451
dev_network_count:  792
learn step counter:  237501
dev_network_count:  792
learn step counter:  237551
dev_network_count:  792

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
793  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  237601
dev_network_count:  793
learn step counter:  237651
dev_network_count:  793
learn step counter:  237701
dev_network_count:  793
learn step counter:  237751
dev_network_count:  793
learn step counter:  237801
dev_network_count:  793
learn step counter:  237851
dev_network_count:  793

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
794  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  237901
dev_network_count:  794
learn step counter:  237951
dev_network_count:  794
EPOCH %d 241
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
3320 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
3321 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
3322 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
3323 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
3324 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
3325 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
3326 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  238001
dev_network_count:  794
learn step counter:  238051
dev_network_count:  794
learn step counter:  238101
dev_network_count:  794
learn step counter:  238151
dev_network_count:  794

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
795  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  238201
dev_network_count:  795
learn step counter:  238251
dev_network_count:  795
learn step counter:  238301
dev_network_count:  795
learn step counter:  238351
dev_network_count:  795
learn step counter:  238401
dev_network_count:  795
learn step counter:  238451
dev_network_count:  795

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
796  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  238501
dev_network_count:  796
learn step counter:  238551
dev_network_count:  796
learn step counter:  238601
dev_network_count:  796
learn step counter:  238651
dev_network_count:  796
learn step counter:  238701
dev_network_count:  796
learn step counter:  238751
dev_network_count:  796

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
797  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  238801
dev_network_count:  797
learn step counter:  238851
dev_network_count:  797
learn step counter:  238901
dev_network_count:  797
learn step counter:  238951
dev_network_count:  797
EPOCH %d 242
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
813 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
814 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
815 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
816 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
817 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
818 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
819 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  239001
dev_network_count:  797
learn step counter:  239051
dev_network_count:  797

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
798  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  239101
dev_network_count:  798
learn step counter:  239151
dev_network_count:  798
learn step counter:  239201
dev_network_count:  798
learn step counter:  239251
dev_network_count:  798
learn step counter:  239301
dev_network_count:  798
learn step counter:  239351
dev_network_count:  798

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
799  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  239401
dev_network_count:  799
learn step counter:  239451
dev_network_count:  799
learn step counter:  239501
dev_network_count:  799
learn step counter:  239551
dev_network_count:  799
learn step counter:  239601
dev_network_count:  799
learn step counter:  239651
dev_network_count:  799

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
800  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  239701
dev_network_count:  800
learn step counter:  239751
dev_network_count:  800
learn step counter:  239801
dev_network_count:  800
learn step counter:  239851
dev_network_count:  800
learn step counter:  239901
dev_network_count:  800
learn step counter:  239951
dev_network_count:  800
EPOCH %d 243
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
3306 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
3307 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
3308 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
3309 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
3310 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
3311 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
3312 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
801  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  240001
dev_network_count:  801
learn step counter:  240051
dev_network_count:  801
learn step counter:  240101
dev_network_count:  801
learn step counter:  240151
dev_network_count:  801
learn step counter:  240201
dev_network_count:  801
learn step counter:  240251
dev_network_count:  801

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
802  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  240301
dev_network_count:  802
learn step counter:  240351
dev_network_count:  802
learn step counter:  240401
dev_network_count:  802
learn step counter:  240451
dev_network_count:  802
learn step counter:  240501
dev_network_count:  802
learn step counter:  240551
dev_network_count:  802

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
803  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  240601
dev_network_count:  803
learn step counter:  240651
dev_network_count:  803
learn step counter:  240701
dev_network_count:  803
learn step counter:  240751
dev_network_count:  803
learn step counter:  240801
dev_network_count:  803
learn step counter:  240851
dev_network_count:  803

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
804  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  240901
dev_network_count:  804
learn step counter:  240951
dev_network_count:  804
EPOCH %d 244
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
799 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
800 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
801 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
802 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
803 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
804 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
805 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  241001
dev_network_count:  804
learn step counter:  241051
dev_network_count:  804
learn step counter:  241101
dev_network_count:  804
learn step counter:  241151
dev_network_count:  804

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
805  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  241201
dev_network_count:  805
learn step counter:  241251
dev_network_count:  805
learn step counter:  241301
dev_network_count:  805
learn step counter:  241351
dev_network_count:  805
learn step counter:  241401
dev_network_count:  805
learn step counter:  241451
dev_network_count:  805

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
806  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  241501
dev_network_count:  806
learn step counter:  241551
dev_network_count:  806
learn step counter:  241601
dev_network_count:  806
learn step counter:  241651
dev_network_count:  806
learn step counter:  241701
dev_network_count:  806
learn step counter:  241751
dev_network_count:  806

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
807  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  241801
dev_network_count:  807
learn step counter:  241851
dev_network_count:  807
learn step counter:  241901
dev_network_count:  807
learn step counter:  241951
dev_network_count:  807
EPOCH %d 245
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
3292 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
3293 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
3294 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
3295 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
3296 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
3297 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
3298 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  242001
dev_network_count:  807
learn step counter:  242051
dev_network_count:  807

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
808  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  242101
dev_network_count:  808
learn step counter:  242151
dev_network_count:  808
learn step counter:  242201
dev_network_count:  808
learn step counter:  242251
dev_network_count:  808
learn step counter:  242301
dev_network_count:  808
learn step counter:  242351
dev_network_count:  808

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
809  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  242401
dev_network_count:  809
learn step counter:  242451
dev_network_count:  809
learn step counter:  242501
dev_network_count:  809
learn step counter:  242551
dev_network_count:  809
learn step counter:  242601
dev_network_count:  809
learn step counter:  242651
dev_network_count:  809

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
810  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  242701
dev_network_count:  810
learn step counter:  242751
dev_network_count:  810
learn step counter:  242801
dev_network_count:  810
learn step counter:  242851
dev_network_count:  810
learn step counter:  242901
dev_network_count:  810
learn step counter:  242951
dev_network_count:  810
EPOCH %d 246
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
785 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
786 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
787 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
788 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
789 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
790 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
791 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
811  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  243001
dev_network_count:  811
learn step counter:  243051
dev_network_count:  811
learn step counter:  243101
dev_network_count:  811
learn step counter:  243151
dev_network_count:  811
learn step counter:  243201
dev_network_count:  811
learn step counter:  243251
dev_network_count:  811

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
812  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  243301
dev_network_count:  812
learn step counter:  243351
dev_network_count:  812
learn step counter:  243401
dev_network_count:  812
learn step counter:  243451
dev_network_count:  812
learn step counter:  243501
dev_network_count:  812
learn step counter:  243551
dev_network_count:  812

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
813  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  243601
dev_network_count:  813
learn step counter:  243651
dev_network_count:  813
learn step counter:  243701
dev_network_count:  813
learn step counter:  243751
dev_network_count:  813
learn step counter:  243801
dev_network_count:  813
learn step counter:  243851
dev_network_count:  813

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
814  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  243901
dev_network_count:  814
learn step counter:  243951
dev_network_count:  814
EPOCH %d 247
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
3278 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
3279 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
3280 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
3281 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
3282 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
3283 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
3284 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  244001
dev_network_count:  814
learn step counter:  244051
dev_network_count:  814
learn step counter:  244101
dev_network_count:  814
learn step counter:  244151
dev_network_count:  814

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
815  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  244201
dev_network_count:  815
learn step counter:  244251
dev_network_count:  815
learn step counter:  244301
dev_network_count:  815
learn step counter:  244351
dev_network_count:  815
learn step counter:  244401
dev_network_count:  815
learn step counter:  244451
dev_network_count:  815

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
816  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  244501
dev_network_count:  816
learn step counter:  244551
dev_network_count:  816
learn step counter:  244601
dev_network_count:  816
learn step counter:  244651
dev_network_count:  816
learn step counter:  244701
dev_network_count:  816
learn step counter:  244751
dev_network_count:  816

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
817  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  244801
dev_network_count:  817
learn step counter:  244851
dev_network_count:  817
learn step counter:  244901
dev_network_count:  817
learn step counter:  244951
dev_network_count:  817
EPOCH %d 248
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
771 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
772 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
773 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
774 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
775 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
776 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
777 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  245001
dev_network_count:  817
learn step counter:  245051
dev_network_count:  817

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
818  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  245101
dev_network_count:  818
learn step counter:  245151
dev_network_count:  818
learn step counter:  245201
dev_network_count:  818
learn step counter:  245251
dev_network_count:  818
learn step counter:  245301
dev_network_count:  818
learn step counter:  245351
dev_network_count:  818

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
819  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  245401
dev_network_count:  819
learn step counter:  245451
dev_network_count:  819
learn step counter:  245501
dev_network_count:  819
learn step counter:  245551
dev_network_count:  819
learn step counter:  245601
dev_network_count:  819
learn step counter:  245651
dev_network_count:  819

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
820  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  245701
dev_network_count:  820
learn step counter:  245751
dev_network_count:  820
learn step counter:  245801
dev_network_count:  820
learn step counter:  245851
dev_network_count:  820
learn step counter:  245901
dev_network_count:  820
learn step counter:  245951
dev_network_count:  820
EPOCH %d 249
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
3264 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
3265 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
3266 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
3267 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
3268 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
3269 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
3270 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
821  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  246001
dev_network_count:  821
learn step counter:  246051
dev_network_count:  821
learn step counter:  246101
dev_network_count:  821
learn step counter:  246151
dev_network_count:  821
learn step counter:  246201
dev_network_count:  821
learn step counter:  246251
dev_network_count:  821

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
822  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  246301
dev_network_count:  822
learn step counter:  246351
dev_network_count:  822
learn step counter:  246401
dev_network_count:  822
learn step counter:  246451
dev_network_count:  822
learn step counter:  246501
dev_network_count:  822
learn step counter:  246551
dev_network_count:  822

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
823  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  246601
dev_network_count:  823
learn step counter:  246651
dev_network_count:  823
learn step counter:  246701
dev_network_count:  823
learn step counter:  246751
dev_network_count:  823
learn step counter:  246801
dev_network_count:  823
learn step counter:  246851
dev_network_count:  823

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
824  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  246901
dev_network_count:  824
learn step counter:  246951
dev_network_count:  824
EPOCH %d 250
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
757 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
758 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
759 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
760 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
761 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
762 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
763 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  247001
dev_network_count:  824
learn step counter:  247051
dev_network_count:  824
learn step counter:  247101
dev_network_count:  824
learn step counter:  247151
dev_network_count:  824

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
825  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  247201
dev_network_count:  825
learn step counter:  247251
dev_network_count:  825
learn step counter:  247301
dev_network_count:  825
learn step counter:  247351
dev_network_count:  825
learn step counter:  247401
dev_network_count:  825
learn step counter:  247451
dev_network_count:  825

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
826  r_total and score:  193.40000000000015 50.30791127659767
Current Bleu score is:  50.30791127659767
learn step counter:  247501
dev_network_count:  826
learn step counter:  247551
dev_network_count:  826
learn step counter:  247601
dev_network_count:  826
learn step counter:  247651
dev_network_count:  826
learn step counter:  247701
dev_network_count:  826
learn step counter:  247751
dev_network_count:  826

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
827  r_total and score:  193.40000000000015 50.30791127659767
Current Bleu score is:  50.30791127659767
learn step counter:  247801
dev_network_count:  827
learn step counter:  247851
dev_network_count:  827
learn step counter:  247901
dev_network_count:  827
learn step counter:  247951
dev_network_count:  827
EPOCH %d 251
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
3250 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
3251 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
3252 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
3253 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
3254 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
3255 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
3256 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  248001
dev_network_count:  827
learn step counter:  248051
dev_network_count:  827

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
828  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  248101
dev_network_count:  828
learn step counter:  248151
dev_network_count:  828
learn step counter:  248201
dev_network_count:  828
learn step counter:  248251
dev_network_count:  828
learn step counter:  248301
dev_network_count:  828
learn step counter:  248351
dev_network_count:  828

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
829  r_total and score:  193.40000000000015 50.30791127659767
Current Bleu score is:  50.30791127659767
learn step counter:  248401
dev_network_count:  829
learn step counter:  248451
dev_network_count:  829
learn step counter:  248501
dev_network_count:  829
learn step counter:  248551
dev_network_count:  829
learn step counter:  248601
dev_network_count:  829
learn step counter:  248651
dev_network_count:  829

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
830  r_total and score:  193.40000000000015 50.30791127659767
Current Bleu score is:  50.30791127659767
learn step counter:  248701
dev_network_count:  830
learn step counter:  248751
dev_network_count:  830
learn step counter:  248801
dev_network_count:  830
learn step counter:  248851
dev_network_count:  830
learn step counter:  248901
dev_network_count:  830
learn step counter:  248951
dev_network_count:  830
EPOCH %d 252
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
743 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
744 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
745 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
746 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
747 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
748 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
749 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
831  r_total and score:  193.40000000000015 50.920228302927605
Current Bleu score is:  50.920228302927605
learn step counter:  249001
dev_network_count:  831
learn step counter:  249051
dev_network_count:  831
learn step counter:  249101
dev_network_count:  831
learn step counter:  249151
dev_network_count:  831
learn step counter:  249201
dev_network_count:  831
learn step counter:  249251
dev_network_count:  831

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
832  r_total and score:  193.40000000000015 50.30791127659767
Current Bleu score is:  50.30791127659767
learn step counter:  249301
dev_network_count:  832
learn step counter:  249351
dev_network_count:  832
learn step counter:  249401
dev_network_count:  832
learn step counter:  249451
dev_network_count:  832
learn step counter:  249501
dev_network_count:  832
learn step counter:  249551
dev_network_count:  832

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
833  r_total and score:  193.40000000000015 50.30791127659767
Current Bleu score is:  50.30791127659767
learn step counter:  249601
dev_network_count:  833
learn step counter:  249651
dev_network_count:  833
learn step counter:  249701
dev_network_count:  833
learn step counter:  249751
dev_network_count:  833
learn step counter:  249801
dev_network_count:  833
learn step counter:  249851
dev_network_count:  833

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
834  r_total and score:  193.40000000000015 50.30791127659767
Current Bleu score is:  50.30791127659767
learn step counter:  249901
dev_network_count:  834
learn step counter:  249951
dev_network_count:  834
EPOCH %d 253
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
3236 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
3237 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
3238 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
3239 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
3240 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
3241 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
3242 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  250001
dev_network_count:  834
learn step counter:  250051
dev_network_count:  834
learn step counter:  250101
dev_network_count:  834
learn step counter:  250151
dev_network_count:  834

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
835  r_total and score:  193.40000000000015 50.30791127659767
Current Bleu score is:  50.30791127659767
learn step counter:  250201
dev_network_count:  835
learn step counter:  250251
dev_network_count:  835
learn step counter:  250301
dev_network_count:  835
learn step counter:  250351
dev_network_count:  835
learn step counter:  250401
dev_network_count:  835
learn step counter:  250451
dev_network_count:  835

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
836  r_total and score:  193.40000000000015 50.30791127659767
Current Bleu score is:  50.30791127659767
learn step counter:  250501
dev_network_count:  836
learn step counter:  250551
dev_network_count:  836
learn step counter:  250601
dev_network_count:  836
learn step counter:  250651
dev_network_count:  836
learn step counter:  250701
dev_network_count:  836
learn step counter:  250751
dev_network_count:  836

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
837  r_total and score:  193.40000000000015 50.30791127659767
Current Bleu score is:  50.30791127659767
learn step counter:  250801
dev_network_count:  837
learn step counter:  250851
dev_network_count:  837
learn step counter:  250901
dev_network_count:  837
learn step counter:  250951
dev_network_count:  837
EPOCH %d 254
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
729 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
730 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
731 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
732 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
733 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
734 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
735 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  251001
dev_network_count:  837
learn step counter:  251051
dev_network_count:  837

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
838  r_total and score:  193.40000000000015 50.30791127659767
Current Bleu score is:  50.30791127659767
learn step counter:  251101
dev_network_count:  838
learn step counter:  251151
dev_network_count:  838
learn step counter:  251201
dev_network_count:  838
learn step counter:  251251
dev_network_count:  838
learn step counter:  251301
dev_network_count:  838
learn step counter:  251351
dev_network_count:  838

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
839  r_total and score:  193.40000000000015 50.30791127659767
Current Bleu score is:  50.30791127659767
learn step counter:  251401
dev_network_count:  839
learn step counter:  251451
dev_network_count:  839
learn step counter:  251501
dev_network_count:  839
learn step counter:  251551
dev_network_count:  839
learn step counter:  251601
dev_network_count:  839
learn step counter:  251651
dev_network_count:  839

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
840  r_total and score:  193.40000000000015 50.30791127659767
Current Bleu score is:  50.30791127659767
learn step counter:  251701
dev_network_count:  840
learn step counter:  251751
dev_network_count:  840
learn step counter:  251801
dev_network_count:  840
learn step counter:  251851
dev_network_count:  840
learn step counter:  251901
dev_network_count:  840
learn step counter:  251951
dev_network_count:  840
EPOCH %d 255
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
3222 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
3223 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
3224 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
3225 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
3226 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
3227 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
3228 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
841  r_total and score:  193.40000000000015 50.30791127659767
Current Bleu score is:  50.30791127659767
learn step counter:  252001
dev_network_count:  841
learn step counter:  252051
dev_network_count:  841
learn step counter:  252101
dev_network_count:  841
learn step counter:  252151
dev_network_count:  841
learn step counter:  252201
dev_network_count:  841
learn step counter:  252251
dev_network_count:  841

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
842  r_total and score:  193.40000000000015 50.30791127659767
Current Bleu score is:  50.30791127659767
learn step counter:  252301
dev_network_count:  842
learn step counter:  252351
dev_network_count:  842
learn step counter:  252401
dev_network_count:  842
learn step counter:  252451
dev_network_count:  842
learn step counter:  252501
dev_network_count:  842
learn step counter:  252551
dev_network_count:  842

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
843  r_total and score:  193.40000000000015 50.30791127659767
Current Bleu score is:  50.30791127659767
learn step counter:  252601
dev_network_count:  843
learn step counter:  252651
dev_network_count:  843
learn step counter:  252701
dev_network_count:  843
learn step counter:  252751
dev_network_count:  843
learn step counter:  252801
dev_network_count:  843
learn step counter:  252851
dev_network_count:  843

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
844  r_total and score:  193.40000000000015 50.30791127659767
Current Bleu score is:  50.30791127659767
learn step counter:  252901
dev_network_count:  844
learn step counter:  252951
dev_network_count:  844
EPOCH %d 256
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
715 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
716 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
717 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
718 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
719 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
720 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
721 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  253001
dev_network_count:  844
learn step counter:  253051
dev_network_count:  844
learn step counter:  253101
dev_network_count:  844
learn step counter:  253151
dev_network_count:  844

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
845  r_total and score:  193.40000000000015 50.30791127659767
Current Bleu score is:  50.30791127659767
learn step counter:  253201
dev_network_count:  845
learn step counter:  253251
dev_network_count:  845
learn step counter:  253301
dev_network_count:  845
learn step counter:  253351
dev_network_count:  845
learn step counter:  253401
dev_network_count:  845
learn step counter:  253451
dev_network_count:  845

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
846  r_total and score:  193.40000000000015 50.30791127659767
Current Bleu score is:  50.30791127659767
learn step counter:  253501
dev_network_count:  846
learn step counter:  253551
dev_network_count:  846
learn step counter:  253601
dev_network_count:  846
learn step counter:  253651
dev_network_count:  846
learn step counter:  253701
dev_network_count:  846
learn step counter:  253751
dev_network_count:  846

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
847  r_total and score:  193.40000000000015 50.30791127659767
Current Bleu score is:  50.30791127659767
learn step counter:  253801
dev_network_count:  847
learn step counter:  253851
dev_network_count:  847
learn step counter:  253901
dev_network_count:  847
learn step counter:  253951
dev_network_count:  847
EPOCH %d 257
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
3208 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
3209 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
3210 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
3211 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
3212 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
3213 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
3214 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  254001
dev_network_count:  847
learn step counter:  254051
dev_network_count:  847

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
848  r_total and score:  193.40000000000015 50.30791127659767
Current Bleu score is:  50.30791127659767
learn step counter:  254101
dev_network_count:  848
learn step counter:  254151
dev_network_count:  848
learn step counter:  254201
dev_network_count:  848
learn step counter:  254251
dev_network_count:  848
learn step counter:  254301
dev_network_count:  848
learn step counter:  254351
dev_network_count:  848

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
849  r_total and score:  193.40000000000015 50.30791127659767
Current Bleu score is:  50.30791127659767
learn step counter:  254401
dev_network_count:  849
learn step counter:  254451
dev_network_count:  849
learn step counter:  254501
dev_network_count:  849
learn step counter:  254551
dev_network_count:  849
learn step counter:  254601
dev_network_count:  849
learn step counter:  254651
dev_network_count:  849

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
850  r_total and score:  193.40000000000015 50.30791127659767
Current Bleu score is:  50.30791127659767
learn step counter:  254701
dev_network_count:  850
learn step counter:  254751
dev_network_count:  850
learn step counter:  254801
dev_network_count:  850
learn step counter:  254851
dev_network_count:  850
learn step counter:  254901
dev_network_count:  850
learn step counter:  254951
dev_network_count:  850
EPOCH %d 258
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
701 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
702 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
703 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
704 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
705 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
706 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
707 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
851  r_total and score:  193.40000000000015 50.30791127659767
Current Bleu score is:  50.30791127659767
learn step counter:  255001
dev_network_count:  851
learn step counter:  255051
dev_network_count:  851
learn step counter:  255101
dev_network_count:  851
learn step counter:  255151
dev_network_count:  851
learn step counter:  255201
dev_network_count:  851
learn step counter:  255251
dev_network_count:  851

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
852  r_total and score:  193.40000000000015 50.30791127659767
Current Bleu score is:  50.30791127659767
learn step counter:  255301
dev_network_count:  852
learn step counter:  255351
dev_network_count:  852
learn step counter:  255401
dev_network_count:  852
learn step counter:  255451
dev_network_count:  852
learn step counter:  255501
dev_network_count:  852
learn step counter:  255551
dev_network_count:  852

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
853  r_total and score:  193.40000000000015 50.30791127659767
Current Bleu score is:  50.30791127659767
learn step counter:  255601
dev_network_count:  853
learn step counter:  255651
dev_network_count:  853
learn step counter:  255701
dev_network_count:  853
learn step counter:  255751
dev_network_count:  853
learn step counter:  255801
dev_network_count:  853
learn step counter:  255851
dev_network_count:  853

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
854  r_total and score:  193.40000000000015 50.30791127659767
Current Bleu score is:  50.30791127659767
learn step counter:  255901
dev_network_count:  854
learn step counter:  255951
dev_network_count:  854
EPOCH %d 259
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
3194 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
3195 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
3196 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
3197 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
3198 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
3199 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
3200 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  256001
dev_network_count:  854
learn step counter:  256051
dev_network_count:  854
learn step counter:  256101
dev_network_count:  854
learn step counter:  256151
dev_network_count:  854

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
855  r_total and score:  193.40000000000015 50.30791127659767
Current Bleu score is:  50.30791127659767
learn step counter:  256201
dev_network_count:  855
learn step counter:  256251
dev_network_count:  855
learn step counter:  256301
dev_network_count:  855
learn step counter:  256351
dev_network_count:  855
learn step counter:  256401
dev_network_count:  855
learn step counter:  256451
dev_network_count:  855

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
856  r_total and score:  193.40000000000015 50.30791127659767
Current Bleu score is:  50.30791127659767
learn step counter:  256501
dev_network_count:  856
learn step counter:  256551
dev_network_count:  856
learn step counter:  256601
dev_network_count:  856
learn step counter:  256651
dev_network_count:  856
learn step counter:  256701
dev_network_count:  856
learn step counter:  256751
dev_network_count:  856

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
857  r_total and score:  182.60000000000014 49.69458861284427
Current Bleu score is:  49.69458861284427
learn step counter:  256801
dev_network_count:  857
learn step counter:  256851
dev_network_count:  857
learn step counter:  256901
dev_network_count:  857
learn step counter:  256951
dev_network_count:  857
EPOCH %d 260
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
687 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
688 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
689 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
690 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
691 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
692 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
693 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  257001
dev_network_count:  857
learn step counter:  257051
dev_network_count:  857

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
858  r_total and score:  182.60000000000014 49.69458861284427
Current Bleu score is:  49.69458861284427
learn step counter:  257101
dev_network_count:  858
learn step counter:  257151
dev_network_count:  858
learn step counter:  257201
dev_network_count:  858
learn step counter:  257251
dev_network_count:  858
learn step counter:  257301
dev_network_count:  858
learn step counter:  257351
dev_network_count:  858

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
859  r_total and score:  182.60000000000014 49.69458861284427
Current Bleu score is:  49.69458861284427
learn step counter:  257401
dev_network_count:  859
learn step counter:  257451
dev_network_count:  859
learn step counter:  257501
dev_network_count:  859
learn step counter:  257551
dev_network_count:  859
learn step counter:  257601
dev_network_count:  859
learn step counter:  257651
dev_network_count:  859

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
860  r_total and score:  182.60000000000014 49.69458861284427
Current Bleu score is:  49.69458861284427
learn step counter:  257701
dev_network_count:  860
learn step counter:  257751
dev_network_count:  860
learn step counter:  257801
dev_network_count:  860
learn step counter:  257851
dev_network_count:  860
learn step counter:  257901
dev_network_count:  860
learn step counter:  257951
dev_network_count:  860
EPOCH %d 261
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
3180 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
3181 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
3182 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
3183 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
3184 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
3185 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
3186 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
861  r_total and score:  182.60000000000014 49.69458861284427
Current Bleu score is:  49.69458861284427
learn step counter:  258001
dev_network_count:  861
learn step counter:  258051
dev_network_count:  861
learn step counter:  258101
dev_network_count:  861
learn step counter:  258151
dev_network_count:  861
learn step counter:  258201
dev_network_count:  861
learn step counter:  258251
dev_network_count:  861

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
862  r_total and score:  182.60000000000014 49.69458861284427
Current Bleu score is:  49.69458861284427
learn step counter:  258301
dev_network_count:  862
learn step counter:  258351
dev_network_count:  862
learn step counter:  258401
dev_network_count:  862
learn step counter:  258451
dev_network_count:  862
learn step counter:  258501
dev_network_count:  862
learn step counter:  258551
dev_network_count:  862

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
863  r_total and score:  182.60000000000014 49.69458861284427
Current Bleu score is:  49.69458861284427
learn step counter:  258601
dev_network_count:  863
learn step counter:  258651
dev_network_count:  863
learn step counter:  258701
dev_network_count:  863
learn step counter:  258751
dev_network_count:  863
learn step counter:  258801
dev_network_count:  863
learn step counter:  258851
dev_network_count:  863

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
864  r_total and score:  182.60000000000014 49.69458861284427
Current Bleu score is:  49.69458861284427
learn step counter:  258901
dev_network_count:  864
learn step counter:  258951
dev_network_count:  864
EPOCH %d 262
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
673 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
674 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
675 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
676 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
677 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
678 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
679 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  259001
dev_network_count:  864
learn step counter:  259051
dev_network_count:  864
learn step counter:  259101
dev_network_count:  864
learn step counter:  259151
dev_network_count:  864

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
865  r_total and score:  182.60000000000014 49.69458861284427
Current Bleu score is:  49.69458861284427
learn step counter:  259201
dev_network_count:  865
learn step counter:  259251
dev_network_count:  865
learn step counter:  259301
dev_network_count:  865
learn step counter:  259351
dev_network_count:  865
learn step counter:  259401
dev_network_count:  865
learn step counter:  259451
dev_network_count:  865

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
866  r_total and score:  182.60000000000014 49.69458861284427
Current Bleu score is:  49.69458861284427
learn step counter:  259501
dev_network_count:  866
learn step counter:  259551
dev_network_count:  866
learn step counter:  259601
dev_network_count:  866
learn step counter:  259651
dev_network_count:  866
learn step counter:  259701
dev_network_count:  866
learn step counter:  259751
dev_network_count:  866

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
867  r_total and score:  182.60000000000014 49.69458861284427
Current Bleu score is:  49.69458861284427
learn step counter:  259801
dev_network_count:  867
learn step counter:  259851
dev_network_count:  867
learn step counter:  259901
dev_network_count:  867
learn step counter:  259951
dev_network_count:  867
EPOCH %d 263
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
3166 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
3167 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
3168 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
3169 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
3170 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
3171 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
3172 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  260001
dev_network_count:  867
learn step counter:  260051
dev_network_count:  867

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
868  r_total and score:  182.60000000000014 49.69458861284427
Current Bleu score is:  49.69458861284427
learn step counter:  260101
dev_network_count:  868
learn step counter:  260151
dev_network_count:  868
learn step counter:  260201
dev_network_count:  868
learn step counter:  260251
dev_network_count:  868
learn step counter:  260301
dev_network_count:  868
learn step counter:  260351
dev_network_count:  868

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
869  r_total and score:  182.60000000000014 49.69458861284427
Current Bleu score is:  49.69458861284427
learn step counter:  260401
dev_network_count:  869
learn step counter:  260451
dev_network_count:  869
learn step counter:  260501
dev_network_count:  869
learn step counter:  260551
dev_network_count:  869
learn step counter:  260601
dev_network_count:  869
learn step counter:  260651
dev_network_count:  869

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
870  r_total and score:  182.60000000000014 49.69458861284427
Current Bleu score is:  49.69458861284427
learn step counter:  260701
dev_network_count:  870
learn step counter:  260751
dev_network_count:  870
learn step counter:  260801
dev_network_count:  870
learn step counter:  260851
dev_network_count:  870
learn step counter:  260901
dev_network_count:  870
learn step counter:  260951
dev_network_count:  870
EPOCH %d 264
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
659 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
660 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
661 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
662 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
663 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
664 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
665 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
871  r_total and score:  182.60000000000014 49.69458861284427
Current Bleu score is:  49.69458861284427
learn step counter:  261001
dev_network_count:  871
learn step counter:  261051
dev_network_count:  871
learn step counter:  261101
dev_network_count:  871
learn step counter:  261151
dev_network_count:  871
learn step counter:  261201
dev_network_count:  871
learn step counter:  261251
dev_network_count:  871

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
872  r_total and score:  182.60000000000014 49.69458861284427
Current Bleu score is:  49.69458861284427
learn step counter:  261301
dev_network_count:  872
learn step counter:  261351
dev_network_count:  872
learn step counter:  261401
dev_network_count:  872
learn step counter:  261451
dev_network_count:  872
learn step counter:  261501
dev_network_count:  872
learn step counter:  261551
dev_network_count:  872

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
873  r_total and score:  182.60000000000014 49.69458861284427
Current Bleu score is:  49.69458861284427
learn step counter:  261601
dev_network_count:  873
learn step counter:  261651
dev_network_count:  873
learn step counter:  261701
dev_network_count:  873
learn step counter:  261751
dev_network_count:  873
learn step counter:  261801
dev_network_count:  873
learn step counter:  261851
dev_network_count:  873

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
874  r_total and score:  182.60000000000014 49.69458861284427
Current Bleu score is:  49.69458861284427
learn step counter:  261901
dev_network_count:  874
learn step counter:  261951
dev_network_count:  874
EPOCH %d 265
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
3152 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
3153 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
3154 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
3155 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
3156 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
3157 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
3158 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  262001
dev_network_count:  874
learn step counter:  262051
dev_network_count:  874
learn step counter:  262101
dev_network_count:  874
learn step counter:  262151
dev_network_count:  874

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
875  r_total and score:  182.60000000000014 49.69458861284427
Current Bleu score is:  49.69458861284427
learn step counter:  262201
dev_network_count:  875
learn step counter:  262251
dev_network_count:  875
learn step counter:  262301
dev_network_count:  875
learn step counter:  262351
dev_network_count:  875
learn step counter:  262401
dev_network_count:  875
learn step counter:  262451
dev_network_count:  875

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
876  r_total and score:  182.60000000000014 49.69458861284427
Current Bleu score is:  49.69458861284427
learn step counter:  262501
dev_network_count:  876
learn step counter:  262551
dev_network_count:  876
learn step counter:  262601
dev_network_count:  876
learn step counter:  262651
dev_network_count:  876
learn step counter:  262701
dev_network_count:  876
learn step counter:  262751
dev_network_count:  876

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
877  r_total and score:  182.60000000000014 49.69458861284427
Current Bleu score is:  49.69458861284427
learn step counter:  262801
dev_network_count:  877
learn step counter:  262851
dev_network_count:  877
learn step counter:  262901
dev_network_count:  877
learn step counter:  262951
dev_network_count:  877
EPOCH %d 266
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
645 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
646 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
647 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
648 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
649 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
650 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
651 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  263001
dev_network_count:  877
learn step counter:  263051
dev_network_count:  877

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
878  r_total and score:  182.60000000000014 49.69458861284427
Current Bleu score is:  49.69458861284427
learn step counter:  263101
dev_network_count:  878
learn step counter:  263151
dev_network_count:  878
learn step counter:  263201
dev_network_count:  878
learn step counter:  263251
dev_network_count:  878
learn step counter:  263301
dev_network_count:  878
learn step counter:  263351
dev_network_count:  878

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
879  r_total and score:  182.60000000000014 49.69458861284427
Current Bleu score is:  49.69458861284427
learn step counter:  263401
dev_network_count:  879
learn step counter:  263451
dev_network_count:  879
learn step counter:  263501
dev_network_count:  879
learn step counter:  263551
dev_network_count:  879
learn step counter:  263601
dev_network_count:  879
learn step counter:  263651
dev_network_count:  879

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
880  r_total and score:  182.60000000000014 49.69458861284427
Current Bleu score is:  49.69458861284427
learn step counter:  263701
dev_network_count:  880
learn step counter:  263751
dev_network_count:  880
learn step counter:  263801
dev_network_count:  880
learn step counter:  263851
dev_network_count:  880
learn step counter:  263901
dev_network_count:  880
learn step counter:  263951
dev_network_count:  880
EPOCH %d 267
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
3138 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
3139 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
3140 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
3141 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
3142 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
3143 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
3144 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
881  r_total and score:  182.60000000000014 49.69458861284427
Current Bleu score is:  49.69458861284427
learn step counter:  264001
dev_network_count:  881
learn step counter:  264051
dev_network_count:  881
learn step counter:  264101
dev_network_count:  881
learn step counter:  264151
dev_network_count:  881
learn step counter:  264201
dev_network_count:  881
learn step counter:  264251
dev_network_count:  881

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
882  r_total and score:  182.60000000000014 49.69458861284427
Current Bleu score is:  49.69458861284427
learn step counter:  264301
dev_network_count:  882
learn step counter:  264351
dev_network_count:  882
learn step counter:  264401
dev_network_count:  882
learn step counter:  264451
dev_network_count:  882
learn step counter:  264501
dev_network_count:  882
learn step counter:  264551
dev_network_count:  882

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
883  r_total and score:  182.60000000000014 49.69458861284427
Current Bleu score is:  49.69458861284427
learn step counter:  264601
dev_network_count:  883
learn step counter:  264651
dev_network_count:  883
learn step counter:  264701
dev_network_count:  883
learn step counter:  264751
dev_network_count:  883
learn step counter:  264801
dev_network_count:  883
learn step counter:  264851
dev_network_count:  883

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
884  r_total and score:  182.60000000000014 49.69458861284427
Current Bleu score is:  49.69458861284427
learn step counter:  264901
dev_network_count:  884
learn step counter:  264951
dev_network_count:  884
EPOCH %d 268
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
631 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
632 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
633 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
634 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
635 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
636 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
637 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  265001
dev_network_count:  884
learn step counter:  265051
dev_network_count:  884
learn step counter:  265101
dev_network_count:  884
learn step counter:  265151
dev_network_count:  884

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
885  r_total and score:  182.60000000000014 49.69458861284427
Current Bleu score is:  49.69458861284427
learn step counter:  265201
dev_network_count:  885
learn step counter:  265251
dev_network_count:  885
learn step counter:  265301
dev_network_count:  885
learn step counter:  265351
dev_network_count:  885
learn step counter:  265401
dev_network_count:  885
learn step counter:  265451
dev_network_count:  885

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
886  r_total and score:  182.60000000000014 49.69458861284427
Current Bleu score is:  49.69458861284427
learn step counter:  265501
dev_network_count:  886
learn step counter:  265551
dev_network_count:  886
learn step counter:  265601
dev_network_count:  886
learn step counter:  265651
dev_network_count:  886
learn step counter:  265701
dev_network_count:  886
learn step counter:  265751
dev_network_count:  886

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
887  r_total and score:  182.60000000000014 49.69458861284427
Current Bleu score is:  49.69458861284427
learn step counter:  265801
dev_network_count:  887
learn step counter:  265851
dev_network_count:  887
learn step counter:  265901
dev_network_count:  887
learn step counter:  265951
dev_network_count:  887
EPOCH %d 269
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
3124 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
3125 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
3126 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
3127 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
3128 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
3129 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
3130 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  266001
dev_network_count:  887
learn step counter:  266051
dev_network_count:  887

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
888  r_total and score:  182.60000000000014 49.69458861284427
Current Bleu score is:  49.69458861284427
learn step counter:  266101
dev_network_count:  888
learn step counter:  266151
dev_network_count:  888
learn step counter:  266201
dev_network_count:  888
learn step counter:  266251
dev_network_count:  888
learn step counter:  266301
dev_network_count:  888
learn step counter:  266351
dev_network_count:  888

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
889  r_total and score:  182.60000000000014 49.69458861284427
Current Bleu score is:  49.69458861284427
learn step counter:  266401
dev_network_count:  889
learn step counter:  266451
dev_network_count:  889
learn step counter:  266501
dev_network_count:  889
learn step counter:  266551
dev_network_count:  889
learn step counter:  266601
dev_network_count:  889
learn step counter:  266651
dev_network_count:  889

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
890  r_total and score:  182.60000000000014 49.69458861284427
Current Bleu score is:  49.69458861284427
learn step counter:  266701
dev_network_count:  890
learn step counter:  266751
dev_network_count:  890
learn step counter:  266801
dev_network_count:  890
learn step counter:  266851
dev_network_count:  890
learn step counter:  266901
dev_network_count:  890
learn step counter:  266951
dev_network_count:  890
EPOCH %d 270
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
617 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
618 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
619 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
620 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
621 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
622 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
623 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
891  r_total and score:  182.60000000000014 49.69458861284427
Current Bleu score is:  49.69458861284427
learn step counter:  267001
dev_network_count:  891
learn step counter:  267051
dev_network_count:  891
learn step counter:  267101
dev_network_count:  891
learn step counter:  267151
dev_network_count:  891
learn step counter:  267201
dev_network_count:  891
learn step counter:  267251
dev_network_count:  891

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
892  r_total and score:  182.60000000000014 49.69458861284427
Current Bleu score is:  49.69458861284427
learn step counter:  267301
dev_network_count:  892
learn step counter:  267351
dev_network_count:  892
learn step counter:  267401
dev_network_count:  892
learn step counter:  267451
dev_network_count:  892
learn step counter:  267501
dev_network_count:  892
learn step counter:  267551
dev_network_count:  892

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
893  r_total and score:  182.60000000000014 49.69458861284427
Current Bleu score is:  49.69458861284427
learn step counter:  267601
dev_network_count:  893
learn step counter:  267651
dev_network_count:  893
learn step counter:  267701
dev_network_count:  893
learn step counter:  267751
dev_network_count:  893
learn step counter:  267801
dev_network_count:  893
learn step counter:  267851
dev_network_count:  893

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
894  r_total and score:  182.60000000000014 49.69458861284427
Current Bleu score is:  49.69458861284427
learn step counter:  267901
dev_network_count:  894
learn step counter:  267951
dev_network_count:  894
EPOCH %d 271
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
3110 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
3111 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
3112 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
3113 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
3114 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
3115 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
3116 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  268001
dev_network_count:  894
learn step counter:  268051
dev_network_count:  894
learn step counter:  268101
dev_network_count:  894
learn step counter:  268151
dev_network_count:  894

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
895  r_total and score:  182.60000000000014 49.69458861284427
Current Bleu score is:  49.69458861284427
learn step counter:  268201
dev_network_count:  895
learn step counter:  268251
dev_network_count:  895
learn step counter:  268301
dev_network_count:  895
learn step counter:  268351
dev_network_count:  895
learn step counter:  268401
dev_network_count:  895
learn step counter:  268451
dev_network_count:  895

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
896  r_total and score:  182.60000000000014 49.69458861284427
Current Bleu score is:  49.69458861284427
learn step counter:  268501
dev_network_count:  896
learn step counter:  268551
dev_network_count:  896
learn step counter:  268601
dev_network_count:  896
learn step counter:  268651
dev_network_count:  896
learn step counter:  268701
dev_network_count:  896
learn step counter:  268751
dev_network_count:  896

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
897  r_total and score:  182.60000000000014 49.69458861284427
Current Bleu score is:  49.69458861284427
learn step counter:  268801
dev_network_count:  897
learn step counter:  268851
dev_network_count:  897
learn step counter:  268901
dev_network_count:  897
learn step counter:  268951
dev_network_count:  897
EPOCH %d 272
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
603 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
604 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
605 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
606 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
607 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
608 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
609 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  269001
dev_network_count:  897
learn step counter:  269051
dev_network_count:  897

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 7 3]]
Reward:  [-2.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '3', '3 3', '1', '3']
898  r_total and score:  190.60000000000014 47.64443691262452
Current Bleu score is:  47.64443691262452
learn step counter:  269101
dev_network_count:  898
learn step counter:  269151
dev_network_count:  898
learn step counter:  269201
dev_network_count:  898
learn step counter:  269251
dev_network_count:  898
learn step counter:  269301
dev_network_count:  898
learn step counter:  269351
dev_network_count:  898

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
899  r_total and score:  182.60000000000014 49.69458861284427
Current Bleu score is:  49.69458861284427
learn step counter:  269401
dev_network_count:  899
learn step counter:  269451
dev_network_count:  899
learn step counter:  269501
dev_network_count:  899
learn step counter:  269551
dev_network_count:  899
learn step counter:  269601
dev_network_count:  899
learn step counter:  269651
dev_network_count:  899

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
900  r_total and score:  182.60000000000014 49.69458861284427
Current Bleu score is:  49.69458861284427
learn step counter:  269701
dev_network_count:  900
learn step counter:  269751
dev_network_count:  900
learn step counter:  269801
dev_network_count:  900
learn step counter:  269851
dev_network_count:  900
learn step counter:  269901
dev_network_count:  900
learn step counter:  269951
dev_network_count:  900
EPOCH %d 273
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
3096 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
3097 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
3098 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
3099 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
3100 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
3101 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
3102 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
901  r_total and score:  182.60000000000014 49.69458861284427
Current Bleu score is:  49.69458861284427
learn step counter:  270001
dev_network_count:  901
learn step counter:  270051
dev_network_count:  901
learn step counter:  270101
dev_network_count:  901
learn step counter:  270151
dev_network_count:  901
learn step counter:  270201
dev_network_count:  901
learn step counter:  270251
dev_network_count:  901

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
902  r_total and score:  182.60000000000014 49.69458861284427
Current Bleu score is:  49.69458861284427
learn step counter:  270301
dev_network_count:  902
learn step counter:  270351
dev_network_count:  902
learn step counter:  270401
dev_network_count:  902
learn step counter:  270451
dev_network_count:  902
learn step counter:  270501
dev_network_count:  902
learn step counter:  270551
dev_network_count:  902

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
903  r_total and score:  182.60000000000014 49.69458861284427
Current Bleu score is:  49.69458861284427
learn step counter:  270601
dev_network_count:  903
learn step counter:  270651
dev_network_count:  903
learn step counter:  270701
dev_network_count:  903
learn step counter:  270751
dev_network_count:  903
learn step counter:  270801
dev_network_count:  903
learn step counter:  270851
dev_network_count:  903

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
904  r_total and score:  182.60000000000014 49.69458861284427
Current Bleu score is:  49.69458861284427
learn step counter:  270901
dev_network_count:  904
learn step counter:  270951
dev_network_count:  904
EPOCH %d 274
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
589 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
590 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
591 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
592 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
593 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
594 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
595 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  271001
dev_network_count:  904
learn step counter:  271051
dev_network_count:  904
learn step counter:  271101
dev_network_count:  904
learn step counter:  271151
dev_network_count:  904

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 7 3]]
Reward:  [-2.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '3', '3 3', '1', '3']
905  r_total and score:  190.60000000000014 47.64443691262452
Current Bleu score is:  47.64443691262452
learn step counter:  271201
dev_network_count:  905
learn step counter:  271251
dev_network_count:  905
learn step counter:  271301
dev_network_count:  905
learn step counter:  271351
dev_network_count:  905
learn step counter:  271401
dev_network_count:  905
learn step counter:  271451
dev_network_count:  905

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '4', '3 3', '1', '3']
906  r_total and score:  182.60000000000014 49.69458861284427
Current Bleu score is:  49.69458861284427
learn step counter:  271501
dev_network_count:  906
learn step counter:  271551
dev_network_count:  906
learn step counter:  271601
dev_network_count:  906
learn step counter:  271651
dev_network_count:  906
learn step counter:  271701
dev_network_count:  906
learn step counter:  271751
dev_network_count:  906

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 7 3]]
Reward:  [-2.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '3', '3 3', '1', '3']
907  r_total and score:  190.60000000000014 47.64443691262452
Current Bleu score is:  47.64443691262452
learn step counter:  271801
dev_network_count:  907
learn step counter:  271851
dev_network_count:  907
learn step counter:  271901
dev_network_count:  907
learn step counter:  271951
dev_network_count:  907
EPOCH %d 275
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
3082 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
3083 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
3084 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
3085 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
3086 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
3087 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
3088 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  272001
dev_network_count:  907
learn step counter:  272051
dev_network_count:  907

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 7 3]]
Reward:  [-2.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '3', '3 3', '1', '3']
908  r_total and score:  190.60000000000014 47.64443691262452
Current Bleu score is:  47.64443691262452
learn step counter:  272101
dev_network_count:  908
learn step counter:  272151
dev_network_count:  908
learn step counter:  272201
dev_network_count:  908
learn step counter:  272251
dev_network_count:  908
learn step counter:  272301
dev_network_count:  908
learn step counter:  272351
dev_network_count:  908

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 7 3]]
Reward:  [-2.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '3', '3 3', '1', '3']
909  r_total and score:  190.60000000000014 47.64443691262452
Current Bleu score is:  47.64443691262452
learn step counter:  272401
dev_network_count:  909
learn step counter:  272451
dev_network_count:  909
learn step counter:  272501
dev_network_count:  909
learn step counter:  272551
dev_network_count:  909
learn step counter:  272601
dev_network_count:  909
learn step counter:  272651
dev_network_count:  909

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 7 3]]
Reward:  [-2.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '3', '3 3', '1', '3']
910  r_total and score:  190.60000000000014 47.64443691262452
Current Bleu score is:  47.64443691262452
learn step counter:  272701
dev_network_count:  910
learn step counter:  272751
dev_network_count:  910
learn step counter:  272801
dev_network_count:  910
learn step counter:  272851
dev_network_count:  910
learn step counter:  272901
dev_network_count:  910
learn step counter:  272951
dev_network_count:  910
EPOCH %d 276
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
575 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
576 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
577 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
578 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
579 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
580 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
581 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 7 3]]
Reward:  [-2.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '3', '3 3', '1', '3']
911  r_total and score:  190.60000000000014 47.64443691262452
Current Bleu score is:  47.64443691262452
learn step counter:  273001
dev_network_count:  911
learn step counter:  273051
dev_network_count:  911
learn step counter:  273101
dev_network_count:  911
learn step counter:  273151
dev_network_count:  911
learn step counter:  273201
dev_network_count:  911
learn step counter:  273251
dev_network_count:  911

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 7 3]]
Reward:  [-2.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '3', '3 3', '1', '3']
912  r_total and score:  190.60000000000014 47.64443691262452
Current Bleu score is:  47.64443691262452
learn step counter:  273301
dev_network_count:  912
learn step counter:  273351
dev_network_count:  912
learn step counter:  273401
dev_network_count:  912
learn step counter:  273451
dev_network_count:  912
learn step counter:  273501
dev_network_count:  912
learn step counter:  273551
dev_network_count:  912

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 7 3]]
Reward:  [-2.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '3', '3 3', '1', '3']
913  r_total and score:  190.60000000000014 47.64443691262452
Current Bleu score is:  47.64443691262452
learn step counter:  273601
dev_network_count:  913
learn step counter:  273651
dev_network_count:  913
learn step counter:  273701
dev_network_count:  913
learn step counter:  273751
dev_network_count:  913
learn step counter:  273801
dev_network_count:  913
learn step counter:  273851
dev_network_count:  913

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 7 3]]
Reward:  [-2.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '3', '3 3', '1', '3']
914  r_total and score:  190.60000000000014 47.64443691262452
Current Bleu score is:  47.64443691262452
learn step counter:  273901
dev_network_count:  914
learn step counter:  273951
dev_network_count:  914
EPOCH %d 277
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
3068 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
3069 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
3070 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
3071 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
3072 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
3073 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
3074 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  274001
dev_network_count:  914
learn step counter:  274051
dev_network_count:  914
learn step counter:  274101
dev_network_count:  914
learn step counter:  274151
dev_network_count:  914

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 7 3]]
Reward:  [-2.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '3', '3 3', '1', '3']
915  r_total and score:  190.60000000000014 47.64443691262452
Current Bleu score is:  47.64443691262452
learn step counter:  274201
dev_network_count:  915
learn step counter:  274251
dev_network_count:  915
learn step counter:  274301
dev_network_count:  915
learn step counter:  274351
dev_network_count:  915
learn step counter:  274401
dev_network_count:  915
learn step counter:  274451
dev_network_count:  915

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 7 3]]
Reward:  [-2.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '3', '3 3', '1', '3']
916  r_total and score:  190.60000000000014 47.64443691262452
Current Bleu score is:  47.64443691262452
learn step counter:  274501
dev_network_count:  916
learn step counter:  274551
dev_network_count:  916
learn step counter:  274601
dev_network_count:  916
learn step counter:  274651
dev_network_count:  916
learn step counter:  274701
dev_network_count:  916
learn step counter:  274751
dev_network_count:  916

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 7 3]]
Reward:  [-2.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '3', '3 3', '1', '3']
917  r_total and score:  190.60000000000014 47.64443691262452
Current Bleu score is:  47.64443691262452
learn step counter:  274801
dev_network_count:  917
learn step counter:  274851
dev_network_count:  917
learn step counter:  274901
dev_network_count:  917
learn step counter:  274951
dev_network_count:  917
EPOCH %d 278
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
561 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
562 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
563 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
564 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
565 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
566 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
567 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  275001
dev_network_count:  917
learn step counter:  275051
dev_network_count:  917

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 7 3]]
Reward:  [-2.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '3', '3 3', '1', '3']
918  r_total and score:  190.60000000000014 47.64443691262452
Current Bleu score is:  47.64443691262452
learn step counter:  275101
dev_network_count:  918
learn step counter:  275151
dev_network_count:  918
learn step counter:  275201
dev_network_count:  918
learn step counter:  275251
dev_network_count:  918
learn step counter:  275301
dev_network_count:  918
learn step counter:  275351
dev_network_count:  918

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 7 3]]
Reward:  [-2.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '3', '3 3', '1', '3']
919  r_total and score:  190.60000000000014 47.64443691262452
Current Bleu score is:  47.64443691262452
learn step counter:  275401
dev_network_count:  919
learn step counter:  275451
dev_network_count:  919
learn step counter:  275501
dev_network_count:  919
learn step counter:  275551
dev_network_count:  919
learn step counter:  275601
dev_network_count:  919
learn step counter:  275651
dev_network_count:  919

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 7 3]]
Reward:  [-2.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '3', '3 3', '1', '3']
920  r_total and score:  190.60000000000014 47.64443691262452
Current Bleu score is:  47.64443691262452
learn step counter:  275701
dev_network_count:  920
learn step counter:  275751
dev_network_count:  920
learn step counter:  275801
dev_network_count:  920
learn step counter:  275851
dev_network_count:  920
learn step counter:  275901
dev_network_count:  920
learn step counter:  275951
dev_network_count:  920
EPOCH %d 279
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
3054 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
3055 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
3056 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
3057 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
3058 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
3059 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
3060 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 7 3]]
Reward:  [-2.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '3', '3 3', '1', '3']
921  r_total and score:  190.60000000000014 47.64443691262452
Current Bleu score is:  47.64443691262452
learn step counter:  276001
dev_network_count:  921
learn step counter:  276051
dev_network_count:  921
learn step counter:  276101
dev_network_count:  921
learn step counter:  276151
dev_network_count:  921
learn step counter:  276201
dev_network_count:  921
learn step counter:  276251
dev_network_count:  921

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 7 3]]
Reward:  [-2.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '3', '3 3', '1', '3']
922  r_total and score:  190.60000000000014 47.64443691262452
Current Bleu score is:  47.64443691262452
learn step counter:  276301
dev_network_count:  922
learn step counter:  276351
dev_network_count:  922
learn step counter:  276401
dev_network_count:  922
learn step counter:  276451
dev_network_count:  922
learn step counter:  276501
dev_network_count:  922
learn step counter:  276551
dev_network_count:  922

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 7 3]]
Reward:  [-2.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '3', '3 3', '1', '3']
923  r_total and score:  190.60000000000014 47.64443691262452
Current Bleu score is:  47.64443691262452
learn step counter:  276601
dev_network_count:  923
learn step counter:  276651
dev_network_count:  923
learn step counter:  276701
dev_network_count:  923
learn step counter:  276751
dev_network_count:  923
learn step counter:  276801
dev_network_count:  923
learn step counter:  276851
dev_network_count:  923

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([4])]  the state[:3] is:  tensor([-0.5009,  0.9886,  0.3796])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5]
Eval  :  [[2 4 3]]
Reward:  [ 1.  -4.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5]
Eval  :  [[2 3]]
Reward:  [-2.] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 7 3]]
Reward:  [-2.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 7 7 3]]
Reward:  [-2.  -0.4  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5]
Eval  :  [[2 4 3]]
Reward:  [-2.  -0.4] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['3', '2', '1', '4 3', '2', '', '3', '3 3', '1', '3']
924  r_total and score:  190.60000000000014 47.64443691262452
Current Bleu score is:  47.64443691262452
learn step counter:  276901
dev_network_count:  924
learn step counter:  276951
dev_network_count:  924
EPOCH %d 280
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
547 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
548 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
549 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
550 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
551 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
552 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
553 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  277001
dev_network_count:  924
learn step counter:  277051
dev_network_count:  924
learn step counter:  277101
dev_network_count:  924
learn step counter:  277151
dev_network_count:  924

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9274,  0.6028, -0.6498])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----