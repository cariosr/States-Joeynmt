Sample size:  256
State size:  24
Action size:  9
bleu_seq
You select the reward based on the sequence accuaracy bleu_seq
EPOCH %d 1
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.9 0.9

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 7 3]]
Reward:  [1.  0.8] 

0 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

2 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
3 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 4 5 3]]
Reward:  [1.  0.8 0.6] 

4 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
5 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
6 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 3]
Eval  :  [[2 4 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 

EPOCH %d 2
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.81 0.81
2493 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
2494 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
2495 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
2496 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
2497 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
2498 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
2499 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
EPOCH %d 3
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.7290000000000001 0.7290000000000001
4986 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4987 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4988 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4989 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4990 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4991 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4992 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
As referece this first test on dev data. Is maded with the Q networks, initialized randomly : 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])
So far:  [array([2]), array([6]), array([4])]  the state[:3] is:  tensor([-0.9571,  0.7688, -0.8984])
So far:  [array([2]), array([6]), array([4]), array([4])]  the state[:3] is:  tensor([-0.9536,  0.8128, -0.9270])
So far:  [array([2]), array([6]), array([4]), array([4]), array([4])]  the state[:3] is:  tensor([-0.9513,  0.8209, -0.9319])
So far:  [array([2]), array([6]), array([4]), array([4]), array([4]), array([4])]  the state[:3] is:  tensor([-0.9492,  0.8241, -0.9335])
So far:  [array([2]), array([6]), array([4]), array([4]), array([4]), array([4]), array([4])]  the state[:3] is:  tensor([-0.9478,  0.8256, -0.9341])
So far:  [array([2]), array([6]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4])]  the state[:3] is:  tensor([-0.9470,  0.8264, -0.9344])
So far:  [array([2]), array([6]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4])]  the state[:3] is:  tensor([-0.9465,  0.8269, -0.9345])
So far:  [array([2]), array([6]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4])]  the state[:3] is:  tensor([-0.9461,  0.8271, -0.9346])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  3 10 10 10 10 10 10 10 10]
Eval  :  [[2 6 4 4 4 4 4 4 4 4 4]]
Reward:  [-1.  -0.2 -0.4 -0.6 -0.8 -1.  -1.2 -1.4 -1.6 -1.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9761,  0.9061, -0.9193])
So far:  [array([2]), array([6]), array([4])]  the state[:3] is:  tensor([-0.9925,  0.9367, -0.9825])
So far:  [array([2]), array([6]), array([4]), array([4])]  the state[:3] is:  tensor([-0.9903,  0.9516, -0.9884])
So far:  [array([2]), array([6]), array([4]), array([4]), array([4])]  the state[:3] is:  tensor([-0.9894,  0.9543, -0.9892])
So far:  [array([2]), array([6]), array([4]), array([4]), array([4]), array([4])]  the state[:3] is:  tensor([-0.9887,  0.9556, -0.9894])
So far:  [array([2]), array([6]), array([4]), array([4]), array([4]), array([4]), array([4])]  the state[:3] is:  tensor([-0.9883,  0.9562, -0.9895])
So far:  [array([2]), array([6]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4])]  the state[:3] is:  tensor([-0.9880,  0.9565, -0.9895])
So far:  [array([2]), array([6]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4])]  the state[:3] is:  tensor([-0.9879,  0.9567, -0.9895])
So far:  [array([2]), array([6]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4])]  the state[:3] is:  tensor([-0.9878,  0.9567, -0.9895])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  8  3 10 10 10 10 10 10 10 10]
Eval  :  [[2 6 4 4 4 4 4 4 4 4 4]]
Reward:  [-1.  -0.2 -0.4 -0.6 -0.8 -1.  -1.2 -1.4 -1.6 -1.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([6])]  the state[:3] is:  tensor([-0.8803,  0.9928, -0.5555])
So far:  [array([2]), array([6]), array([6]), array([4])]  the state[:3] is:  tensor([-0.9799,  0.9927, -0.8951])
So far:  [array([2]), array([6]), array([6]), array([4]), array([4])]  the state[:3] is:  tensor([-0.9840,  0.9933, -0.9345])
So far:  [array([2]), array([6]), array([6]), array([4]), array([4]), array([4])]  the state[:3] is:  tensor([-0.9849,  0.9934, -0.9418])
So far:  [array([2]), array([6]), array([6]), array([4]), array([4]), array([4]), array([4])]  the state[:3] is:  tensor([-0.9849,  0.9934, -0.9440])
So far:  [array([2]), array([6]), array([6]), array([4]), array([4]), array([4]), array([4]), array([4])]  the state[:3] is:  tensor([-0.9847,  0.9934, -0.9448])
So far:  [array([2]), array([6]), array([6]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4])]  the state[:3] is:  tensor([-0.9846,  0.9934, -0.9451])
So far:  [array([2]), array([6]), array([6]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4]), array([4])]  the state[:3] is:  tensor([-0.9845,  0.9934, -0.9453])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  4  5  3 10 10 10 10 10 10 10]
Eval  :  [[2 6 6 4 4 4 4 4 4 4 4]]
Reward:  [-1.  -0.2 -0.4 -0.6 -0.8 -1.  -1.2 -1.4 -1.6 -1.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10 10 10 10 10 10 10]
Eval  :  [[2 6 6 4 4 4 4 4 4 4 4]]
Reward:  [-1.   2.8 -3.4 -0.6 -0.8 -1.  -1.2 -1.4 -1.6 -1.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  8  3 10 10 10 10 10 10 10 10]
Eval  :  [[2 6 4 4 4 4 4 4 4 4 4]]
Reward:  [-1.  -0.2 -0.4 -0.6 -0.8 -1.  -1.2 -1.4 -1.6 -1.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  5  3 10 10 10 10 10 10 10 10]
Eval  :  [[2 4 4 4 4 4 4 4 4 4 4]]
Reward:  [-1.  -0.2 -0.4 -0.6 -0.8 -1.  -1.2 -1.4 -1.6 -1.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  6  3 10 10 10 10 10 10 10 10]
Eval  :  [[2 4 4 4 4 4 4 4 4 4 4]]
Reward:  [-1.  -0.2 -0.4 -0.6 -0.8 -1.  -1.2 -1.4 -1.6 -1.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  4  6  3 10 10 10 10 10 10 10]
Eval  :  [[2 6 6 4 4 4 4 4 4 4 4]]
Reward:  [-1.   2.8 -3.4 -0.6 -0.8 -1.  -1.2 -1.4 -1.6 -1.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  5  3 10 10 10 10 10 10 10]
Eval  :  [[2 6 4 4 4 4 4 4 4 4 4]]
Reward:  [-1.  -0.2 -0.4 -0.6 -0.8 -1.  -1.2 -1.4 -1.6 -1.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  3 10 10 10 10 10 10 10 10]
Eval  :  [[2 6 4 4 4 4 4 4 4 4 4]]
Reward:  [-1.  -0.2 -0.4 -0.6 -0.8 -1.  -1.2 -1.4 -1.6 -1.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4 1 1 1 1 1 1 1 1 1', '4 1 1 1 1 1 1 1 1 1', '4 4 1 1 1 1 1 1 1 1', '4 4 1 1 1 1 1 1 1 1', '4 1 1 1 1 1 1 1 1 1', '1 1 1 1 1 1 1 1 1 1', '1 1 1 1 1 1 1 1 1 1', '4 4 1 1 1 1 1 1 1 1', '4 1 1 1 1 1 1 1 1 1', '4 1 1 1 1 1 1 1 1 1']
1  r_total and score:  52.999999999999986 0.0
Current Bleu score is:  0.0
learn step counter:  1
dev_network_count:  1
Using pretraining...
learn step counter:  51
dev_network_count:  1
learn step counter:  101
dev_network_count:  1
learn step counter:  151
dev_network_count:  1
learn step counter:  201
dev_network_count:  1
learn step counter:  251
dev_network_count:  1

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([5])]  the state[:3] is:  tensor([-0.9854,  0.8979, -0.9274])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 5 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([5])]  the state[:3] is:  tensor([-0.5256,  0.9823,  0.5156])
So far:  [array([2]), array([5]), array([5])]  the state[:3] is:  tensor([-0.9701,  0.9892, -0.6949])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 5 5 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 5 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 5 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 5 5 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '0', '0 0', '4 4', '0', '0', '4', '4 4', '0 0', '4']
2  r_total and score:  304.40000000000026 29.888128883203066
Current Bleu score is:  29.888128883203066
learn step counter:  301
dev_network_count:  2
learn step counter:  351
dev_network_count:  2
learn step counter:  401
dev_network_count:  2
learn step counter:  451
dev_network_count:  2
learn step counter:  501
dev_network_count:  2
learn step counter:  551
dev_network_count:  2

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([4])]  the state[:3] is:  tensor([-0.9231,  0.9931, -0.6332])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 4 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 5 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 4 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 1', '4 4', '2', '0', '4', '4 4', '4 1', '4']
3  r_total and score:  299.4000000000003 29.70074064795726
Current Bleu score is:  29.70074064795726
learn step counter:  601
dev_network_count:  3
learn step counter:  651
dev_network_count:  3
learn step counter:  701
dev_network_count:  3
learn step counter:  751
dev_network_count:  3
learn step counter:  801
dev_network_count:  3
learn step counter:  851
dev_network_count:  3

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([4])]  the state[:3] is:  tensor([-0.9231,  0.9931, -0.6332])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 4 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 5 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 4 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 1', '4 4', '2', '0', '4', '4 4', '4 1', '4']
4  r_total and score:  292.80000000000024 34.51183851258305
Current Bleu score is:  34.51183851258305
learn step counter:  901
dev_network_count:  4
learn step counter:  951
dev_network_count:  4
EPOCH %d 4
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.6561 0.6561
2479 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
2480 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
2481 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
2482 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
Starting using Q target net....
2483 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
2484 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
2485 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  1001
dev_network_count:  4
learn step counter:  1051
dev_network_count:  4
learn step counter:  1101
dev_network_count:  4
learn step counter:  1151
dev_network_count:  4

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([7])]  the state[:3] is:  tensor([-0.9821,  0.9067, -0.9258])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 7 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([4])]  the state[:3] is:  tensor([-0.9231,  0.9931, -0.6332])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 4 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 7 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 5 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 4 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '3', '4 1', '4 4', '3', '0', '4', '4 4', '4 1', '4']
5  r_total and score:  314.8000000000004 23.576074005412455
Current Bleu score is:  23.576074005412455
learn step counter:  1201
dev_network_count:  5
learn step counter:  1251
dev_network_count:  5
learn step counter:  1301
dev_network_count:  5
learn step counter:  1351
dev_network_count:  5
learn step counter:  1401
dev_network_count:  5
learn step counter:  1451
dev_network_count:  5

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([4])]  the state[:3] is:  tensor([-0.9231,  0.9931, -0.6332])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 4 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 5 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 4 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 1', '4 4', '2', '0', '4', '4 4', '4 1', '4']
6  r_total and score:  290.0000000000003 38.42429959033012
Current Bleu score is:  38.42429959033012
learn step counter:  1501
dev_network_count:  6
learn step counter:  1551
dev_network_count:  6
learn step counter:  1601
dev_network_count:  6
learn step counter:  1651
dev_network_count:  6
learn step counter:  1701
dev_network_count:  6
learn step counter:  1751
dev_network_count:  6

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([4])]  the state[:3] is:  tensor([-0.9231,  0.9931, -0.6332])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 4 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 5 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 4 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 1', '4 4', '2', '0', '4', '4 4', '4 1', '4']
7  r_total and score:  290.80000000000024 37.74013272189177
Current Bleu score is:  37.74013272189177
learn step counter:  1801
dev_network_count:  7
learn step counter:  1851
dev_network_count:  7
learn step counter:  1901
dev_network_count:  7
learn step counter:  1951
dev_network_count:  7
EPOCH %d 5
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.5904900000000001 0.5904900000000001
4972 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4973 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4974 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4975 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4976 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4977 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4978 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  2001
dev_network_count:  7
learn step counter:  2051
dev_network_count:  7

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([4])]  the state[:3] is:  tensor([-0.9231,  0.9931, -0.6332])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 4 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 5 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 4 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 1', '4 4 3', '2', '0', '4', '4 4', '4 1', '4']
8  r_total and score:  288.4000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  2101
dev_network_count:  8
learn step counter:  2151
dev_network_count:  8
learn step counter:  2201
dev_network_count:  8
learn step counter:  2251
dev_network_count:  8
learn step counter:  2301
dev_network_count:  8
learn step counter:  2351
dev_network_count:  8

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([4])]  the state[:3] is:  tensor([-0.9231,  0.9931, -0.6332])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 4 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 5 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  4  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 4 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 1', '4 4 3', '2', '0', '4', '4 4 3', '4 1', '4']
9  r_total and score:  280.8000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  2401
dev_network_count:  9
learn step counter:  2451
dev_network_count:  9
learn step counter:  2501
dev_network_count:  9
learn step counter:  2551
dev_network_count:  9
learn step counter:  2601
dev_network_count:  9
learn step counter:  2651
dev_network_count:  9

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([4])]  the state[:3] is:  tensor([-0.9231,  0.9931, -0.6332])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 4 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 5 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  4  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 4 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 1', '4 4 3', '2', '0', '4', '4 4 3', '4 1', '4']
10  r_total and score:  281.40000000000026 0.0
Current Bleu score is:  0.0
learn step counter:  2701
dev_network_count:  10
learn step counter:  2751
dev_network_count:  10
learn step counter:  2801
dev_network_count:  10
learn step counter:  2851
dev_network_count:  10
learn step counter:  2901
dev_network_count:  10
learn step counter:  2951
dev_network_count:  10
EPOCH %d 6
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.531441 0.531441
2465 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
2466 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
2467 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
2468 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
2469 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
2470 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
2471 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([4])]  the state[:3] is:  tensor([-0.9231,  0.9931, -0.6332])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 4 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 5 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 1', '4 4 3', '2', '0', '4', '4 4', '4 3', '4']
11  r_total and score:  291.40000000000026 0.0
Current Bleu score is:  0.0
learn step counter:  3001
dev_network_count:  11
learn step counter:  3051
dev_network_count:  11
learn step counter:  3101
dev_network_count:  11
learn step counter:  3151
dev_network_count:  11
learn step counter:  3201
dev_network_count:  11
learn step counter:  3251
dev_network_count:  11

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([4])]  the state[:3] is:  tensor([-0.9231,  0.9931, -0.6332])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 4 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  4  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 1', '4 4 3', '2', '2', '4', '4 4 3', '4 3', '4']
12  r_total and score:  293.4000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  3301
dev_network_count:  12
learn step counter:  3351
dev_network_count:  12
learn step counter:  3401
dev_network_count:  12
learn step counter:  3451
dev_network_count:  12
learn step counter:  3501
dev_network_count:  12
learn step counter:  3551
dev_network_count:  12

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([4])]  the state[:3] is:  tensor([-0.9231,  0.9931, -0.6332])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 4 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 5 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  4  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 1', '4 4 3', '2', '0', '4', '4 4 3', '4 3', '4']
13  r_total and score:  282.40000000000026 0.0
Current Bleu score is:  0.0
learn step counter:  3601
dev_network_count:  13
learn step counter:  3651
dev_network_count:  13
learn step counter:  3701
dev_network_count:  13
learn step counter:  3751
dev_network_count:  13
learn step counter:  3801
dev_network_count:  13
learn step counter:  3851
dev_network_count:  13

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([4])]  the state[:3] is:  tensor([-0.9231,  0.9931, -0.6332])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 4 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 5 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  4  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 1', '4 4 3', '2', '0', '4', '4 4 3', '4 3', '4']
14  r_total and score:  282.40000000000026 0.0
Current Bleu score is:  0.0
learn step counter:  3901
dev_network_count:  14
learn step counter:  3951
dev_network_count:  14
EPOCH %d 7
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.4782969000000001 0.4782969000000001
4958 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4959 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4960 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4961 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4962 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4963 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4964 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  4001
dev_network_count:  14
learn step counter:  4051
dev_network_count:  14
learn step counter:  4101
dev_network_count:  14
learn step counter:  4151
dev_network_count:  14

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([4])]  the state[:3] is:  tensor([-0.9231,  0.9931, -0.6332])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 4 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  4  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 1', '4 4 3', '2', '2', '4', '4 4 3', '4 3', '4']
15  r_total and score:  281.4000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  4201
dev_network_count:  15
learn step counter:  4251
dev_network_count:  15
learn step counter:  4301
dev_network_count:  15
learn step counter:  4351
dev_network_count:  15
learn step counter:  4401
dev_network_count:  15
learn step counter:  4451
dev_network_count:  15

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([4])]  the state[:3] is:  tensor([-0.9231,  0.9931, -0.6332])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 4 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  4  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 1', '4 4 3', '2', '2', '4', '4 4 3', '4 3', '4']
16  r_total and score:  281.4000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  4501
dev_network_count:  16
learn step counter:  4551
dev_network_count:  16
learn step counter:  4601
dev_network_count:  16
learn step counter:  4651
dev_network_count:  16
learn step counter:  4701
dev_network_count:  16
learn step counter:  4751
dev_network_count:  16

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([4])]  the state[:3] is:  tensor([-0.9231,  0.9931, -0.6332])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 4 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  4  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 1', '4 4 3', '2', '2', '4', '4 4 3', '4 3', '4']
17  r_total and score:  281.4000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  4801
dev_network_count:  17
learn step counter:  4851
dev_network_count:  17
learn step counter:  4901
dev_network_count:  17
learn step counter:  4951
dev_network_count:  17
EPOCH %d 8
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.4304672100000001 0.4304672100000001
2451 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
2452 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
2453 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
2454 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
2455 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
2456 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
2457 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  5001
dev_network_count:  17
learn step counter:  5051
dev_network_count:  17

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([7])]  the state[:3] is:  tensor([-0.9036,  0.9936, -0.5937])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  4  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 3', '4 4 3', '2', '2', '4', '4 4 3', '4 3', '4']
18  r_total and score:  281.2000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  5101
dev_network_count:  18
learn step counter:  5151
dev_network_count:  18
learn step counter:  5201
dev_network_count:  18
learn step counter:  5251
dev_network_count:  18
learn step counter:  5301
dev_network_count:  18
learn step counter:  5351
dev_network_count:  18

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([7])]  the state[:3] is:  tensor([-0.9036,  0.9936, -0.5937])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  4  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 3', '4 4 3', '2', '2', '4', '4 4 3', '4 3', '4']
19  r_total and score:  281.4000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  5401
dev_network_count:  19
learn step counter:  5451
dev_network_count:  19
learn step counter:  5501
dev_network_count:  19
learn step counter:  5551
dev_network_count:  19
learn step counter:  5601
dev_network_count:  19
learn step counter:  5651
dev_network_count:  19

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([7])]  the state[:3] is:  tensor([-0.9036,  0.9936, -0.5937])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  4  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 3', '4 4 3', '2', '2', '4', '4 4 3', '4 3', '4']
20  r_total and score:  281.4000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  5701
dev_network_count:  20
learn step counter:  5751
dev_network_count:  20
learn step counter:  5801
dev_network_count:  20
learn step counter:  5851
dev_network_count:  20
learn step counter:  5901
dev_network_count:  20
learn step counter:  5951
dev_network_count:  20
EPOCH %d 9
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.3874204890000001 0.3874204890000001
4944 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4945 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4946 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4947 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4948 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4949 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4950 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([7])]  the state[:3] is:  tensor([-0.9036,  0.9936, -0.5937])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 3', '4 4 3', '2', '2', '4', '4 4', '4 3', '4']
21  r_total and score:  291.6000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  6001
dev_network_count:  21
learn step counter:  6051
dev_network_count:  21
learn step counter:  6101
dev_network_count:  21
learn step counter:  6151
dev_network_count:  21
learn step counter:  6201
dev_network_count:  21
learn step counter:  6251
dev_network_count:  21

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([7])]  the state[:3] is:  tensor([-0.9036,  0.9936, -0.5937])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  4  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 3', '4 4 3', '2', '2', '4', '4 4 3', '4 3', '4']
22  r_total and score:  281.2000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  6301
dev_network_count:  22
learn step counter:  6351
dev_network_count:  22
learn step counter:  6401
dev_network_count:  22
learn step counter:  6451
dev_network_count:  22
learn step counter:  6501
dev_network_count:  22
learn step counter:  6551
dev_network_count:  22

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([7])]  the state[:3] is:  tensor([-0.9036,  0.9936, -0.5937])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  4  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 3', '4 4 3', '2', '2', '4', '4 4 3', '4 3', '4']
23  r_total and score:  281.2000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  6601
dev_network_count:  23
learn step counter:  6651
dev_network_count:  23
learn step counter:  6701
dev_network_count:  23
learn step counter:  6751
dev_network_count:  23
learn step counter:  6801
dev_network_count:  23
learn step counter:  6851
dev_network_count:  23

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([7])]  the state[:3] is:  tensor([-0.9036,  0.9936, -0.5937])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  4  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 3', '4 4 3', '2', '2', '4', '4 4 3', '4 3', '4']
24  r_total and score:  281.2000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  6901
dev_network_count:  24
learn step counter:  6951
dev_network_count:  24
EPOCH %d 10
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.3486784401000001 0.3486784401000001
2437 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
2438 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
2439 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
2440 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
2441 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
2442 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
2443 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  7001
dev_network_count:  24
learn step counter:  7051
dev_network_count:  24
learn step counter:  7101
dev_network_count:  24
learn step counter:  7151
dev_network_count:  24

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([7])]  the state[:3] is:  tensor([-0.9036,  0.9936, -0.5937])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  4  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 3', '4 4 3', '2', '2', '4', '4 4 3', '4 3', '4']
25  r_total and score:  279.2000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  7201
dev_network_count:  25
learn step counter:  7251
dev_network_count:  25
learn step counter:  7301
dev_network_count:  25
learn step counter:  7351
dev_network_count:  25
learn step counter:  7401
dev_network_count:  25
learn step counter:  7451
dev_network_count:  25

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([7])]  the state[:3] is:  tensor([-0.9036,  0.9936, -0.5937])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  4  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 3', '4 4 3', '2', '2', '4', '4 4 3', '4 3', '4']
26  r_total and score:  279.2000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  7501
dev_network_count:  26
learn step counter:  7551
dev_network_count:  26
learn step counter:  7601
dev_network_count:  26
learn step counter:  7651
dev_network_count:  26
learn step counter:  7701
dev_network_count:  26
learn step counter:  7751
dev_network_count:  26

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([7])]  the state[:3] is:  tensor([-0.9036,  0.9936, -0.5937])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  4  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 3', '4 4 3', '2', '2', '4', '4 4 3', '4 3', '4']
27  r_total and score:  279.2000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  7801
dev_network_count:  27
learn step counter:  7851
dev_network_count:  27
learn step counter:  7901
dev_network_count:  27
learn step counter:  7951
dev_network_count:  27
EPOCH %d 11
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.31381059609000006 0.31381059609000006
4930 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4931 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4932 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4933 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4934 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4935 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4936 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  8001
dev_network_count:  27
learn step counter:  8051
dev_network_count:  27

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([7])]  the state[:3] is:  tensor([-0.9036,  0.9936, -0.5937])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 3', '4 4 3', '2', '2', '4', '4 4', '4 3', '4']
28  r_total and score:  289.4000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  8101
dev_network_count:  28
learn step counter:  8151
dev_network_count:  28
learn step counter:  8201
dev_network_count:  28
learn step counter:  8251
dev_network_count:  28
learn step counter:  8301
dev_network_count:  28
learn step counter:  8351
dev_network_count:  28

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([7])]  the state[:3] is:  tensor([-0.9036,  0.9936, -0.5937])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 3', '4 4 3', '2', '2', '4', '4 4', '4 3', '4']
29  r_total and score:  291.20000000000033 0.0
Current Bleu score is:  0.0
learn step counter:  8401
dev_network_count:  29
learn step counter:  8451
dev_network_count:  29
learn step counter:  8501
dev_network_count:  29
learn step counter:  8551
dev_network_count:  29
learn step counter:  8601
dev_network_count:  29
learn step counter:  8651
dev_network_count:  29

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([7])]  the state[:3] is:  tensor([-0.9036,  0.9936, -0.5937])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 3', '4 4 3', '2', '2', '4', '4 4', '4 3', '4']
30  r_total and score:  291.20000000000033 0.0
Current Bleu score is:  0.0
learn step counter:  8701
dev_network_count:  30
learn step counter:  8751
dev_network_count:  30
learn step counter:  8801
dev_network_count:  30
learn step counter:  8851
dev_network_count:  30
learn step counter:  8901
dev_network_count:  30
learn step counter:  8951
dev_network_count:  30
EPOCH %d 12
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.2824295364810001 0.3
2423 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
2424 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
2425 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
2426 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
2427 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
2428 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
2429 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([7])]  the state[:3] is:  tensor([-0.9036,  0.9936, -0.5937])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 3', '4 4 3', '2', '2', '4', '4 4', '4 3', '4']
31  r_total and score:  291.20000000000033 0.0
Current Bleu score is:  0.0
learn step counter:  9001
dev_network_count:  31
learn step counter:  9051
dev_network_count:  31
learn step counter:  9101
dev_network_count:  31
learn step counter:  9151
dev_network_count:  31
learn step counter:  9201
dev_network_count:  31
learn step counter:  9251
dev_network_count:  31

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([7])]  the state[:3] is:  tensor([-0.9036,  0.9936, -0.5937])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 3', '4 4 3', '2', '2', '4', '4 4', '4 3', '4']
32  r_total and score:  291.20000000000033 0.0
Current Bleu score is:  0.0
learn step counter:  9301
dev_network_count:  32
learn step counter:  9351
dev_network_count:  32
learn step counter:  9401
dev_network_count:  32
learn step counter:  9451
dev_network_count:  32
learn step counter:  9501
dev_network_count:  32
learn step counter:  9551
dev_network_count:  32

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([7])]  the state[:3] is:  tensor([-0.9036,  0.9936, -0.5937])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 3', '4 4 3', '2', '2', '4', '4 4', '4 3', '4']
33  r_total and score:  291.20000000000033 0.0
Current Bleu score is:  0.0
learn step counter:  9601
dev_network_count:  33
learn step counter:  9651
dev_network_count:  33
learn step counter:  9701
dev_network_count:  33
learn step counter:  9751
dev_network_count:  33
learn step counter:  9801
dev_network_count:  33
learn step counter:  9851
dev_network_count:  33

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([7])]  the state[:3] is:  tensor([-0.9036,  0.9936, -0.5937])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 3', '4 4 3', '2', '2', '4', '4 4', '4 3', '4']
34  r_total and score:  291.20000000000033 0.0
Current Bleu score is:  0.0
learn step counter:  9901
dev_network_count:  34
learn step counter:  9951
dev_network_count:  34
EPOCH %d 13
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.2541865828329001 0.3
4916 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4917 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4918 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4919 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4920 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4921 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4922 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  10001
dev_network_count:  34
learn step counter:  10051
dev_network_count:  34
learn step counter:  10101
dev_network_count:  34
learn step counter:  10151
dev_network_count:  34

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([7])]  the state[:3] is:  tensor([-0.9036,  0.9936, -0.5937])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 3', '4 4 3', '2', '2', '4', '4 4', '4 3', '4']
35  r_total and score:  291.20000000000033 0.0
Current Bleu score is:  0.0
learn step counter:  10201
dev_network_count:  35
learn step counter:  10251
dev_network_count:  35
learn step counter:  10301
dev_network_count:  35
learn step counter:  10351
dev_network_count:  35
learn step counter:  10401
dev_network_count:  35
learn step counter:  10451
dev_network_count:  35

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([7])]  the state[:3] is:  tensor([-0.9036,  0.9936, -0.5937])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 3', '4 4 3', '2', '2', '4', '4 4', '4 3', '4']
36  r_total and score:  291.20000000000033 0.0
Current Bleu score is:  0.0
learn step counter:  10501
dev_network_count:  36
learn step counter:  10551
dev_network_count:  36
learn step counter:  10601
dev_network_count:  36
learn step counter:  10651
dev_network_count:  36
learn step counter:  10701
dev_network_count:  36
learn step counter:  10751
dev_network_count:  36

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([7])]  the state[:3] is:  tensor([-0.9036,  0.9936, -0.5937])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 3', '4 4 3', '2', '2', '4', '4 4', '4 3', '4']
37  r_total and score:  291.20000000000033 0.0
Current Bleu score is:  0.0
learn step counter:  10801
dev_network_count:  37
learn step counter:  10851
dev_network_count:  37
learn step counter:  10901
dev_network_count:  37
learn step counter:  10951
dev_network_count:  37
EPOCH %d 14
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.2287679245496101 0.3
2409 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
2410 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
2411 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
2412 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
2413 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
2414 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
2415 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  11001
dev_network_count:  37
learn step counter:  11051
dev_network_count:  37

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([7])]  the state[:3] is:  tensor([-0.9036,  0.9936, -0.5937])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 3', '4 4 3', '2', '2', '4', '4 4', '4 3', '4']
38  r_total and score:  291.20000000000033 0.0
Current Bleu score is:  0.0
learn step counter:  11101
dev_network_count:  38
learn step counter:  11151
dev_network_count:  38
learn step counter:  11201
dev_network_count:  38
learn step counter:  11251
dev_network_count:  38
learn step counter:  11301
dev_network_count:  38
learn step counter:  11351
dev_network_count:  38

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([7])]  the state[:3] is:  tensor([-0.9036,  0.9936, -0.5937])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 3', '4 4 3', '2', '2', '4', '4 4', '4 3', '4']
39  r_total and score:  291.20000000000033 0.0
Current Bleu score is:  0.0
learn step counter:  11401
dev_network_count:  39
learn step counter:  11451
dev_network_count:  39
learn step counter:  11501
dev_network_count:  39
learn step counter:  11551
dev_network_count:  39
learn step counter:  11601
dev_network_count:  39
learn step counter:  11651
dev_network_count:  39

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([7])]  the state[:3] is:  tensor([-0.9036,  0.9936, -0.5937])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 3', '4 4 3', '2', '2', '4', '4 4', '4 3', '4']
40  r_total and score:  291.20000000000033 0.0
Current Bleu score is:  0.0
learn step counter:  11701
dev_network_count:  40
learn step counter:  11751
dev_network_count:  40
learn step counter:  11801
dev_network_count:  40
learn step counter:  11851
dev_network_count:  40
learn step counter:  11901
dev_network_count:  40
learn step counter:  11951
dev_network_count:  40
EPOCH %d 15
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.20589113209464907 0.3
4902 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4903 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4904 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4905 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4906 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4907 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4908 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([7])]  the state[:3] is:  tensor([-0.9036,  0.9936, -0.5937])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 3', '4 4 3', '2', '2', '4', '4 4', '4 3', '4']
41  r_total and score:  291.20000000000033 0.0
Current Bleu score is:  0.0
learn step counter:  12001
dev_network_count:  41
learn step counter:  12051
dev_network_count:  41
learn step counter:  12101
dev_network_count:  41
learn step counter:  12151
dev_network_count:  41
learn step counter:  12201
dev_network_count:  41
learn step counter:  12251
dev_network_count:  41

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([7])]  the state[:3] is:  tensor([-0.9036,  0.9936, -0.5937])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 3', '4 4 3', '2', '2', '4', '4 4', '4 3', '4']
42  r_total and score:  291.20000000000033 0.0
Current Bleu score is:  0.0
learn step counter:  12301
dev_network_count:  42
learn step counter:  12351
dev_network_count:  42
learn step counter:  12401
dev_network_count:  42
learn step counter:  12451
dev_network_count:  42
learn step counter:  12501
dev_network_count:  42
learn step counter:  12551
dev_network_count:  42

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([7])]  the state[:3] is:  tensor([-0.9036,  0.9936, -0.5937])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 3', '4 4 3', '2', '2', '4', '4 4', '4 3', '4']
43  r_total and score:  291.20000000000033 0.0
Current Bleu score is:  0.0
learn step counter:  12601
dev_network_count:  43
learn step counter:  12651
dev_network_count:  43
learn step counter:  12701
dev_network_count:  43
learn step counter:  12751
dev_network_count:  43
learn step counter:  12801
dev_network_count:  43
learn step counter:  12851
dev_network_count:  43

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([7])]  the state[:3] is:  tensor([-0.9036,  0.9936, -0.5937])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 3', '4 4 3', '2', '2', '4', '4 4', '4 3', '4']
44  r_total and score:  291.20000000000033 0.0
Current Bleu score is:  0.0
learn step counter:  12901
dev_network_count:  44
learn step counter:  12951
dev_network_count:  44
EPOCH %d 16
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.18530201888518416 0.3
2395 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
2396 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
2397 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
2398 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
2399 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
2400 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
2401 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  13001
dev_network_count:  44
learn step counter:  13051
dev_network_count:  44
learn step counter:  13101
dev_network_count:  44
learn step counter:  13151
dev_network_count:  44

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([7])]  the state[:3] is:  tensor([-0.9036,  0.9936, -0.5937])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 3', '4 4 3', '2', '2', '4', '4 4', '4 3', '4']
45  r_total and score:  291.20000000000033 0.0
Current Bleu score is:  0.0
learn step counter:  13201
dev_network_count:  45
learn step counter:  13251
dev_network_count:  45
learn step counter:  13301
dev_network_count:  45
learn step counter:  13351
dev_network_count:  45
learn step counter:  13401
dev_network_count:  45
learn step counter:  13451
dev_network_count:  45

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([7])]  the state[:3] is:  tensor([-0.9036,  0.9936, -0.5937])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 3', '4 4 3', '2', '2', '4', '4 4', '4 3', '4']
46  r_total and score:  291.20000000000033 0.0
Current Bleu score is:  0.0
learn step counter:  13501
dev_network_count:  46
learn step counter:  13551
dev_network_count:  46
learn step counter:  13601
dev_network_count:  46
learn step counter:  13651
dev_network_count:  46
learn step counter:  13701
dev_network_count:  46
learn step counter:  13751
dev_network_count:  46

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([7])]  the state[:3] is:  tensor([-0.9036,  0.9936, -0.5937])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 3', '4 4 3', '2', '2', '4', '4 4', '4 3', '4']
47  r_total and score:  291.20000000000033 0.0
Current Bleu score is:  0.0
learn step counter:  13801
dev_network_count:  47
learn step counter:  13851
dev_network_count:  47
learn step counter:  13901
dev_network_count:  47
learn step counter:  13951
dev_network_count:  47
EPOCH %d 17
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.16677181699666577 0.3
4888 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4889 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4890 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4891 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4892 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4893 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4894 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  14001
dev_network_count:  47
learn step counter:  14051
dev_network_count:  47

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([7])]  the state[:3] is:  tensor([-0.9036,  0.9936, -0.5937])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 3', '4 4 3', '2', '2', '4', '4 4', '4 3', '4']
48  r_total and score:  291.20000000000033 0.0
Current Bleu score is:  0.0
learn step counter:  14101
dev_network_count:  48
learn step counter:  14151
dev_network_count:  48
learn step counter:  14201
dev_network_count:  48
learn step counter:  14251
dev_network_count:  48
learn step counter:  14301
dev_network_count:  48
learn step counter:  14351
dev_network_count:  48

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([7])]  the state[:3] is:  tensor([-0.9036,  0.9936, -0.5937])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 3', '4 4 3', '2', '2', '4', '4 4', '4 3', '4']
49  r_total and score:  291.20000000000033 0.0
Current Bleu score is:  0.0
learn step counter:  14401
dev_network_count:  49
learn step counter:  14451
dev_network_count:  49
learn step counter:  14501
dev_network_count:  49
learn step counter:  14551
dev_network_count:  49
learn step counter:  14601
dev_network_count:  49
learn step counter:  14651
dev_network_count:  49

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([7])]  the state[:3] is:  tensor([-0.9036,  0.9936, -0.5937])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 3', '4 4 3', '2', '2', '4', '4 4', '4 3', '4']
50  r_total and score:  291.20000000000033 0.0
Current Bleu score is:  0.0
learn step counter:  14701
dev_network_count:  50
learn step counter:  14751
dev_network_count:  50
learn step counter:  14801
dev_network_count:  50
learn step counter:  14851
dev_network_count:  50
learn step counter:  14901
dev_network_count:  50
learn step counter:  14951
dev_network_count:  50
EPOCH %d 18
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.15009463529699918 0.3
2381 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
2382 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
2383 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
2384 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
2385 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
2386 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
2387 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([7])]  the state[:3] is:  tensor([-0.9036,  0.9936, -0.5937])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 3', '4 4 3', '2', '2', '4', '4 4', '4 3', '4']
51  r_total and score:  291.20000000000033 0.0
Current Bleu score is:  0.0
learn step counter:  15001
dev_network_count:  51
learn step counter:  15051
dev_network_count:  51
learn step counter:  15101
dev_network_count:  51
learn step counter:  15151
dev_network_count:  51
learn step counter:  15201
dev_network_count:  51
learn step counter:  15251
dev_network_count:  51

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([7])]  the state[:3] is:  tensor([-0.9036,  0.9936, -0.5937])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 3', '4 4 3', '2', '2', '4', '4 4', '4 3', '4']
52  r_total and score:  291.20000000000033 0.0
Current Bleu score is:  0.0
learn step counter:  15301
dev_network_count:  52
learn step counter:  15351
dev_network_count:  52
learn step counter:  15401
dev_network_count:  52
learn step counter:  15451
dev_network_count:  52
learn step counter:  15501
dev_network_count:  52
learn step counter:  15551
dev_network_count:  52

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([7])]  the state[:3] is:  tensor([-0.9036,  0.9936, -0.5937])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 3', '4 4 3', '2', '2', '4', '4 4', '4 3', '4']
53  r_total and score:  291.20000000000033 0.0
Current Bleu score is:  0.0
learn step counter:  15601
dev_network_count:  53
learn step counter:  15651
dev_network_count:  53
learn step counter:  15701
dev_network_count:  53
learn step counter:  15751
dev_network_count:  53
learn step counter:  15801
dev_network_count:  53
learn step counter:  15851
dev_network_count:  53

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([7])]  the state[:3] is:  tensor([-0.9036,  0.9936, -0.5937])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 3', '4 4 3', '2', '2', '4', '4 4', '4 3', '4']
54  r_total and score:  291.20000000000033 0.0
Current Bleu score is:  0.0
learn step counter:  15901
dev_network_count:  54
learn step counter:  15951
dev_network_count:  54
EPOCH %d 19
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.13508517176729928 0.3
4874 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4875 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4876 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4877 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4878 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4879 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4880 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  16001
dev_network_count:  54
learn step counter:  16051
dev_network_count:  54
learn step counter:  16101
dev_network_count:  54
learn step counter:  16151
dev_network_count:  54

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([7])]  the state[:3] is:  tensor([-0.9036,  0.9936, -0.5937])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 3', '4 4 3', '2', '2', '4', '4 4', '4 3', '4']
55  r_total and score:  291.20000000000033 0.0
Current Bleu score is:  0.0
learn step counter:  16201
dev_network_count:  55
learn step counter:  16251
dev_network_count:  55
learn step counter:  16301
dev_network_count:  55
learn step counter:  16351
dev_network_count:  55
learn step counter:  16401
dev_network_count:  55
learn step counter:  16451
dev_network_count:  55

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([7])]  the state[:3] is:  tensor([-0.9036,  0.9936, -0.5937])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 3', '4 4 3', '2', '2', '4', '4 4', '4 3', '4']
56  r_total and score:  291.20000000000033 0.0
Current Bleu score is:  0.0
learn step counter:  16501
dev_network_count:  56
learn step counter:  16551
dev_network_count:  56
learn step counter:  16601
dev_network_count:  56
learn step counter:  16651
dev_network_count:  56
learn step counter:  16701
dev_network_count:  56
learn step counter:  16751
dev_network_count:  56

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([7])]  the state[:3] is:  tensor([-0.9036,  0.9936, -0.5937])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 3', '4 4 3', '2', '2', '4', '4 4', '4 3', '4']
57  r_total and score:  291.20000000000033 0.0
Current Bleu score is:  0.0
learn step counter:  16801
dev_network_count:  57
learn step counter:  16851
dev_network_count:  57
learn step counter:  16901
dev_network_count:  57
learn step counter:  16951
dev_network_count:  57
EPOCH %d 20
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.12157665459056935 0.3
2367 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
2368 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
2369 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
2370 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
2371 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
2372 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
2373 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  17001
dev_network_count:  57
learn step counter:  17051
dev_network_count:  57

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([7])]  the state[:3] is:  tensor([-0.9036,  0.9936, -0.5937])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 3', '4 4 3', '2', '2', '4', '4 4', '4 3', '4']
58  r_total and score:  291.20000000000033 0.0
Current Bleu score is:  0.0
learn step counter:  17101
dev_network_count:  58
learn step counter:  17151
dev_network_count:  58
learn step counter:  17201
dev_network_count:  58
learn step counter:  17251
dev_network_count:  58
learn step counter:  17301
dev_network_count:  58
learn step counter:  17351
dev_network_count:  58

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([7])]  the state[:3] is:  tensor([-0.9036,  0.9936, -0.5937])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 3', '4 4 3', '2', '2', '4', '4 4', '4 3', '4']
59  r_total and score:  291.20000000000033 0.0
Current Bleu score is:  0.0
learn step counter:  17401
dev_network_count:  59
learn step counter:  17451
dev_network_count:  59
learn step counter:  17501
dev_network_count:  59
learn step counter:  17551
dev_network_count:  59
learn step counter:  17601
dev_network_count:  59
learn step counter:  17651
dev_network_count:  59

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([7])]  the state[:3] is:  tensor([-0.9036,  0.9936, -0.5937])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 3', '4 4 3', '2', '2', '4', '4 4', '4 3', '4']
60  r_total and score:  291.20000000000033 0.0
Current Bleu score is:  0.0
learn step counter:  17701
dev_network_count:  60
learn step counter:  17751
dev_network_count:  60
learn step counter:  17801
dev_network_count:  60
learn step counter:  17851
dev_network_count:  60
learn step counter:  17901
dev_network_count:  60
learn step counter:  17951
dev_network_count:  60
EPOCH %d 21
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.10941898913151242 0.3
4860 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4861 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4862 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4863 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4864 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4865 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4866 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([7])]  the state[:3] is:  tensor([-0.9036,  0.9936, -0.5937])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 3', '4 4 3', '2', '2', '4', '4 4', '4 3', '4']
61  r_total and score:  291.20000000000033 0.0
Current Bleu score is:  0.0
learn step counter:  18001
dev_network_count:  61
learn step counter:  18051
dev_network_count:  61
learn step counter:  18101
dev_network_count:  61
learn step counter:  18151
dev_network_count:  61
learn step counter:  18201
dev_network_count:  61
learn step counter:  18251
dev_network_count:  61

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([7])]  the state[:3] is:  tensor([-0.9036,  0.9936, -0.5937])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 3', '4 4 3', '2', '2', '4', '4 4', '4 3', '4']
62  r_total and score:  291.20000000000033 0.0
Current Bleu score is:  0.0
learn step counter:  18301
dev_network_count:  62
learn step counter:  18351
dev_network_count:  62
learn step counter:  18401
dev_network_count:  62
learn step counter:  18451
dev_network_count:  62
learn step counter:  18501
dev_network_count:  62
learn step counter:  18551
dev_network_count:  62

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([7])]  the state[:3] is:  tensor([-0.9036,  0.9936, -0.5937])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 3', '4 4 3', '2', '2', '4', '4 4', '4 3', '4']
63  r_total and score:  291.20000000000033 0.0
Current Bleu score is:  0.0
learn step counter:  18601
dev_network_count:  63
learn step counter:  18651
dev_network_count:  63
learn step counter:  18701
dev_network_count:  63
learn step counter:  18751
dev_network_count:  63
learn step counter:  18801
dev_network_count:  63
learn step counter:  18851
dev_network_count:  63

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([7])]  the state[:3] is:  tensor([-0.9036,  0.9936, -0.5937])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 3', '4 4 3', '2', '2', '4', '4 4', '4 3', '4']
64  r_total and score:  291.20000000000033 0.0
Current Bleu score is:  0.0
learn step counter:  18901
dev_network_count:  64
learn step counter:  18951
dev_network_count:  64
EPOCH %d 22
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.09847709021836118 0.3
2353 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
2354 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
2355 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
2356 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
2357 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
2358 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
2359 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  19001
dev_network_count:  64
learn step counter:  19051
dev_network_count:  64
learn step counter:  19101
dev_network_count:  64
learn step counter:  19151
dev_network_count:  64

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([7])]  the state[:3] is:  tensor([-0.9036,  0.9936, -0.5937])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 3', '4 4 3', '2', '2', '4', '4 4', '4 3', '4']
65  r_total and score:  291.20000000000033 0.0
Current Bleu score is:  0.0
learn step counter:  19201
dev_network_count:  65
learn step counter:  19251
dev_network_count:  65
learn step counter:  19301
dev_network_count:  65
learn step counter:  19351
dev_network_count:  65
learn step counter:  19401
dev_network_count:  65
learn step counter:  19451
dev_network_count:  65

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([7])]  the state[:3] is:  tensor([-0.9036,  0.9936, -0.5937])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 3', '4 4 3', '2', '2', '4', '4 4', '4 3', '4']
66  r_total and score:  291.20000000000033 0.0
Current Bleu score is:  0.0
learn step counter:  19501
dev_network_count:  66
learn step counter:  19551
dev_network_count:  66
learn step counter:  19601
dev_network_count:  66
learn step counter:  19651
dev_network_count:  66
learn step counter:  19701
dev_network_count:  66
learn step counter:  19751
dev_network_count:  66

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([7])]  the state[:3] is:  tensor([-0.9036,  0.9936, -0.5937])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 3', '4 4 3', '2', '2', '4', '4 4', '4 3', '4']
67  r_total and score:  291.20000000000033 0.0
Current Bleu score is:  0.0
learn step counter:  19801
dev_network_count:  67
learn step counter:  19851
dev_network_count:  67
learn step counter:  19901
dev_network_count:  67
learn step counter:  19951
dev_network_count:  67
EPOCH %d 23
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.08862938119652507 0.3
4846 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4847 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4848 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4849 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4850 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4851 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4852 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  20001
dev_network_count:  67
learn step counter:  20051
dev_network_count:  67

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([7])]  the state[:3] is:  tensor([-0.9036,  0.9936, -0.5937])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 3', '4 4 3', '2', '2', '4', '4 4', '4 3', '4']
68  r_total and score:  291.20000000000033 0.0
Current Bleu score is:  0.0
learn step counter:  20101
dev_network_count:  68
learn step counter:  20151
dev_network_count:  68
learn step counter:  20201
dev_network_count:  68
learn step counter:  20251
dev_network_count:  68
learn step counter:  20301
dev_network_count:  68
learn step counter:  20351
dev_network_count:  68

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([7])]  the state[:3] is:  tensor([-0.9036,  0.9936, -0.5937])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 3', '4 4 3', '2', '2', '4', '4 4', '4 3', '4']
69  r_total and score:  291.20000000000033 0.0
Current Bleu score is:  0.0
learn step counter:  20401
dev_network_count:  69
learn step counter:  20451
dev_network_count:  69
learn step counter:  20501
dev_network_count:  69
learn step counter:  20551
dev_network_count:  69
learn step counter:  20601
dev_network_count:  69
learn step counter:  20651
dev_network_count:  69

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([7])]  the state[:3] is:  tensor([-0.9036,  0.9936, -0.5937])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 3', '4 4 3', '2', '2', '4', '4 4', '4 3', '4']
70  r_total and score:  291.20000000000033 0.0
Current Bleu score is:  0.0
learn step counter:  20701
dev_network_count:  70
learn step counter:  20751
dev_network_count:  70
learn step counter:  20801
dev_network_count:  70
learn step counter:  20851
dev_network_count:  70
learn step counter:  20901
dev_network_count:  70
learn step counter:  20951
dev_network_count:  70
EPOCH %d 24
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.07976644307687256 0.3
2339 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
2340 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
2341 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
2342 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
2343 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
2344 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
2345 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4 3', '2', '2', '4', '4 4', '4 3', '4']
71  r_total and score:  291.20000000000033 0.0
Current Bleu score is:  0.0
learn step counter:  21001
dev_network_count:  71
learn step counter:  21051
dev_network_count:  71
learn step counter:  21101
dev_network_count:  71
learn step counter:  21151
dev_network_count:  71
learn step counter:  21201
dev_network_count:  71
learn step counter:  21251
dev_network_count:  71

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4 3', '2', '2', '4', '4 4', '4 3', '4']
72  r_total and score:  291.20000000000033 0.0
Current Bleu score is:  0.0
learn step counter:  21301
dev_network_count:  72
learn step counter:  21351
dev_network_count:  72
learn step counter:  21401
dev_network_count:  72
learn step counter:  21451
dev_network_count:  72
learn step counter:  21501
dev_network_count:  72
learn step counter:  21551
dev_network_count:  72

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4 3', '2', '2', '4', '4 4', '4 3', '4']
73  r_total and score:  291.20000000000033 0.0
Current Bleu score is:  0.0
learn step counter:  21601
dev_network_count:  73
learn step counter:  21651
dev_network_count:  73
learn step counter:  21701
dev_network_count:  73
learn step counter:  21751
dev_network_count:  73
learn step counter:  21801
dev_network_count:  73
learn step counter:  21851
dev_network_count:  73

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([7])]  the state[:3] is:  tensor([-0.9036,  0.9936, -0.5937])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 3', '4 4 3', '2', '2', '4', '4 4', '4 3', '4']
74  r_total and score:  291.20000000000033 0.0
Current Bleu score is:  0.0
learn step counter:  21901
dev_network_count:  74
learn step counter:  21951
dev_network_count:  74
EPOCH %d 25
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.0717897987691853 0.3
4832 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4833 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4834 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4835 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4836 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4837 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4838 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  22001
dev_network_count:  74
learn step counter:  22051
dev_network_count:  74
learn step counter:  22101
dev_network_count:  74
learn step counter:  22151
dev_network_count:  74

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4 3', '2', '2', '4', '4 4', '4 2', '4']
75  r_total and score:  291.20000000000033 0.0
Current Bleu score is:  0.0
learn step counter:  22201
dev_network_count:  75
learn step counter:  22251
dev_network_count:  75
learn step counter:  22301
dev_network_count:  75
learn step counter:  22351
dev_network_count:  75
learn step counter:  22401
dev_network_count:  75
learn step counter:  22451
dev_network_count:  75

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4 3', '2', '2', '4', '4 4', '4 3', '4']
76  r_total and score:  291.20000000000033 0.0
Current Bleu score is:  0.0
learn step counter:  22501
dev_network_count:  76
learn step counter:  22551
dev_network_count:  76
learn step counter:  22601
dev_network_count:  76
learn step counter:  22651
dev_network_count:  76
learn step counter:  22701
dev_network_count:  76
learn step counter:  22751
dev_network_count:  76

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4 3', '2', '2', '4', '4 4', '4 3', '4']
77  r_total and score:  291.20000000000033 0.0
Current Bleu score is:  0.0
learn step counter:  22801
dev_network_count:  77
learn step counter:  22851
dev_network_count:  77
learn step counter:  22901
dev_network_count:  77
learn step counter:  22951
dev_network_count:  77
EPOCH %d 26
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.06461081889226677 0.3
2325 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
2326 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
2327 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
2328 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
2329 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
2330 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
2331 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  23001
dev_network_count:  77
learn step counter:  23051
dev_network_count:  77

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4 3', '2', '2', '4', '4 4', '4 3', '4']
78  r_total and score:  291.20000000000033 0.0
Current Bleu score is:  0.0
learn step counter:  23101
dev_network_count:  78
learn step counter:  23151
dev_network_count:  78
learn step counter:  23201
dev_network_count:  78
learn step counter:  23251
dev_network_count:  78
learn step counter:  23301
dev_network_count:  78
learn step counter:  23351
dev_network_count:  78

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 7 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4 3', '2', '2', '4', '4 4', '4 3', '4']
79  r_total and score:  291.20000000000033 0.0
Current Bleu score is:  0.0
learn step counter:  23401
dev_network_count:  79
learn step counter:  23451
dev_network_count:  79
learn step counter:  23501
dev_network_count:  79
learn step counter:  23551
dev_network_count:  79
learn step counter:  23601
dev_network_count:  79
learn step counter:  23651
dev_network_count:  79

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4 3', '2', '2', '4', '4 4', '4 2', '4']
80  r_total and score:  291.20000000000033 0.0
Current Bleu score is:  0.0
learn step counter:  23701
dev_network_count:  80
learn step counter:  23751
dev_network_count:  80
learn step counter:  23801
dev_network_count:  80
learn step counter:  23851
dev_network_count:  80
learn step counter:  23901
dev_network_count:  80
learn step counter:  23951
dev_network_count:  80
EPOCH %d 27
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.058149737003040096 0.3
4818 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4819 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4820 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4821 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4822 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4823 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4824 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4 3', '2', '2', '4', '4 4', '4 2', '4']
81  r_total and score:  291.20000000000033 0.0
Current Bleu score is:  0.0
learn step counter:  24001
dev_network_count:  81
learn step counter:  24051
dev_network_count:  81
learn step counter:  24101
dev_network_count:  81
learn step counter:  24151
dev_network_count:  81
learn step counter:  24201
dev_network_count:  81
learn step counter:  24251
dev_network_count:  81

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4 3', '2', '2', '4', '4 4', '4 2', '4']
82  r_total and score:  286.8000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  24301
dev_network_count:  82
learn step counter:  24351
dev_network_count:  82
learn step counter:  24401
dev_network_count:  82
learn step counter:  24451
dev_network_count:  82
learn step counter:  24501
dev_network_count:  82
learn step counter:  24551
dev_network_count:  82

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4 3', '2', '2', '4', '4 4', '4 2', '4']
83  r_total and score:  286.8000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  24601
dev_network_count:  83
learn step counter:  24651
dev_network_count:  83
learn step counter:  24701
dev_network_count:  83
learn step counter:  24751
dev_network_count:  83
learn step counter:  24801
dev_network_count:  83
learn step counter:  24851
dev_network_count:  83

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4 3', '2', '2', '4', '4 4', '4 2', '4']
84  r_total and score:  286.8000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  24901
dev_network_count:  84
learn step counter:  24951
dev_network_count:  84
EPOCH %d 28
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.05233476330273609 0.3
2311 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
2312 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
2313 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
2314 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
2315 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
2316 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
2317 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  25001
dev_network_count:  84
learn step counter:  25051
dev_network_count:  84
learn step counter:  25101
dev_network_count:  84
learn step counter:  25151
dev_network_count:  84

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4 3', '2', '2', '4', '4 4', '4 2', '4']
85  r_total and score:  286.8000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  25201
dev_network_count:  85
learn step counter:  25251
dev_network_count:  85
learn step counter:  25301
dev_network_count:  85
learn step counter:  25351
dev_network_count:  85
learn step counter:  25401
dev_network_count:  85
learn step counter:  25451
dev_network_count:  85

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4 3', '2', '2', '4', '4 4', '4 2', '4']
86  r_total and score:  286.8000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  25501
dev_network_count:  86
learn step counter:  25551
dev_network_count:  86
learn step counter:  25601
dev_network_count:  86
learn step counter:  25651
dev_network_count:  86
learn step counter:  25701
dev_network_count:  86
learn step counter:  25751
dev_network_count:  86

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4 3', '2', '2', '4', '4 4', '4 2', '4']
87  r_total and score:  286.8000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  25801
dev_network_count:  87
learn step counter:  25851
dev_network_count:  87
learn step counter:  25901
dev_network_count:  87
learn step counter:  25951
dev_network_count:  87
EPOCH %d 29
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.047101286972462485 0.3
4804 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4805 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4806 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4807 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4808 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4809 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4810 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  26001
dev_network_count:  87
learn step counter:  26051
dev_network_count:  87

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4 3', '2', '2', '4', '4 4', '4 2', '4']
88  r_total and score:  286.8000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  26101
dev_network_count:  88
learn step counter:  26151
dev_network_count:  88
learn step counter:  26201
dev_network_count:  88
learn step counter:  26251
dev_network_count:  88
learn step counter:  26301
dev_network_count:  88
learn step counter:  26351
dev_network_count:  88

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4 3', '2', '2', '4', '4 4', '4 2', '4']
89  r_total and score:  286.8000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  26401
dev_network_count:  89
learn step counter:  26451
dev_network_count:  89
learn step counter:  26501
dev_network_count:  89
learn step counter:  26551
dev_network_count:  89
learn step counter:  26601
dev_network_count:  89
learn step counter:  26651
dev_network_count:  89

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4 3', '2', '2', '4', '4 4', '4 2', '4']
90  r_total and score:  286.8000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  26701
dev_network_count:  90
learn step counter:  26751
dev_network_count:  90
learn step counter:  26801
dev_network_count:  90
learn step counter:  26851
dev_network_count:  90
learn step counter:  26901
dev_network_count:  90
learn step counter:  26951
dev_network_count:  90
EPOCH %d 30
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.04239115827521624 0.3
2297 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
2298 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
2299 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
2300 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
2301 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
2302 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
2303 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4 3', '2', '2', '4', '4 4', '4 2', '4']
91  r_total and score:  286.8000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  27001
dev_network_count:  91
learn step counter:  27051
dev_network_count:  91
learn step counter:  27101
dev_network_count:  91
learn step counter:  27151
dev_network_count:  91
learn step counter:  27201
dev_network_count:  91
learn step counter:  27251
dev_network_count:  91

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4 3', '2', '2', '4', '4 4', '4 2', '4']
92  r_total and score:  286.8000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  27301
dev_network_count:  92
learn step counter:  27351
dev_network_count:  92
learn step counter:  27401
dev_network_count:  92
learn step counter:  27451
dev_network_count:  92
learn step counter:  27501
dev_network_count:  92
learn step counter:  27551
dev_network_count:  92

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4 3', '2', '2', '4', '4 4', '4 2', '4']
93  r_total and score:  286.8000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  27601
dev_network_count:  93
learn step counter:  27651
dev_network_count:  93
learn step counter:  27701
dev_network_count:  93
learn step counter:  27751
dev_network_count:  93
learn step counter:  27801
dev_network_count:  93
learn step counter:  27851
dev_network_count:  93

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4 3', '2', '2', '4', '4 4', '4 2', '4']
94  r_total and score:  286.8000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  27901
dev_network_count:  94
learn step counter:  27951
dev_network_count:  94
EPOCH %d 31
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.038152042447694615 0.3
4790 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4791 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4792 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4793 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4794 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4795 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4796 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  28001
dev_network_count:  94
learn step counter:  28051
dev_network_count:  94
learn step counter:  28101
dev_network_count:  94
learn step counter:  28151
dev_network_count:  94

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4 3', '2', '2', '4', '4 4', '4 2', '4']
95  r_total and score:  286.8000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  28201
dev_network_count:  95
learn step counter:  28251
dev_network_count:  95
learn step counter:  28301
dev_network_count:  95
learn step counter:  28351
dev_network_count:  95
learn step counter:  28401
dev_network_count:  95
learn step counter:  28451
dev_network_count:  95

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4 3', '2', '2', '4', '4 4', '4 2', '4']
96  r_total and score:  286.8000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  28501
dev_network_count:  96
learn step counter:  28551
dev_network_count:  96
learn step counter:  28601
dev_network_count:  96
learn step counter:  28651
dev_network_count:  96
learn step counter:  28701
dev_network_count:  96
learn step counter:  28751
dev_network_count:  96

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4 3', '2', '2', '4', '4 4', '4 2', '4']
97  r_total and score:  286.8000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  28801
dev_network_count:  97
learn step counter:  28851
dev_network_count:  97
learn step counter:  28901
dev_network_count:  97
learn step counter:  28951
dev_network_count:  97
EPOCH %d 32
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.03433683820292515 0.3
2283 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
2284 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
2285 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
2286 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
2287 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
2288 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
2289 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  29001
dev_network_count:  97
learn step counter:  29051
dev_network_count:  97

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4 3', '2', '2', '4', '4 4', '4 2', '4']
98  r_total and score:  286.8000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  29101
dev_network_count:  98
learn step counter:  29151
dev_network_count:  98
learn step counter:  29201
dev_network_count:  98
learn step counter:  29251
dev_network_count:  98
learn step counter:  29301
dev_network_count:  98
learn step counter:  29351
dev_network_count:  98

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4 3', '2', '2', '4', '4 4', '4 2', '4']
99  r_total and score:  286.8000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  29401
dev_network_count:  99
learn step counter:  29451
dev_network_count:  99
learn step counter:  29501
dev_network_count:  99
learn step counter:  29551
dev_network_count:  99
learn step counter:  29601
dev_network_count:  99
learn step counter:  29651
dev_network_count:  99

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4 3', '2', '2', '4', '4 4', '4 2', '4']
100  r_total and score:  286.8000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  29701
dev_network_count:  100
learn step counter:  29751
dev_network_count:  100
learn step counter:  29801
dev_network_count:  100
learn step counter:  29851
dev_network_count:  100
learn step counter:  29901
dev_network_count:  100
learn step counter:  29951
dev_network_count:  100
EPOCH %d 33
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.030903154382632636 0.3
4776 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4777 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4778 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4779 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4780 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4781 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4782 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4 3', '2', '2', '4', '4 4', '4 2', '4']
101  r_total and score:  286.8000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  30001
dev_network_count:  101
learn step counter:  30051
dev_network_count:  101
learn step counter:  30101
dev_network_count:  101
learn step counter:  30151
dev_network_count:  101
learn step counter:  30201
dev_network_count:  101
learn step counter:  30251
dev_network_count:  101

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4 3', '2', '2', '4', '4 4', '4 2', '4']
102  r_total and score:  286.8000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  30301
dev_network_count:  102
learn step counter:  30351
dev_network_count:  102
learn step counter:  30401
dev_network_count:  102
learn step counter:  30451
dev_network_count:  102
learn step counter:  30501
dev_network_count:  102
learn step counter:  30551
dev_network_count:  102

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4 3', '2', '2', '4', '4 4', '4 2', '4']
103  r_total and score:  286.8000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  30601
dev_network_count:  103
learn step counter:  30651
dev_network_count:  103
learn step counter:  30701
dev_network_count:  103
learn step counter:  30751
dev_network_count:  103
learn step counter:  30801
dev_network_count:  103
learn step counter:  30851
dev_network_count:  103

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4 3', '2', '2', '4', '4 4', '4 2', '4']
104  r_total and score:  286.8000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  30901
dev_network_count:  104
learn step counter:  30951
dev_network_count:  104
EPOCH %d 34
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.027812838944369374 0.3
2269 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
2270 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
2271 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
2272 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
2273 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
2274 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
2275 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  31001
dev_network_count:  104
learn step counter:  31051
dev_network_count:  104
learn step counter:  31101
dev_network_count:  104
learn step counter:  31151
dev_network_count:  104

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4 3', '2', '2', '4', '4 4', '4 2', '4']
105  r_total and score:  286.8000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  31201
dev_network_count:  105
learn step counter:  31251
dev_network_count:  105
learn step counter:  31301
dev_network_count:  105
learn step counter:  31351
dev_network_count:  105
learn step counter:  31401
dev_network_count:  105
learn step counter:  31451
dev_network_count:  105

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4 3', '2', '2', '4', '4 4', '4 2', '4']
106  r_total and score:  286.8000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  31501
dev_network_count:  106
learn step counter:  31551
dev_network_count:  106
learn step counter:  31601
dev_network_count:  106
learn step counter:  31651
dev_network_count:  106
learn step counter:  31701
dev_network_count:  106
learn step counter:  31751
dev_network_count:  106

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4 3', '2', '2', '4', '4 4', '4 2', '4']
107  r_total and score:  286.8000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  31801
dev_network_count:  107
learn step counter:  31851
dev_network_count:  107
learn step counter:  31901
dev_network_count:  107
learn step counter:  31951
dev_network_count:  107
EPOCH %d 35
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.025031555049932437 0.3
4762 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4763 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4764 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4765 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4766 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4767 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4768 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  32001
dev_network_count:  107
learn step counter:  32051
dev_network_count:  107

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4 3', '2', '2', '4', '4 4', '4 2', '4']
108  r_total and score:  286.8000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  32101
dev_network_count:  108
learn step counter:  32151
dev_network_count:  108
learn step counter:  32201
dev_network_count:  108
learn step counter:  32251
dev_network_count:  108
learn step counter:  32301
dev_network_count:  108
learn step counter:  32351
dev_network_count:  108

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4 3', '2', '2', '4', '4 4', '4 2', '4']
109  r_total and score:  286.8000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  32401
dev_network_count:  109
learn step counter:  32451
dev_network_count:  109
learn step counter:  32501
dev_network_count:  109
learn step counter:  32551
dev_network_count:  109
learn step counter:  32601
dev_network_count:  109
learn step counter:  32651
dev_network_count:  109

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4 3', '2', '2', '4', '4 4', '4 2', '4']
110  r_total and score:  286.8000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  32701
dev_network_count:  110
learn step counter:  32751
dev_network_count:  110
learn step counter:  32801
dev_network_count:  110
learn step counter:  32851
dev_network_count:  110
learn step counter:  32901
dev_network_count:  110
learn step counter:  32951
dev_network_count:  110
EPOCH %d 36
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.022528399544939195 0.3
2255 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
2256 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
2257 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
2258 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
2259 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
2260 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
2261 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4 3', '2', '2', '4', '4 4', '4 2', '4']
111  r_total and score:  286.8000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  33001
dev_network_count:  111
learn step counter:  33051
dev_network_count:  111
learn step counter:  33101
dev_network_count:  111
learn step counter:  33151
dev_network_count:  111
learn step counter:  33201
dev_network_count:  111
learn step counter:  33251
dev_network_count:  111

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4 3', '2', '2', '4', '4 4', '4 2', '4']
112  r_total and score:  286.8000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  33301
dev_network_count:  112
learn step counter:  33351
dev_network_count:  112
learn step counter:  33401
dev_network_count:  112
learn step counter:  33451
dev_network_count:  112
learn step counter:  33501
dev_network_count:  112
learn step counter:  33551
dev_network_count:  112

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4 3', '2', '2', '4', '4 4', '4 2', '4']
113  r_total and score:  286.8000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  33601
dev_network_count:  113
learn step counter:  33651
dev_network_count:  113
learn step counter:  33701
dev_network_count:  113
learn step counter:  33751
dev_network_count:  113
learn step counter:  33801
dev_network_count:  113
learn step counter:  33851
dev_network_count:  113

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4 3', '2', '2', '4', '4 4', '4 2', '4']
114  r_total and score:  286.8000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  33901
dev_network_count:  114
learn step counter:  33951
dev_network_count:  114
EPOCH %d 37
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.020275559590445275 0.3
4748 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4749 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4750 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4751 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4752 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4753 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4754 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  34001
dev_network_count:  114
learn step counter:  34051
dev_network_count:  114
learn step counter:  34101
dev_network_count:  114
learn step counter:  34151
dev_network_count:  114

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4 3', '2', '2', '4', '4 4', '4 2', '4']
115  r_total and score:  286.8000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  34201
dev_network_count:  115
learn step counter:  34251
dev_network_count:  115
learn step counter:  34301
dev_network_count:  115
learn step counter:  34351
dev_network_count:  115
learn step counter:  34401
dev_network_count:  115
learn step counter:  34451
dev_network_count:  115

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4 3', '2', '2', '4', '4 4', '4 2', '4']
116  r_total and score:  286.8000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  34501
dev_network_count:  116
learn step counter:  34551
dev_network_count:  116
learn step counter:  34601
dev_network_count:  116
learn step counter:  34651
dev_network_count:  116
learn step counter:  34701
dev_network_count:  116
learn step counter:  34751
dev_network_count:  116

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
117  r_total and score:  289.2000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  34801
dev_network_count:  117
learn step counter:  34851
dev_network_count:  117
learn step counter:  34901
dev_network_count:  117
learn step counter:  34951
dev_network_count:  117
EPOCH %d 38
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.01824800363140075 0.3
2241 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
2242 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
2243 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
2244 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
2245 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
2246 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
2247 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  35001
dev_network_count:  117
learn step counter:  35051
dev_network_count:  117

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4 3', '2', '2', '4', '4 4', '4 2', '4']
118  r_total and score:  286.8000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  35101
dev_network_count:  118
learn step counter:  35151
dev_network_count:  118
learn step counter:  35201
dev_network_count:  118
learn step counter:  35251
dev_network_count:  118
learn step counter:  35301
dev_network_count:  118
learn step counter:  35351
dev_network_count:  118

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
119  r_total and score:  289.2000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  35401
dev_network_count:  119
learn step counter:  35451
dev_network_count:  119
learn step counter:  35501
dev_network_count:  119
learn step counter:  35551
dev_network_count:  119
learn step counter:  35601
dev_network_count:  119
learn step counter:  35651
dev_network_count:  119

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4 3', '2', '2', '4', '4 4', '4 2', '4']
120  r_total and score:  286.8000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  35701
dev_network_count:  120
learn step counter:  35751
dev_network_count:  120
learn step counter:  35801
dev_network_count:  120
learn step counter:  35851
dev_network_count:  120
learn step counter:  35901
dev_network_count:  120
learn step counter:  35951
dev_network_count:  120
EPOCH %d 39
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.016423203268260675 0.3
4734 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4735 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4736 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4737 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4738 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4739 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4740 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4 3', '2', '2', '4', '4 4', '4 2', '4']
121  r_total and score:  286.8000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  36001
dev_network_count:  121
learn step counter:  36051
dev_network_count:  121
learn step counter:  36101
dev_network_count:  121
learn step counter:  36151
dev_network_count:  121
learn step counter:  36201
dev_network_count:  121
learn step counter:  36251
dev_network_count:  121

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
122  r_total and score:  289.2000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  36301
dev_network_count:  122
learn step counter:  36351
dev_network_count:  122
learn step counter:  36401
dev_network_count:  122
learn step counter:  36451
dev_network_count:  122
learn step counter:  36501
dev_network_count:  122
learn step counter:  36551
dev_network_count:  122

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
123  r_total and score:  289.2000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  36601
dev_network_count:  123
learn step counter:  36651
dev_network_count:  123
learn step counter:  36701
dev_network_count:  123
learn step counter:  36751
dev_network_count:  123
learn step counter:  36801
dev_network_count:  123
learn step counter:  36851
dev_network_count:  123

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
124  r_total and score:  289.2000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  36901
dev_network_count:  124
learn step counter:  36951
dev_network_count:  124
EPOCH %d 40
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.014780882941434608 0.3
2227 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
2228 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
2229 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
2230 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
2231 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
2232 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
2233 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  37001
dev_network_count:  124
learn step counter:  37051
dev_network_count:  124
learn step counter:  37101
dev_network_count:  124
learn step counter:  37151
dev_network_count:  124

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [ 2  7  6  3 10]
Eval  :  [[2 6 6 7 3]]
Reward:  [-1.   2.8 -3.4 -0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4 3', '2', '2', '4', '4 4', '4 2', '4']
125  r_total and score:  286.8000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  37201
dev_network_count:  125
learn step counter:  37251
dev_network_count:  125
learn step counter:  37301
dev_network_count:  125
learn step counter:  37351
dev_network_count:  125
learn step counter:  37401
dev_network_count:  125
learn step counter:  37451
dev_network_count:  125

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
126  r_total and score:  289.2000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  37501
dev_network_count:  126
learn step counter:  37551
dev_network_count:  126
learn step counter:  37601
dev_network_count:  126
learn step counter:  37651
dev_network_count:  126
learn step counter:  37701
dev_network_count:  126
learn step counter:  37751
dev_network_count:  126

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
127  r_total and score:  289.2000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  37801
dev_network_count:  127
learn step counter:  37851
dev_network_count:  127
learn step counter:  37901
dev_network_count:  127
learn step counter:  37951
dev_network_count:  127
EPOCH %d 41
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.013302794647291146 0.3
4720 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4721 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4722 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4723 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4724 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4725 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4726 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  38001
dev_network_count:  127
learn step counter:  38051
dev_network_count:  127

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
128  r_total and score:  289.2000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  38101
dev_network_count:  128
learn step counter:  38151
dev_network_count:  128
learn step counter:  38201
dev_network_count:  128
learn step counter:  38251
dev_network_count:  128
learn step counter:  38301
dev_network_count:  128
learn step counter:  38351
dev_network_count:  128

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
129  r_total and score:  289.2000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  38401
dev_network_count:  129
learn step counter:  38451
dev_network_count:  129
learn step counter:  38501
dev_network_count:  129
learn step counter:  38551
dev_network_count:  129
learn step counter:  38601
dev_network_count:  129
learn step counter:  38651
dev_network_count:  129

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
130  r_total and score:  289.2000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  38701
dev_network_count:  130
learn step counter:  38751
dev_network_count:  130
learn step counter:  38801
dev_network_count:  130
learn step counter:  38851
dev_network_count:  130
learn step counter:  38901
dev_network_count:  130
learn step counter:  38951
dev_network_count:  130
EPOCH %d 42
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.011972515182562033 0.3
2213 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
2214 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
2215 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
2216 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
2217 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
2218 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
2219 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
131  r_total and score:  289.2000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  39001
dev_network_count:  131
learn step counter:  39051
dev_network_count:  131
learn step counter:  39101
dev_network_count:  131
learn step counter:  39151
dev_network_count:  131
learn step counter:  39201
dev_network_count:  131
learn step counter:  39251
dev_network_count:  131

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
132  r_total and score:  292.8000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  39301
dev_network_count:  132
learn step counter:  39351
dev_network_count:  132
learn step counter:  39401
dev_network_count:  132
learn step counter:  39451
dev_network_count:  132
learn step counter:  39501
dev_network_count:  132
learn step counter:  39551
dev_network_count:  132

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
133  r_total and score:  292.8000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  39601
dev_network_count:  133
learn step counter:  39651
dev_network_count:  133
learn step counter:  39701
dev_network_count:  133
learn step counter:  39751
dev_network_count:  133
learn step counter:  39801
dev_network_count:  133
learn step counter:  39851
dev_network_count:  133

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
134  r_total and score:  292.8000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  39901
dev_network_count:  134
learn step counter:  39951
dev_network_count:  134
EPOCH %d 43
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.01077526366430583 0.3
4706 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4707 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4708 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4709 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4710 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4711 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4712 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  40001
dev_network_count:  134
learn step counter:  40051
dev_network_count:  134
learn step counter:  40101
dev_network_count:  134
learn step counter:  40151
dev_network_count:  134

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
135  r_total and score:  292.8000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  40201
dev_network_count:  135
learn step counter:  40251
dev_network_count:  135
learn step counter:  40301
dev_network_count:  135
learn step counter:  40351
dev_network_count:  135
learn step counter:  40401
dev_network_count:  135
learn step counter:  40451
dev_network_count:  135

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
136  r_total and score:  292.8000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  40501
dev_network_count:  136
learn step counter:  40551
dev_network_count:  136
learn step counter:  40601
dev_network_count:  136
learn step counter:  40651
dev_network_count:  136
learn step counter:  40701
dev_network_count:  136
learn step counter:  40751
dev_network_count:  136

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
137  r_total and score:  292.8000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  40801
dev_network_count:  137
learn step counter:  40851
dev_network_count:  137
learn step counter:  40901
dev_network_count:  137
learn step counter:  40951
dev_network_count:  137
EPOCH %d 44
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.009697737297875247 0.3
2199 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
2200 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
2201 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
2202 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
2203 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
2204 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
2205 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  41001
dev_network_count:  137
learn step counter:  41051
dev_network_count:  137

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
138  r_total and score:  292.8000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  41101
dev_network_count:  138
learn step counter:  41151
dev_network_count:  138
learn step counter:  41201
dev_network_count:  138
learn step counter:  41251
dev_network_count:  138
learn step counter:  41301
dev_network_count:  138
learn step counter:  41351
dev_network_count:  138

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
139  r_total and score:  292.8000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  41401
dev_network_count:  139
learn step counter:  41451
dev_network_count:  139
learn step counter:  41501
dev_network_count:  139
learn step counter:  41551
dev_network_count:  139
learn step counter:  41601
dev_network_count:  139
learn step counter:  41651
dev_network_count:  139

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
140  r_total and score:  292.8000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  41701
dev_network_count:  140
learn step counter:  41751
dev_network_count:  140
learn step counter:  41801
dev_network_count:  140
learn step counter:  41851
dev_network_count:  140
learn step counter:  41901
dev_network_count:  140
learn step counter:  41951
dev_network_count:  140
EPOCH %d 45
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.008727963568087723 0.3
4692 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4693 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4694 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4695 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4696 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4697 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4698 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
141  r_total and score:  292.8000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  42001
dev_network_count:  141
learn step counter:  42051
dev_network_count:  141
learn step counter:  42101
dev_network_count:  141
learn step counter:  42151
dev_network_count:  141
learn step counter:  42201
dev_network_count:  141
learn step counter:  42251
dev_network_count:  141

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
142  r_total and score:  292.8000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  42301
dev_network_count:  142
learn step counter:  42351
dev_network_count:  142
learn step counter:  42401
dev_network_count:  142
learn step counter:  42451
dev_network_count:  142
learn step counter:  42501
dev_network_count:  142
learn step counter:  42551
dev_network_count:  142

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
143  r_total and score:  292.8000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  42601
dev_network_count:  143
learn step counter:  42651
dev_network_count:  143
learn step counter:  42701
dev_network_count:  143
learn step counter:  42751
dev_network_count:  143
learn step counter:  42801
dev_network_count:  143
learn step counter:  42851
dev_network_count:  143

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
144  r_total and score:  292.8000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  42901
dev_network_count:  144
learn step counter:  42951
dev_network_count:  144
EPOCH %d 46
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.00785516721127895 0.3
2185 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
2186 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
2187 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
2188 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
2189 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
2190 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
2191 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  43001
dev_network_count:  144
learn step counter:  43051
dev_network_count:  144
learn step counter:  43101
dev_network_count:  144
learn step counter:  43151
dev_network_count:  144

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
145  r_total and score:  292.8000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  43201
dev_network_count:  145
learn step counter:  43251
dev_network_count:  145
learn step counter:  43301
dev_network_count:  145
learn step counter:  43351
dev_network_count:  145
learn step counter:  43401
dev_network_count:  145
learn step counter:  43451
dev_network_count:  145

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
146  r_total and score:  292.8000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  43501
dev_network_count:  146
learn step counter:  43551
dev_network_count:  146
learn step counter:  43601
dev_network_count:  146
learn step counter:  43651
dev_network_count:  146
learn step counter:  43701
dev_network_count:  146
learn step counter:  43751
dev_network_count:  146

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
147  r_total and score:  292.8000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  43801
dev_network_count:  147
learn step counter:  43851
dev_network_count:  147
learn step counter:  43901
dev_network_count:  147
learn step counter:  43951
dev_network_count:  147
EPOCH %d 47
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.007069650490151055 0.3
4678 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4679 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4680 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4681 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4682 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4683 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4684 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  44001
dev_network_count:  147
learn step counter:  44051
dev_network_count:  147

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
148  r_total and score:  292.8000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  44101
dev_network_count:  148
learn step counter:  44151
dev_network_count:  148
learn step counter:  44201
dev_network_count:  148
learn step counter:  44251
dev_network_count:  148
learn step counter:  44301
dev_network_count:  148
learn step counter:  44351
dev_network_count:  148

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
149  r_total and score:  292.8000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  44401
dev_network_count:  149
learn step counter:  44451
dev_network_count:  149
learn step counter:  44501
dev_network_count:  149
learn step counter:  44551
dev_network_count:  149
learn step counter:  44601
dev_network_count:  149
learn step counter:  44651
dev_network_count:  149

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
150  r_total and score:  292.8000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  44701
dev_network_count:  150
learn step counter:  44751
dev_network_count:  150
learn step counter:  44801
dev_network_count:  150
learn step counter:  44851
dev_network_count:  150
learn step counter:  44901
dev_network_count:  150
learn step counter:  44951
dev_network_count:  150
EPOCH %d 48
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.00636268544113595 0.3
2171 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
2172 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
2173 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
2174 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
2175 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
2176 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
2177 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
151  r_total and score:  292.8000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  45001
dev_network_count:  151
learn step counter:  45051
dev_network_count:  151
learn step counter:  45101
dev_network_count:  151
learn step counter:  45151
dev_network_count:  151
learn step counter:  45201
dev_network_count:  151
learn step counter:  45251
dev_network_count:  151

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
152  r_total and score:  292.8000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  45301
dev_network_count:  152
learn step counter:  45351
dev_network_count:  152
learn step counter:  45401
dev_network_count:  152
learn step counter:  45451
dev_network_count:  152
learn step counter:  45501
dev_network_count:  152
learn step counter:  45551
dev_network_count:  152

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
153  r_total and score:  292.8000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  45601
dev_network_count:  153
learn step counter:  45651
dev_network_count:  153
learn step counter:  45701
dev_network_count:  153
learn step counter:  45751
dev_network_count:  153
learn step counter:  45801
dev_network_count:  153
learn step counter:  45851
dev_network_count:  153

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
154  r_total and score:  292.8000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  45901
dev_network_count:  154
learn step counter:  45951
dev_network_count:  154
EPOCH %d 49
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.005726416897022355 0.3
4664 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4665 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4666 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4667 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4668 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4669 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4670 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  46001
dev_network_count:  154
learn step counter:  46051
dev_network_count:  154
learn step counter:  46101
dev_network_count:  154
learn step counter:  46151
dev_network_count:  154

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
155  r_total and score:  292.8000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  46201
dev_network_count:  155
learn step counter:  46251
dev_network_count:  155
learn step counter:  46301
dev_network_count:  155
learn step counter:  46351
dev_network_count:  155
learn step counter:  46401
dev_network_count:  155
learn step counter:  46451
dev_network_count:  155

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
156  r_total and score:  292.8000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  46501
dev_network_count:  156
learn step counter:  46551
dev_network_count:  156
learn step counter:  46601
dev_network_count:  156
learn step counter:  46651
dev_network_count:  156
learn step counter:  46701
dev_network_count:  156
learn step counter:  46751
dev_network_count:  156

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
157  r_total and score:  292.8000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  46801
dev_network_count:  157
learn step counter:  46851
dev_network_count:  157
learn step counter:  46901
dev_network_count:  157
learn step counter:  46951
dev_network_count:  157
EPOCH %d 50
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.00515377520732012 0.3
2157 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
2158 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
2159 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
2160 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
2161 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
2162 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
2163 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  47001
dev_network_count:  157
learn step counter:  47051
dev_network_count:  157

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
158  r_total and score:  292.8000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  47101
dev_network_count:  158
learn step counter:  47151
dev_network_count:  158
learn step counter:  47201
dev_network_count:  158
learn step counter:  47251
dev_network_count:  158
learn step counter:  47301
dev_network_count:  158
learn step counter:  47351
dev_network_count:  158

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
159  r_total and score:  292.8000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  47401
dev_network_count:  159
learn step counter:  47451
dev_network_count:  159
learn step counter:  47501
dev_network_count:  159
learn step counter:  47551
dev_network_count:  159
learn step counter:  47601
dev_network_count:  159
learn step counter:  47651
dev_network_count:  159

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
160  r_total and score:  292.8000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  47701
dev_network_count:  160
learn step counter:  47751
dev_network_count:  160
learn step counter:  47801
dev_network_count:  160
learn step counter:  47851
dev_network_count:  160
learn step counter:  47901
dev_network_count:  160
learn step counter:  47951
dev_network_count:  160
EPOCH %d 51
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.004638397686588108 0.3
4650 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4651 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4652 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4653 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4654 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4655 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4656 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
161  r_total and score:  292.8000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  48001
dev_network_count:  161
learn step counter:  48051
dev_network_count:  161
learn step counter:  48101
dev_network_count:  161
learn step counter:  48151
dev_network_count:  161
learn step counter:  48201
dev_network_count:  161
learn step counter:  48251
dev_network_count:  161

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
162  r_total and score:  292.8000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  48301
dev_network_count:  162
learn step counter:  48351
dev_network_count:  162
learn step counter:  48401
dev_network_count:  162
learn step counter:  48451
dev_network_count:  162
learn step counter:  48501
dev_network_count:  162
learn step counter:  48551
dev_network_count:  162

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
163  r_total and score:  292.8000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  48601
dev_network_count:  163
learn step counter:  48651
dev_network_count:  163
learn step counter:  48701
dev_network_count:  163
learn step counter:  48751
dev_network_count:  163
learn step counter:  48801
dev_network_count:  163
learn step counter:  48851
dev_network_count:  163

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
164  r_total and score:  292.8000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  48901
dev_network_count:  164
learn step counter:  48951
dev_network_count:  164
EPOCH %d 52
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.004174557917929297 0.3
2143 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
2144 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
2145 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
2146 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
2147 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
2148 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
2149 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  49001
dev_network_count:  164
learn step counter:  49051
dev_network_count:  164
learn step counter:  49101
dev_network_count:  164
learn step counter:  49151
dev_network_count:  164

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
165  r_total and score:  292.8000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  49201
dev_network_count:  165
learn step counter:  49251
dev_network_count:  165
learn step counter:  49301
dev_network_count:  165
learn step counter:  49351
dev_network_count:  165
learn step counter:  49401
dev_network_count:  165
learn step counter:  49451
dev_network_count:  165

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
166  r_total and score:  292.8000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  49501
dev_network_count:  166
learn step counter:  49551
dev_network_count:  166
learn step counter:  49601
dev_network_count:  166
learn step counter:  49651
dev_network_count:  166
learn step counter:  49701
dev_network_count:  166
learn step counter:  49751
dev_network_count:  166

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
167  r_total and score:  292.8000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  49801
dev_network_count:  167
learn step counter:  49851
dev_network_count:  167
learn step counter:  49901
dev_network_count:  167
learn step counter:  49951
dev_network_count:  167
EPOCH %d 53
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.0037571021261363674 0.3
4636 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4637 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4638 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4639 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4640 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4641 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4642 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  50001
dev_network_count:  167
learn step counter:  50051
dev_network_count:  167

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
168  r_total and score:  292.8000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  50101
dev_network_count:  168
learn step counter:  50151
dev_network_count:  168
learn step counter:  50201
dev_network_count:  168
learn step counter:  50251
dev_network_count:  168
learn step counter:  50301
dev_network_count:  168
learn step counter:  50351
dev_network_count:  168

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
169  r_total and score:  292.8000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  50401
dev_network_count:  169
learn step counter:  50451
dev_network_count:  169
learn step counter:  50501
dev_network_count:  169
learn step counter:  50551
dev_network_count:  169
learn step counter:  50601
dev_network_count:  169
learn step counter:  50651
dev_network_count:  169

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
170  r_total and score:  292.8000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  50701
dev_network_count:  170
learn step counter:  50751
dev_network_count:  170
learn step counter:  50801
dev_network_count:  170
learn step counter:  50851
dev_network_count:  170
learn step counter:  50901
dev_network_count:  170
learn step counter:  50951
dev_network_count:  170
EPOCH %d 54
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.0033813919135227306 0.3
2129 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
2130 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
2131 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
2132 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
2133 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
2134 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
2135 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
171  r_total and score:  292.8000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  51001
dev_network_count:  171
learn step counter:  51051
dev_network_count:  171
learn step counter:  51101
dev_network_count:  171
learn step counter:  51151
dev_network_count:  171
learn step counter:  51201
dev_network_count:  171
learn step counter:  51251
dev_network_count:  171

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
172  r_total and score:  292.8000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  51301
dev_network_count:  172
learn step counter:  51351
dev_network_count:  172
learn step counter:  51401
dev_network_count:  172
learn step counter:  51451
dev_network_count:  172
learn step counter:  51501
dev_network_count:  172
learn step counter:  51551
dev_network_count:  172

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
173  r_total and score:  292.8000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  51601
dev_network_count:  173
learn step counter:  51651
dev_network_count:  173
learn step counter:  51701
dev_network_count:  173
learn step counter:  51751
dev_network_count:  173
learn step counter:  51801
dev_network_count:  173
learn step counter:  51851
dev_network_count:  173

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
174  r_total and score:  292.8000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  51901
dev_network_count:  174
learn step counter:  51951
dev_network_count:  174
EPOCH %d 55
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.0030432527221704577 0.3
4622 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4623 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4624 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4625 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4626 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4627 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4628 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  52001
dev_network_count:  174
learn step counter:  52051
dev_network_count:  174
learn step counter:  52101
dev_network_count:  174
learn step counter:  52151
dev_network_count:  174

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
175  r_total and score:  292.8000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  52201
dev_network_count:  175
learn step counter:  52251
dev_network_count:  175
learn step counter:  52301
dev_network_count:  175
learn step counter:  52351
dev_network_count:  175
learn step counter:  52401
dev_network_count:  175
learn step counter:  52451
dev_network_count:  175

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
176  r_total and score:  292.8000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  52501
dev_network_count:  176
learn step counter:  52551
dev_network_count:  176
learn step counter:  52601
dev_network_count:  176
learn step counter:  52651
dev_network_count:  176
learn step counter:  52701
dev_network_count:  176
learn step counter:  52751
dev_network_count:  176

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
177  r_total and score:  292.8000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  52801
dev_network_count:  177
learn step counter:  52851
dev_network_count:  177
learn step counter:  52901
dev_network_count:  177
learn step counter:  52951
dev_network_count:  177
EPOCH %d 56
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.002738927449953412 0.3
2115 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
2116 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
2117 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
2118 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
2119 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
2120 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
2121 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  53001
dev_network_count:  177
learn step counter:  53051
dev_network_count:  177

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
178  r_total and score:  292.8000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  53101
dev_network_count:  178
learn step counter:  53151
dev_network_count:  178
learn step counter:  53201
dev_network_count:  178
learn step counter:  53251
dev_network_count:  178
learn step counter:  53301
dev_network_count:  178
learn step counter:  53351
dev_network_count:  178

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
179  r_total and score:  292.8000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  53401
dev_network_count:  179
learn step counter:  53451
dev_network_count:  179
learn step counter:  53501
dev_network_count:  179
learn step counter:  53551
dev_network_count:  179
learn step counter:  53601
dev_network_count:  179
learn step counter:  53651
dev_network_count:  179

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
180  r_total and score:  292.8000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  53701
dev_network_count:  180
learn step counter:  53751
dev_network_count:  180
learn step counter:  53801
dev_network_count:  180
learn step counter:  53851
dev_network_count:  180
learn step counter:  53901
dev_network_count:  180
learn step counter:  53951
dev_network_count:  180
EPOCH %d 57
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.002465034704958071 0.3
4608 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4609 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4610 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4611 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4612 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4613 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4614 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
181  r_total and score:  292.8000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  54001
dev_network_count:  181
learn step counter:  54051
dev_network_count:  181
learn step counter:  54101
dev_network_count:  181
learn step counter:  54151
dev_network_count:  181
learn step counter:  54201
dev_network_count:  181
learn step counter:  54251
dev_network_count:  181

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
182  r_total and score:  292.8000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  54301
dev_network_count:  182
learn step counter:  54351
dev_network_count:  182
learn step counter:  54401
dev_network_count:  182
learn step counter:  54451
dev_network_count:  182
learn step counter:  54501
dev_network_count:  182
learn step counter:  54551
dev_network_count:  182

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
183  r_total and score:  292.8000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  54601
dev_network_count:  183
learn step counter:  54651
dev_network_count:  183
learn step counter:  54701
dev_network_count:  183
learn step counter:  54751
dev_network_count:  183
learn step counter:  54801
dev_network_count:  183
learn step counter:  54851
dev_network_count:  183

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
184  r_total and score:  292.8000000000003 0.0
Current Bleu score is:  0.0
learn step counter:  54901
dev_network_count:  184
learn step counter:  54951
dev_network_count:  184
EPOCH %d 58
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.002218531234462264 0.3
2101 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
2102 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
2103 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
2104 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
2105 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
2106 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
2107 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  55001
dev_network_count:  184
learn step counter:  55051
dev_network_count:  184
learn step counter:  55101
dev_network_count:  184
learn step counter:  55151
dev_network_count:  184

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
185  r_total and score:  294.80000000000035 0.0
Current Bleu score is:  0.0
learn step counter:  55201
dev_network_count:  185
learn step counter:  55251
dev_network_count:  185
learn step counter:  55301
dev_network_count:  185
learn step counter:  55351
dev_network_count:  185
learn step counter:  55401
dev_network_count:  185
learn step counter:  55451
dev_network_count:  185

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
186  r_total and score:  294.80000000000035 0.0
Current Bleu score is:  0.0
learn step counter:  55501
dev_network_count:  186
learn step counter:  55551
dev_network_count:  186
learn step counter:  55601
dev_network_count:  186
learn step counter:  55651
dev_network_count:  186
learn step counter:  55701
dev_network_count:  186
learn step counter:  55751
dev_network_count:  186

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
187  r_total and score:  294.80000000000035 0.0
Current Bleu score is:  0.0
learn step counter:  55801
dev_network_count:  187
learn step counter:  55851
dev_network_count:  187
learn step counter:  55901
dev_network_count:  187
learn step counter:  55951
dev_network_count:  187
EPOCH %d 59
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.0019966781110160375 0.3
4594 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4595 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4596 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4597 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4598 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4599 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4600 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  56001
dev_network_count:  187
learn step counter:  56051
dev_network_count:  187

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
188  r_total and score:  294.80000000000035 0.0
Current Bleu score is:  0.0
learn step counter:  56101
dev_network_count:  188
learn step counter:  56151
dev_network_count:  188
learn step counter:  56201
dev_network_count:  188
learn step counter:  56251
dev_network_count:  188
learn step counter:  56301
dev_network_count:  188
learn step counter:  56351
dev_network_count:  188

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
189  r_total and score:  294.80000000000035 0.0
Current Bleu score is:  0.0
learn step counter:  56401
dev_network_count:  189
learn step counter:  56451
dev_network_count:  189
learn step counter:  56501
dev_network_count:  189
learn step counter:  56551
dev_network_count:  189
learn step counter:  56601
dev_network_count:  189
learn step counter:  56651
dev_network_count:  189

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
190  r_total and score:  294.80000000000035 0.0
Current Bleu score is:  0.0
learn step counter:  56701
dev_network_count:  190
learn step counter:  56751
dev_network_count:  190
learn step counter:  56801
dev_network_count:  190
learn step counter:  56851
dev_network_count:  190
learn step counter:  56901
dev_network_count:  190
learn step counter:  56951
dev_network_count:  190
EPOCH %d 60
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001797010299914434 0.3
2087 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
2088 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
2089 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
2090 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
2091 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
2092 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
2093 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
191  r_total and score:  294.80000000000035 0.0
Current Bleu score is:  0.0
learn step counter:  57001
dev_network_count:  191
learn step counter:  57051
dev_network_count:  191
learn step counter:  57101
dev_network_count:  191
learn step counter:  57151
dev_network_count:  191
learn step counter:  57201
dev_network_count:  191
learn step counter:  57251
dev_network_count:  191

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
192  r_total and score:  294.80000000000035 0.0
Current Bleu score is:  0.0
learn step counter:  57301
dev_network_count:  192
learn step counter:  57351
dev_network_count:  192
learn step counter:  57401
dev_network_count:  192
learn step counter:  57451
dev_network_count:  192
learn step counter:  57501
dev_network_count:  192
learn step counter:  57551
dev_network_count:  192

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
193  r_total and score:  294.80000000000035 0.0
Current Bleu score is:  0.0
learn step counter:  57601
dev_network_count:  193
learn step counter:  57651
dev_network_count:  193
learn step counter:  57701
dev_network_count:  193
learn step counter:  57751
dev_network_count:  193
learn step counter:  57801
dev_network_count:  193
learn step counter:  57851
dev_network_count:  193

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
194  r_total and score:  294.80000000000035 0.0
Current Bleu score is:  0.0
learn step counter:  57901
dev_network_count:  194
learn step counter:  57951
dev_network_count:  194
EPOCH %d 61
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.0016173092699229906 0.3
4580 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4581 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4582 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4583 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4584 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4585 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4586 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  58001
dev_network_count:  194
learn step counter:  58051
dev_network_count:  194
learn step counter:  58101
dev_network_count:  194
learn step counter:  58151
dev_network_count:  194

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
195  r_total and score:  294.80000000000035 0.0
Current Bleu score is:  0.0
learn step counter:  58201
dev_network_count:  195
learn step counter:  58251
dev_network_count:  195
learn step counter:  58301
dev_network_count:  195
learn step counter:  58351
dev_network_count:  195
learn step counter:  58401
dev_network_count:  195
learn step counter:  58451
dev_network_count:  195

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
196  r_total and score:  294.80000000000035 0.0
Current Bleu score is:  0.0
learn step counter:  58501
dev_network_count:  196
learn step counter:  58551
dev_network_count:  196
learn step counter:  58601
dev_network_count:  196
learn step counter:  58651
dev_network_count:  196
learn step counter:  58701
dev_network_count:  196
learn step counter:  58751
dev_network_count:  196

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
197  r_total and score:  294.80000000000035 0.0
Current Bleu score is:  0.0
learn step counter:  58801
dev_network_count:  197
learn step counter:  58851
dev_network_count:  197
learn step counter:  58901
dev_network_count:  197
learn step counter:  58951
dev_network_count:  197
EPOCH %d 62
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.0014555783429306916 0.3
2073 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
2074 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
2075 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
2076 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
2077 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
2078 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
2079 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  59001
dev_network_count:  197
learn step counter:  59051
dev_network_count:  197

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
198  r_total and score:  294.80000000000035 0.0
Current Bleu score is:  0.0
learn step counter:  59101
dev_network_count:  198
learn step counter:  59151
dev_network_count:  198
learn step counter:  59201
dev_network_count:  198
learn step counter:  59251
dev_network_count:  198
learn step counter:  59301
dev_network_count:  198
learn step counter:  59351
dev_network_count:  198

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
199  r_total and score:  294.80000000000035 0.0
Current Bleu score is:  0.0
learn step counter:  59401
dev_network_count:  199
learn step counter:  59451
dev_network_count:  199
learn step counter:  59501
dev_network_count:  199
learn step counter:  59551
dev_network_count:  199
learn step counter:  59601
dev_network_count:  199
learn step counter:  59651
dev_network_count:  199

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
200  r_total and score:  294.80000000000035 0.0
Current Bleu score is:  0.0
learn step counter:  59701
dev_network_count:  200
learn step counter:  59751
dev_network_count:  200
learn step counter:  59801
dev_network_count:  200
learn step counter:  59851
dev_network_count:  200
learn step counter:  59901
dev_network_count:  200
learn step counter:  59951
dev_network_count:  200
EPOCH %d 63
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.0013100205086376223 0.3
4566 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4567 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4568 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4569 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4570 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4571 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4572 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
201  r_total and score:  294.80000000000035 0.0
Current Bleu score is:  0.0
learn step counter:  60001
dev_network_count:  201
learn step counter:  60051
dev_network_count:  201
learn step counter:  60101
dev_network_count:  201
learn step counter:  60151
dev_network_count:  201
learn step counter:  60201
dev_network_count:  201
learn step counter:  60251
dev_network_count:  201

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
202  r_total and score:  294.80000000000035 0.0
Current Bleu score is:  0.0
learn step counter:  60301
dev_network_count:  202
learn step counter:  60351
dev_network_count:  202
learn step counter:  60401
dev_network_count:  202
learn step counter:  60451
dev_network_count:  202
learn step counter:  60501
dev_network_count:  202
learn step counter:  60551
dev_network_count:  202

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
203  r_total and score:  294.80000000000035 0.0
Current Bleu score is:  0.0
learn step counter:  60601
dev_network_count:  203
learn step counter:  60651
dev_network_count:  203
learn step counter:  60701
dev_network_count:  203
learn step counter:  60751
dev_network_count:  203
learn step counter:  60801
dev_network_count:  203
learn step counter:  60851
dev_network_count:  203

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
204  r_total and score:  294.80000000000035 0.0
Current Bleu score is:  0.0
learn step counter:  60901
dev_network_count:  204
learn step counter:  60951
dev_network_count:  204
EPOCH %d 64
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.0011790184577738603 0.3
2059 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
2060 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
2061 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
2062 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
2063 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
2064 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
2065 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  61001
dev_network_count:  204
learn step counter:  61051
dev_network_count:  204
learn step counter:  61101
dev_network_count:  204
learn step counter:  61151
dev_network_count:  204

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
205  r_total and score:  294.80000000000035 0.0
Current Bleu score is:  0.0
learn step counter:  61201
dev_network_count:  205
learn step counter:  61251
dev_network_count:  205
learn step counter:  61301
dev_network_count:  205
learn step counter:  61351
dev_network_count:  205
learn step counter:  61401
dev_network_count:  205
learn step counter:  61451
dev_network_count:  205

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
206  r_total and score:  294.80000000000035 0.0
Current Bleu score is:  0.0
learn step counter:  61501
dev_network_count:  206
learn step counter:  61551
dev_network_count:  206
learn step counter:  61601
dev_network_count:  206
learn step counter:  61651
dev_network_count:  206
learn step counter:  61701
dev_network_count:  206
learn step counter:  61751
dev_network_count:  206

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
207  r_total and score:  294.80000000000035 0.0
Current Bleu score is:  0.0
learn step counter:  61801
dev_network_count:  207
learn step counter:  61851
dev_network_count:  207
learn step counter:  61901
dev_network_count:  207
learn step counter:  61951
dev_network_count:  207
EPOCH %d 65
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001061116611996474 0.3
4552 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4553 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4554 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4555 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4556 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4557 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4558 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  62001
dev_network_count:  207
learn step counter:  62051
dev_network_count:  207

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
208  r_total and score:  294.80000000000035 0.0
Current Bleu score is:  0.0
learn step counter:  62101
dev_network_count:  208
learn step counter:  62151
dev_network_count:  208
learn step counter:  62201
dev_network_count:  208
learn step counter:  62251
dev_network_count:  208
learn step counter:  62301
dev_network_count:  208
learn step counter:  62351
dev_network_count:  208

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
209  r_total and score:  294.80000000000035 0.0
Current Bleu score is:  0.0
learn step counter:  62401
dev_network_count:  209
learn step counter:  62451
dev_network_count:  209
learn step counter:  62501
dev_network_count:  209
learn step counter:  62551
dev_network_count:  209
learn step counter:  62601
dev_network_count:  209
learn step counter:  62651
dev_network_count:  209

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
210  r_total and score:  294.80000000000035 0.0
Current Bleu score is:  0.0
learn step counter:  62701
dev_network_count:  210
learn step counter:  62751
dev_network_count:  210
learn step counter:  62801
dev_network_count:  210
learn step counter:  62851
dev_network_count:  210
learn step counter:  62901
dev_network_count:  210
learn step counter:  62951
dev_network_count:  210
EPOCH %d 66
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
2045 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
2046 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
2047 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
2048 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
2049 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
2050 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
2051 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
211  r_total and score:  294.80000000000035 0.0
Current Bleu score is:  0.0
learn step counter:  63001
dev_network_count:  211
learn step counter:  63051
dev_network_count:  211
learn step counter:  63101
dev_network_count:  211
learn step counter:  63151
dev_network_count:  211
learn step counter:  63201
dev_network_count:  211
learn step counter:  63251
dev_network_count:  211

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
212  r_total and score:  294.80000000000035 0.0
Current Bleu score is:  0.0
learn step counter:  63301
dev_network_count:  212
learn step counter:  63351
dev_network_count:  212
learn step counter:  63401
dev_network_count:  212
learn step counter:  63451
dev_network_count:  212
learn step counter:  63501
dev_network_count:  212
learn step counter:  63551
dev_network_count:  212

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
213  r_total and score:  294.80000000000035 0.0
Current Bleu score is:  0.0
learn step counter:  63601
dev_network_count:  213
learn step counter:  63651
dev_network_count:  213
learn step counter:  63701
dev_network_count:  213
learn step counter:  63751
dev_network_count:  213
learn step counter:  63801
dev_network_count:  213
learn step counter:  63851
dev_network_count:  213

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
214  r_total and score:  294.80000000000035 0.0
Current Bleu score is:  0.0
learn step counter:  63901
dev_network_count:  214
learn step counter:  63951
dev_network_count:  214
EPOCH %d 67
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
4538 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4539 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4540 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4541 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4542 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4543 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4544 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  64001
dev_network_count:  214
learn step counter:  64051
dev_network_count:  214
learn step counter:  64101
dev_network_count:  214
learn step counter:  64151
dev_network_count:  214

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
215  r_total and score:  294.80000000000035 0.0
Current Bleu score is:  0.0
learn step counter:  64201
dev_network_count:  215
learn step counter:  64251
dev_network_count:  215
learn step counter:  64301
dev_network_count:  215
learn step counter:  64351
dev_network_count:  215
learn step counter:  64401
dev_network_count:  215
learn step counter:  64451
dev_network_count:  215

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
216  r_total and score:  294.80000000000035 0.0
Current Bleu score is:  0.0
learn step counter:  64501
dev_network_count:  216
learn step counter:  64551
dev_network_count:  216
learn step counter:  64601
dev_network_count:  216
learn step counter:  64651
dev_network_count:  216
learn step counter:  64701
dev_network_count:  216
learn step counter:  64751
dev_network_count:  216

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
217  r_total and score:  294.80000000000035 0.0
Current Bleu score is:  0.0
learn step counter:  64801
dev_network_count:  217
learn step counter:  64851
dev_network_count:  217
learn step counter:  64901
dev_network_count:  217
learn step counter:  64951
dev_network_count:  217
EPOCH %d 68
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
2031 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
2032 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
2033 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
2034 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
2035 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
2036 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
2037 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  65001
dev_network_count:  217
learn step counter:  65051
dev_network_count:  217

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
218  r_total and score:  294.80000000000035 0.0
Current Bleu score is:  0.0
learn step counter:  65101
dev_network_count:  218
learn step counter:  65151
dev_network_count:  218
learn step counter:  65201
dev_network_count:  218
learn step counter:  65251
dev_network_count:  218
learn step counter:  65301
dev_network_count:  218
learn step counter:  65351
dev_network_count:  218

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
219  r_total and score:  294.80000000000035 0.0
Current Bleu score is:  0.0
learn step counter:  65401
dev_network_count:  219
learn step counter:  65451
dev_network_count:  219
learn step counter:  65501
dev_network_count:  219
learn step counter:  65551
dev_network_count:  219
learn step counter:  65601
dev_network_count:  219
learn step counter:  65651
dev_network_count:  219

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
220  r_total and score:  294.80000000000035 0.0
Current Bleu score is:  0.0
learn step counter:  65701
dev_network_count:  220
learn step counter:  65751
dev_network_count:  220
learn step counter:  65801
dev_network_count:  220
learn step counter:  65851
dev_network_count:  220
learn step counter:  65901
dev_network_count:  220
learn step counter:  65951
dev_network_count:  220
EPOCH %d 69
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
4524 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4525 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4526 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4527 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4528 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4529 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4530 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
221  r_total and score:  294.80000000000035 0.0
Current Bleu score is:  0.0
learn step counter:  66001
dev_network_count:  221
learn step counter:  66051
dev_network_count:  221
learn step counter:  66101
dev_network_count:  221
learn step counter:  66151
dev_network_count:  221
learn step counter:  66201
dev_network_count:  221
learn step counter:  66251
dev_network_count:  221

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
222  r_total and score:  294.80000000000035 0.0
Current Bleu score is:  0.0
learn step counter:  66301
dev_network_count:  222
learn step counter:  66351
dev_network_count:  222
learn step counter:  66401
dev_network_count:  222
learn step counter:  66451
dev_network_count:  222
learn step counter:  66501
dev_network_count:  222
learn step counter:  66551
dev_network_count:  222

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
223  r_total and score:  294.80000000000035 0.0
Current Bleu score is:  0.0
learn step counter:  66601
dev_network_count:  223
learn step counter:  66651
dev_network_count:  223
learn step counter:  66701
dev_network_count:  223
learn step counter:  66751
dev_network_count:  223
learn step counter:  66801
dev_network_count:  223
learn step counter:  66851
dev_network_count:  223

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
224  r_total and score:  294.80000000000035 0.0
Current Bleu score is:  0.0
learn step counter:  66901
dev_network_count:  224
learn step counter:  66951
dev_network_count:  224
EPOCH %d 70
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
2017 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
2018 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
2019 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
2020 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
2021 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
2022 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
2023 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  67001
dev_network_count:  224
learn step counter:  67051
dev_network_count:  224
learn step counter:  67101
dev_network_count:  224
learn step counter:  67151
dev_network_count:  224

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
225  r_total and score:  294.80000000000035 0.0
Current Bleu score is:  0.0
learn step counter:  67201
dev_network_count:  225
learn step counter:  67251
dev_network_count:  225
learn step counter:  67301
dev_network_count:  225
learn step counter:  67351
dev_network_count:  225
learn step counter:  67401
dev_network_count:  225
learn step counter:  67451
dev_network_count:  225

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
226  r_total and score:  294.80000000000035 0.0
Current Bleu score is:  0.0
learn step counter:  67501
dev_network_count:  226
learn step counter:  67551
dev_network_count:  226
learn step counter:  67601
dev_network_count:  226
learn step counter:  67651
dev_network_count:  226
learn step counter:  67701
dev_network_count:  226
learn step counter:  67751
dev_network_count:  226

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
227  r_total and score:  294.80000000000035 0.0
Current Bleu score is:  0.0
learn step counter:  67801
dev_network_count:  227
learn step counter:  67851
dev_network_count:  227
learn step counter:  67901
dev_network_count:  227
learn step counter:  67951
dev_network_count:  227
EPOCH %d 71
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
4510 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4511 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4512 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4513 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4514 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4515 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4516 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  68001
dev_network_count:  227
learn step counter:  68051
dev_network_count:  227

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
228  r_total and score:  294.80000000000035 0.0
Current Bleu score is:  0.0
learn step counter:  68101
dev_network_count:  228
learn step counter:  68151
dev_network_count:  228
learn step counter:  68201
dev_network_count:  228
learn step counter:  68251
dev_network_count:  228
learn step counter:  68301
dev_network_count:  228
learn step counter:  68351
dev_network_count:  228

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
229  r_total and score:  294.80000000000035 0.0
Current Bleu score is:  0.0
learn step counter:  68401
dev_network_count:  229
learn step counter:  68451
dev_network_count:  229
learn step counter:  68501
dev_network_count:  229
learn step counter:  68551
dev_network_count:  229
learn step counter:  68601
dev_network_count:  229
learn step counter:  68651
dev_network_count:  229

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
230  r_total and score:  294.80000000000035 0.0
Current Bleu score is:  0.0
learn step counter:  68701
dev_network_count:  230
learn step counter:  68751
dev_network_count:  230
learn step counter:  68801
dev_network_count:  230
learn step counter:  68851
dev_network_count:  230
learn step counter:  68901
dev_network_count:  230
learn step counter:  68951
dev_network_count:  230
EPOCH %d 72
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
2003 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
2004 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
2005 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
2006 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
2007 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
2008 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
2009 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
231  r_total and score:  294.80000000000035 0.0
Current Bleu score is:  0.0
learn step counter:  69001
dev_network_count:  231
learn step counter:  69051
dev_network_count:  231
learn step counter:  69101
dev_network_count:  231
learn step counter:  69151
dev_network_count:  231
learn step counter:  69201
dev_network_count:  231
learn step counter:  69251
dev_network_count:  231

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
232  r_total and score:  294.80000000000035 0.0
Current Bleu score is:  0.0
learn step counter:  69301
dev_network_count:  232
learn step counter:  69351
dev_network_count:  232
learn step counter:  69401
dev_network_count:  232
learn step counter:  69451
dev_network_count:  232
learn step counter:  69501
dev_network_count:  232
learn step counter:  69551
dev_network_count:  232

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
233  r_total and score:  294.80000000000035 0.0
Current Bleu score is:  0.0
learn step counter:  69601
dev_network_count:  233
learn step counter:  69651
dev_network_count:  233
learn step counter:  69701
dev_network_count:  233
learn step counter:  69751
dev_network_count:  233
learn step counter:  69801
dev_network_count:  233
learn step counter:  69851
dev_network_count:  233

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
234  r_total and score:  294.80000000000035 0.0
Current Bleu score is:  0.0
learn step counter:  69901
dev_network_count:  234
learn step counter:  69951
dev_network_count:  234
EPOCH %d 73
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
4496 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4497 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4498 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4499 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4500 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4501 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4502 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  70001
dev_network_count:  234
learn step counter:  70051
dev_network_count:  234
learn step counter:  70101
dev_network_count:  234
learn step counter:  70151
dev_network_count:  234

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
235  r_total and score:  294.80000000000035 0.0
Current Bleu score is:  0.0
learn step counter:  70201
dev_network_count:  235
learn step counter:  70251
dev_network_count:  235
learn step counter:  70301
dev_network_count:  235
learn step counter:  70351
dev_network_count:  235
learn step counter:  70401
dev_network_count:  235
learn step counter:  70451
dev_network_count:  235

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
236  r_total and score:  294.80000000000035 0.0
Current Bleu score is:  0.0
learn step counter:  70501
dev_network_count:  236
learn step counter:  70551
dev_network_count:  236
learn step counter:  70601
dev_network_count:  236
learn step counter:  70651
dev_network_count:  236
learn step counter:  70701
dev_network_count:  236
learn step counter:  70751
dev_network_count:  236

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
237  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  70801
dev_network_count:  237
learn step counter:  70851
dev_network_count:  237
learn step counter:  70901
dev_network_count:  237
learn step counter:  70951
dev_network_count:  237
EPOCH %d 74
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1989 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1990 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1991 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1992 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1993 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1994 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1995 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  71001
dev_network_count:  237
learn step counter:  71051
dev_network_count:  237

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
238  r_total and score:  294.80000000000035 0.0
Current Bleu score is:  0.0
learn step counter:  71101
dev_network_count:  238
learn step counter:  71151
dev_network_count:  238
learn step counter:  71201
dev_network_count:  238
learn step counter:  71251
dev_network_count:  238
learn step counter:  71301
dev_network_count:  238
learn step counter:  71351
dev_network_count:  238

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
239  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  71401
dev_network_count:  239
learn step counter:  71451
dev_network_count:  239
learn step counter:  71501
dev_network_count:  239
learn step counter:  71551
dev_network_count:  239
learn step counter:  71601
dev_network_count:  239
learn step counter:  71651
dev_network_count:  239

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
240  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  71701
dev_network_count:  240
learn step counter:  71751
dev_network_count:  240
learn step counter:  71801
dev_network_count:  240
learn step counter:  71851
dev_network_count:  240
learn step counter:  71901
dev_network_count:  240
learn step counter:  71951
dev_network_count:  240
EPOCH %d 75
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
4482 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4483 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4484 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4485 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4486 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4487 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4488 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
241  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  72001
dev_network_count:  241
learn step counter:  72051
dev_network_count:  241
learn step counter:  72101
dev_network_count:  241
learn step counter:  72151
dev_network_count:  241
learn step counter:  72201
dev_network_count:  241
learn step counter:  72251
dev_network_count:  241

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
242  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  72301
dev_network_count:  242
learn step counter:  72351
dev_network_count:  242
learn step counter:  72401
dev_network_count:  242
learn step counter:  72451
dev_network_count:  242
learn step counter:  72501
dev_network_count:  242
learn step counter:  72551
dev_network_count:  242

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
243  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  72601
dev_network_count:  243
learn step counter:  72651
dev_network_count:  243
learn step counter:  72701
dev_network_count:  243
learn step counter:  72751
dev_network_count:  243
learn step counter:  72801
dev_network_count:  243
learn step counter:  72851
dev_network_count:  243

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
244  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  72901
dev_network_count:  244
learn step counter:  72951
dev_network_count:  244
EPOCH %d 76
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1975 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1976 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1977 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1978 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1979 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1980 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1981 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  73001
dev_network_count:  244
learn step counter:  73051
dev_network_count:  244
learn step counter:  73101
dev_network_count:  244
learn step counter:  73151
dev_network_count:  244

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
245  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  73201
dev_network_count:  245
learn step counter:  73251
dev_network_count:  245
learn step counter:  73301
dev_network_count:  245
learn step counter:  73351
dev_network_count:  245
learn step counter:  73401
dev_network_count:  245
learn step counter:  73451
dev_network_count:  245

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
246  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  73501
dev_network_count:  246
learn step counter:  73551
dev_network_count:  246
learn step counter:  73601
dev_network_count:  246
learn step counter:  73651
dev_network_count:  246
learn step counter:  73701
dev_network_count:  246
learn step counter:  73751
dev_network_count:  246

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
247  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  73801
dev_network_count:  247
learn step counter:  73851
dev_network_count:  247
learn step counter:  73901
dev_network_count:  247
learn step counter:  73951
dev_network_count:  247
EPOCH %d 77
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
4468 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4469 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4470 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4471 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4472 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4473 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4474 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  74001
dev_network_count:  247
learn step counter:  74051
dev_network_count:  247

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
248  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  74101
dev_network_count:  248
learn step counter:  74151
dev_network_count:  248
learn step counter:  74201
dev_network_count:  248
learn step counter:  74251
dev_network_count:  248
learn step counter:  74301
dev_network_count:  248
learn step counter:  74351
dev_network_count:  248

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
249  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  74401
dev_network_count:  249
learn step counter:  74451
dev_network_count:  249
learn step counter:  74501
dev_network_count:  249
learn step counter:  74551
dev_network_count:  249
learn step counter:  74601
dev_network_count:  249
learn step counter:  74651
dev_network_count:  249

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
250  r_total and score:  294.80000000000035 0.0
Current Bleu score is:  0.0
learn step counter:  74701
dev_network_count:  250
learn step counter:  74751
dev_network_count:  250
learn step counter:  74801
dev_network_count:  250
learn step counter:  74851
dev_network_count:  250
learn step counter:  74901
dev_network_count:  250
learn step counter:  74951
dev_network_count:  250
EPOCH %d 78
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1961 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1962 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1963 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1964 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1965 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1966 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1967 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
251  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  75001
dev_network_count:  251
learn step counter:  75051
dev_network_count:  251
learn step counter:  75101
dev_network_count:  251
learn step counter:  75151
dev_network_count:  251
learn step counter:  75201
dev_network_count:  251
learn step counter:  75251
dev_network_count:  251

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
252  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  75301
dev_network_count:  252
learn step counter:  75351
dev_network_count:  252
learn step counter:  75401
dev_network_count:  252
learn step counter:  75451
dev_network_count:  252
learn step counter:  75501
dev_network_count:  252
learn step counter:  75551
dev_network_count:  252

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
253  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  75601
dev_network_count:  253
learn step counter:  75651
dev_network_count:  253
learn step counter:  75701
dev_network_count:  253
learn step counter:  75751
dev_network_count:  253
learn step counter:  75801
dev_network_count:  253
learn step counter:  75851
dev_network_count:  253

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
254  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  75901
dev_network_count:  254
learn step counter:  75951
dev_network_count:  254
EPOCH %d 79
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
4454 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4455 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4456 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4457 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4458 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4459 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4460 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  76001
dev_network_count:  254
learn step counter:  76051
dev_network_count:  254
learn step counter:  76101
dev_network_count:  254
learn step counter:  76151
dev_network_count:  254

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
255  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  76201
dev_network_count:  255
learn step counter:  76251
dev_network_count:  255
learn step counter:  76301
dev_network_count:  255
learn step counter:  76351
dev_network_count:  255
learn step counter:  76401
dev_network_count:  255
learn step counter:  76451
dev_network_count:  255

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
256  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  76501
dev_network_count:  256
learn step counter:  76551
dev_network_count:  256
learn step counter:  76601
dev_network_count:  256
learn step counter:  76651
dev_network_count:  256
learn step counter:  76701
dev_network_count:  256
learn step counter:  76751
dev_network_count:  256

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
257  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  76801
dev_network_count:  257
learn step counter:  76851
dev_network_count:  257
learn step counter:  76901
dev_network_count:  257
learn step counter:  76951
dev_network_count:  257
EPOCH %d 80
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1947 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1948 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1949 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1950 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1951 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1952 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1953 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  77001
dev_network_count:  257
learn step counter:  77051
dev_network_count:  257

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
258  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  77101
dev_network_count:  258
learn step counter:  77151
dev_network_count:  258
learn step counter:  77201
dev_network_count:  258
learn step counter:  77251
dev_network_count:  258
learn step counter:  77301
dev_network_count:  258
learn step counter:  77351
dev_network_count:  258

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
259  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  77401
dev_network_count:  259
learn step counter:  77451
dev_network_count:  259
learn step counter:  77501
dev_network_count:  259
learn step counter:  77551
dev_network_count:  259
learn step counter:  77601
dev_network_count:  259
learn step counter:  77651
dev_network_count:  259

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
260  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  77701
dev_network_count:  260
learn step counter:  77751
dev_network_count:  260
learn step counter:  77801
dev_network_count:  260
learn step counter:  77851
dev_network_count:  260
learn step counter:  77901
dev_network_count:  260
learn step counter:  77951
dev_network_count:  260
EPOCH %d 81
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
4440 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4441 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4442 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4443 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4444 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4445 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4446 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
261  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  78001
dev_network_count:  261
learn step counter:  78051
dev_network_count:  261
learn step counter:  78101
dev_network_count:  261
learn step counter:  78151
dev_network_count:  261
learn step counter:  78201
dev_network_count:  261
learn step counter:  78251
dev_network_count:  261

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
262  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  78301
dev_network_count:  262
learn step counter:  78351
dev_network_count:  262
learn step counter:  78401
dev_network_count:  262
learn step counter:  78451
dev_network_count:  262
learn step counter:  78501
dev_network_count:  262
learn step counter:  78551
dev_network_count:  262

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
263  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  78601
dev_network_count:  263
learn step counter:  78651
dev_network_count:  263
learn step counter:  78701
dev_network_count:  263
learn step counter:  78751
dev_network_count:  263
learn step counter:  78801
dev_network_count:  263
learn step counter:  78851
dev_network_count:  263

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
264  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  78901
dev_network_count:  264
learn step counter:  78951
dev_network_count:  264
EPOCH %d 82
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1933 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1934 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1935 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1936 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1937 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1938 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1939 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  79001
dev_network_count:  264
learn step counter:  79051
dev_network_count:  264
learn step counter:  79101
dev_network_count:  264
learn step counter:  79151
dev_network_count:  264

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
265  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  79201
dev_network_count:  265
learn step counter:  79251
dev_network_count:  265
learn step counter:  79301
dev_network_count:  265
learn step counter:  79351
dev_network_count:  265
learn step counter:  79401
dev_network_count:  265
learn step counter:  79451
dev_network_count:  265

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
266  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  79501
dev_network_count:  266
learn step counter:  79551
dev_network_count:  266
learn step counter:  79601
dev_network_count:  266
learn step counter:  79651
dev_network_count:  266
learn step counter:  79701
dev_network_count:  266
learn step counter:  79751
dev_network_count:  266

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
267  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  79801
dev_network_count:  267
learn step counter:  79851
dev_network_count:  267
learn step counter:  79901
dev_network_count:  267
learn step counter:  79951
dev_network_count:  267
EPOCH %d 83
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
4426 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4427 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4428 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4429 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4430 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4431 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4432 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  80001
dev_network_count:  267
learn step counter:  80051
dev_network_count:  267

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
268  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  80101
dev_network_count:  268
learn step counter:  80151
dev_network_count:  268
learn step counter:  80201
dev_network_count:  268
learn step counter:  80251
dev_network_count:  268
learn step counter:  80301
dev_network_count:  268
learn step counter:  80351
dev_network_count:  268

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
269  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  80401
dev_network_count:  269
learn step counter:  80451
dev_network_count:  269
learn step counter:  80501
dev_network_count:  269
learn step counter:  80551
dev_network_count:  269
learn step counter:  80601
dev_network_count:  269
learn step counter:  80651
dev_network_count:  269

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
270  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  80701
dev_network_count:  270
learn step counter:  80751
dev_network_count:  270
learn step counter:  80801
dev_network_count:  270
learn step counter:  80851
dev_network_count:  270
learn step counter:  80901
dev_network_count:  270
learn step counter:  80951
dev_network_count:  270
EPOCH %d 84
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1919 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1920 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1921 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1922 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1923 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1924 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1925 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
271  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  81001
dev_network_count:  271
learn step counter:  81051
dev_network_count:  271
learn step counter:  81101
dev_network_count:  271
learn step counter:  81151
dev_network_count:  271
learn step counter:  81201
dev_network_count:  271
learn step counter:  81251
dev_network_count:  271

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
272  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  81301
dev_network_count:  272
learn step counter:  81351
dev_network_count:  272
learn step counter:  81401
dev_network_count:  272
learn step counter:  81451
dev_network_count:  272
learn step counter:  81501
dev_network_count:  272
learn step counter:  81551
dev_network_count:  272

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
273  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  81601
dev_network_count:  273
learn step counter:  81651
dev_network_count:  273
learn step counter:  81701
dev_network_count:  273
learn step counter:  81751
dev_network_count:  273
learn step counter:  81801
dev_network_count:  273
learn step counter:  81851
dev_network_count:  273

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
274  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  81901
dev_network_count:  274
learn step counter:  81951
dev_network_count:  274
EPOCH %d 85
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
4412 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4413 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4414 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4415 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4416 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4417 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4418 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  82001
dev_network_count:  274
learn step counter:  82051
dev_network_count:  274
learn step counter:  82101
dev_network_count:  274
learn step counter:  82151
dev_network_count:  274

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
275  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  82201
dev_network_count:  275
learn step counter:  82251
dev_network_count:  275
learn step counter:  82301
dev_network_count:  275
learn step counter:  82351
dev_network_count:  275
learn step counter:  82401
dev_network_count:  275
learn step counter:  82451
dev_network_count:  275

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
276  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  82501
dev_network_count:  276
learn step counter:  82551
dev_network_count:  276
learn step counter:  82601
dev_network_count:  276
learn step counter:  82651
dev_network_count:  276
learn step counter:  82701
dev_network_count:  276
learn step counter:  82751
dev_network_count:  276

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
277  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  82801
dev_network_count:  277
learn step counter:  82851
dev_network_count:  277
learn step counter:  82901
dev_network_count:  277
learn step counter:  82951
dev_network_count:  277
EPOCH %d 86
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1905 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1906 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1907 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1908 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1909 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1910 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1911 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  83001
dev_network_count:  277
learn step counter:  83051
dev_network_count:  277

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
278  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  83101
dev_network_count:  278
learn step counter:  83151
dev_network_count:  278
learn step counter:  83201
dev_network_count:  278
learn step counter:  83251
dev_network_count:  278
learn step counter:  83301
dev_network_count:  278
learn step counter:  83351
dev_network_count:  278

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
279  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  83401
dev_network_count:  279
learn step counter:  83451
dev_network_count:  279
learn step counter:  83501
dev_network_count:  279
learn step counter:  83551
dev_network_count:  279
learn step counter:  83601
dev_network_count:  279
learn step counter:  83651
dev_network_count:  279

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
280  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  83701
dev_network_count:  280
learn step counter:  83751
dev_network_count:  280
learn step counter:  83801
dev_network_count:  280
learn step counter:  83851
dev_network_count:  280
learn step counter:  83901
dev_network_count:  280
learn step counter:  83951
dev_network_count:  280
EPOCH %d 87
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
4398 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4399 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4400 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4401 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4402 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4403 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4404 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
281  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  84001
dev_network_count:  281
learn step counter:  84051
dev_network_count:  281
learn step counter:  84101
dev_network_count:  281
learn step counter:  84151
dev_network_count:  281
learn step counter:  84201
dev_network_count:  281
learn step counter:  84251
dev_network_count:  281

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
282  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  84301
dev_network_count:  282
learn step counter:  84351
dev_network_count:  282
learn step counter:  84401
dev_network_count:  282
learn step counter:  84451
dev_network_count:  282
learn step counter:  84501
dev_network_count:  282
learn step counter:  84551
dev_network_count:  282

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
283  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  84601
dev_network_count:  283
learn step counter:  84651
dev_network_count:  283
learn step counter:  84701
dev_network_count:  283
learn step counter:  84751
dev_network_count:  283
learn step counter:  84801
dev_network_count:  283
learn step counter:  84851
dev_network_count:  283

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
284  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  84901
dev_network_count:  284
learn step counter:  84951
dev_network_count:  284
EPOCH %d 88
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1891 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1892 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1893 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1894 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1895 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1896 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1897 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  85001
dev_network_count:  284
learn step counter:  85051
dev_network_count:  284
learn step counter:  85101
dev_network_count:  284
learn step counter:  85151
dev_network_count:  284

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
285  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  85201
dev_network_count:  285
learn step counter:  85251
dev_network_count:  285
learn step counter:  85301
dev_network_count:  285
learn step counter:  85351
dev_network_count:  285
learn step counter:  85401
dev_network_count:  285
learn step counter:  85451
dev_network_count:  285

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
286  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  85501
dev_network_count:  286
learn step counter:  85551
dev_network_count:  286
learn step counter:  85601
dev_network_count:  286
learn step counter:  85651
dev_network_count:  286
learn step counter:  85701
dev_network_count:  286
learn step counter:  85751
dev_network_count:  286

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
287  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  85801
dev_network_count:  287
learn step counter:  85851
dev_network_count:  287
learn step counter:  85901
dev_network_count:  287
learn step counter:  85951
dev_network_count:  287
EPOCH %d 89
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
4384 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4385 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4386 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4387 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4388 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4389 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4390 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  86001
dev_network_count:  287
learn step counter:  86051
dev_network_count:  287

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
288  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  86101
dev_network_count:  288
learn step counter:  86151
dev_network_count:  288
learn step counter:  86201
dev_network_count:  288
learn step counter:  86251
dev_network_count:  288
learn step counter:  86301
dev_network_count:  288
learn step counter:  86351
dev_network_count:  288

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
289  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  86401
dev_network_count:  289
learn step counter:  86451
dev_network_count:  289
learn step counter:  86501
dev_network_count:  289
learn step counter:  86551
dev_network_count:  289
learn step counter:  86601
dev_network_count:  289
learn step counter:  86651
dev_network_count:  289

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
290  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  86701
dev_network_count:  290
learn step counter:  86751
dev_network_count:  290
learn step counter:  86801
dev_network_count:  290
learn step counter:  86851
dev_network_count:  290
learn step counter:  86901
dev_network_count:  290
learn step counter:  86951
dev_network_count:  290
EPOCH %d 90
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1877 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1878 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1879 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1880 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1881 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1882 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1883 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
291  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  87001
dev_network_count:  291
learn step counter:  87051
dev_network_count:  291
learn step counter:  87101
dev_network_count:  291
learn step counter:  87151
dev_network_count:  291
learn step counter:  87201
dev_network_count:  291
learn step counter:  87251
dev_network_count:  291

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
292  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  87301
dev_network_count:  292
learn step counter:  87351
dev_network_count:  292
learn step counter:  87401
dev_network_count:  292
learn step counter:  87451
dev_network_count:  292
learn step counter:  87501
dev_network_count:  292
learn step counter:  87551
dev_network_count:  292

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
293  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  87601
dev_network_count:  293
learn step counter:  87651
dev_network_count:  293
learn step counter:  87701
dev_network_count:  293
learn step counter:  87751
dev_network_count:  293
learn step counter:  87801
dev_network_count:  293
learn step counter:  87851
dev_network_count:  293

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
294  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  87901
dev_network_count:  294
learn step counter:  87951
dev_network_count:  294
EPOCH %d 91
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
4370 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4371 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4372 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4373 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4374 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4375 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4376 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  88001
dev_network_count:  294
learn step counter:  88051
dev_network_count:  294
learn step counter:  88101
dev_network_count:  294
learn step counter:  88151
dev_network_count:  294

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
295  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  88201
dev_network_count:  295
learn step counter:  88251
dev_network_count:  295
learn step counter:  88301
dev_network_count:  295
learn step counter:  88351
dev_network_count:  295
learn step counter:  88401
dev_network_count:  295
learn step counter:  88451
dev_network_count:  295

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
296  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  88501
dev_network_count:  296
learn step counter:  88551
dev_network_count:  296
learn step counter:  88601
dev_network_count:  296
learn step counter:  88651
dev_network_count:  296
learn step counter:  88701
dev_network_count:  296
learn step counter:  88751
dev_network_count:  296

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
297  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  88801
dev_network_count:  297
learn step counter:  88851
dev_network_count:  297
learn step counter:  88901
dev_network_count:  297
learn step counter:  88951
dev_network_count:  297
EPOCH %d 92
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1863 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1864 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1865 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1866 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1867 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1868 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1869 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  89001
dev_network_count:  297
learn step counter:  89051
dev_network_count:  297

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
298  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  89101
dev_network_count:  298
learn step counter:  89151
dev_network_count:  298
learn step counter:  89201
dev_network_count:  298
learn step counter:  89251
dev_network_count:  298
learn step counter:  89301
dev_network_count:  298
learn step counter:  89351
dev_network_count:  298

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
299  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  89401
dev_network_count:  299
learn step counter:  89451
dev_network_count:  299
learn step counter:  89501
dev_network_count:  299
learn step counter:  89551
dev_network_count:  299
learn step counter:  89601
dev_network_count:  299
learn step counter:  89651
dev_network_count:  299

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
300  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  89701
dev_network_count:  300
learn step counter:  89751
dev_network_count:  300
learn step counter:  89801
dev_network_count:  300
learn step counter:  89851
dev_network_count:  300
learn step counter:  89901
dev_network_count:  300
learn step counter:  89951
dev_network_count:  300
EPOCH %d 93
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
4356 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4357 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4358 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4359 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4360 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4361 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4362 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
301  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  90001
dev_network_count:  301
learn step counter:  90051
dev_network_count:  301
learn step counter:  90101
dev_network_count:  301
learn step counter:  90151
dev_network_count:  301
learn step counter:  90201
dev_network_count:  301
learn step counter:  90251
dev_network_count:  301

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
302  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  90301
dev_network_count:  302
learn step counter:  90351
dev_network_count:  302
learn step counter:  90401
dev_network_count:  302
learn step counter:  90451
dev_network_count:  302
learn step counter:  90501
dev_network_count:  302
learn step counter:  90551
dev_network_count:  302

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
303  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  90601
dev_network_count:  303
learn step counter:  90651
dev_network_count:  303
learn step counter:  90701
dev_network_count:  303
learn step counter:  90751
dev_network_count:  303
learn step counter:  90801
dev_network_count:  303
learn step counter:  90851
dev_network_count:  303

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
304  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  90901
dev_network_count:  304
learn step counter:  90951
dev_network_count:  304
EPOCH %d 94
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1849 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1850 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1851 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1852 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1853 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1854 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1855 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  91001
dev_network_count:  304
learn step counter:  91051
dev_network_count:  304
learn step counter:  91101
dev_network_count:  304
learn step counter:  91151
dev_network_count:  304

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
305  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  91201
dev_network_count:  305
learn step counter:  91251
dev_network_count:  305
learn step counter:  91301
dev_network_count:  305
learn step counter:  91351
dev_network_count:  305
learn step counter:  91401
dev_network_count:  305
learn step counter:  91451
dev_network_count:  305

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
306  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  91501
dev_network_count:  306
learn step counter:  91551
dev_network_count:  306
learn step counter:  91601
dev_network_count:  306
learn step counter:  91651
dev_network_count:  306
learn step counter:  91701
dev_network_count:  306
learn step counter:  91751
dev_network_count:  306

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
307  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  91801
dev_network_count:  307
learn step counter:  91851
dev_network_count:  307
learn step counter:  91901
dev_network_count:  307
learn step counter:  91951
dev_network_count:  307
EPOCH %d 95
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
4342 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4343 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4344 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4345 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4346 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4347 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4348 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  92001
dev_network_count:  307
learn step counter:  92051
dev_network_count:  307

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
308  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  92101
dev_network_count:  308
learn step counter:  92151
dev_network_count:  308
learn step counter:  92201
dev_network_count:  308
learn step counter:  92251
dev_network_count:  308
learn step counter:  92301
dev_network_count:  308
learn step counter:  92351
dev_network_count:  308

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
309  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  92401
dev_network_count:  309
learn step counter:  92451
dev_network_count:  309
learn step counter:  92501
dev_network_count:  309
learn step counter:  92551
dev_network_count:  309
learn step counter:  92601
dev_network_count:  309
learn step counter:  92651
dev_network_count:  309

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
310  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  92701
dev_network_count:  310
learn step counter:  92751
dev_network_count:  310
learn step counter:  92801
dev_network_count:  310
learn step counter:  92851
dev_network_count:  310
learn step counter:  92901
dev_network_count:  310
learn step counter:  92951
dev_network_count:  310
EPOCH %d 96
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1835 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1836 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1837 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1838 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1839 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1840 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1841 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
311  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  93001
dev_network_count:  311
learn step counter:  93051
dev_network_count:  311
learn step counter:  93101
dev_network_count:  311
learn step counter:  93151
dev_network_count:  311
learn step counter:  93201
dev_network_count:  311
learn step counter:  93251
dev_network_count:  311

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
312  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  93301
dev_network_count:  312
learn step counter:  93351
dev_network_count:  312
learn step counter:  93401
dev_network_count:  312
learn step counter:  93451
dev_network_count:  312
learn step counter:  93501
dev_network_count:  312
learn step counter:  93551
dev_network_count:  312

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
313  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  93601
dev_network_count:  313
learn step counter:  93651
dev_network_count:  313
learn step counter:  93701
dev_network_count:  313
learn step counter:  93751
dev_network_count:  313
learn step counter:  93801
dev_network_count:  313
learn step counter:  93851
dev_network_count:  313

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
314  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  93901
dev_network_count:  314
learn step counter:  93951
dev_network_count:  314
EPOCH %d 97
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
4328 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4329 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4330 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4331 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4332 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4333 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4334 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  94001
dev_network_count:  314
learn step counter:  94051
dev_network_count:  314
learn step counter:  94101
dev_network_count:  314
learn step counter:  94151
dev_network_count:  314

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
315  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  94201
dev_network_count:  315
learn step counter:  94251
dev_network_count:  315
learn step counter:  94301
dev_network_count:  315
learn step counter:  94351
dev_network_count:  315
learn step counter:  94401
dev_network_count:  315
learn step counter:  94451
dev_network_count:  315

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
316  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  94501
dev_network_count:  316
learn step counter:  94551
dev_network_count:  316
learn step counter:  94601
dev_network_count:  316
learn step counter:  94651
dev_network_count:  316
learn step counter:  94701
dev_network_count:  316
learn step counter:  94751
dev_network_count:  316

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
317  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  94801
dev_network_count:  317
learn step counter:  94851
dev_network_count:  317
learn step counter:  94901
dev_network_count:  317
learn step counter:  94951
dev_network_count:  317
EPOCH %d 98
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1821 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1822 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1823 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1824 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1825 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1826 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1827 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  95001
dev_network_count:  317
learn step counter:  95051
dev_network_count:  317

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
318  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  95101
dev_network_count:  318
learn step counter:  95151
dev_network_count:  318
learn step counter:  95201
dev_network_count:  318
learn step counter:  95251
dev_network_count:  318
learn step counter:  95301
dev_network_count:  318
learn step counter:  95351
dev_network_count:  318

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
319  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  95401
dev_network_count:  319
learn step counter:  95451
dev_network_count:  319
learn step counter:  95501
dev_network_count:  319
learn step counter:  95551
dev_network_count:  319
learn step counter:  95601
dev_network_count:  319
learn step counter:  95651
dev_network_count:  319

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
320  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  95701
dev_network_count:  320
learn step counter:  95751
dev_network_count:  320
learn step counter:  95801
dev_network_count:  320
learn step counter:  95851
dev_network_count:  320
learn step counter:  95901
dev_network_count:  320
learn step counter:  95951
dev_network_count:  320
EPOCH %d 99
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
4314 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4315 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4316 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4317 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4318 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4319 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4320 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
321  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  96001
dev_network_count:  321
learn step counter:  96051
dev_network_count:  321
learn step counter:  96101
dev_network_count:  321
learn step counter:  96151
dev_network_count:  321
learn step counter:  96201
dev_network_count:  321
learn step counter:  96251
dev_network_count:  321

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
322  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  96301
dev_network_count:  322
learn step counter:  96351
dev_network_count:  322
learn step counter:  96401
dev_network_count:  322
learn step counter:  96451
dev_network_count:  322
learn step counter:  96501
dev_network_count:  322
learn step counter:  96551
dev_network_count:  322

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
323  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  96601
dev_network_count:  323
learn step counter:  96651
dev_network_count:  323
learn step counter:  96701
dev_network_count:  323
learn step counter:  96751
dev_network_count:  323
learn step counter:  96801
dev_network_count:  323
learn step counter:  96851
dev_network_count:  323

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
324  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  96901
dev_network_count:  324
learn step counter:  96951
dev_network_count:  324
EPOCH %d 100
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1807 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1808 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1809 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1810 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1811 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1812 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1813 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  97001
dev_network_count:  324
learn step counter:  97051
dev_network_count:  324
learn step counter:  97101
dev_network_count:  324
learn step counter:  97151
dev_network_count:  324

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
325  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  97201
dev_network_count:  325
learn step counter:  97251
dev_network_count:  325
learn step counter:  97301
dev_network_count:  325
learn step counter:  97351
dev_network_count:  325
learn step counter:  97401
dev_network_count:  325
learn step counter:  97451
dev_network_count:  325

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
326  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  97501
dev_network_count:  326
learn step counter:  97551
dev_network_count:  326
learn step counter:  97601
dev_network_count:  326
learn step counter:  97651
dev_network_count:  326
learn step counter:  97701
dev_network_count:  326
learn step counter:  97751
dev_network_count:  326

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
327  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  97801
dev_network_count:  327
learn step counter:  97851
dev_network_count:  327
learn step counter:  97901
dev_network_count:  327
learn step counter:  97951
dev_network_count:  327
EPOCH %d 101
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
4300 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4301 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4302 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4303 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4304 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4305 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4306 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  98001
dev_network_count:  327
learn step counter:  98051
dev_network_count:  327

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
328  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  98101
dev_network_count:  328
learn step counter:  98151
dev_network_count:  328
learn step counter:  98201
dev_network_count:  328
learn step counter:  98251
dev_network_count:  328
learn step counter:  98301
dev_network_count:  328
learn step counter:  98351
dev_network_count:  328

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
329  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  98401
dev_network_count:  329
learn step counter:  98451
dev_network_count:  329
learn step counter:  98501
dev_network_count:  329
learn step counter:  98551
dev_network_count:  329
learn step counter:  98601
dev_network_count:  329
learn step counter:  98651
dev_network_count:  329

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
330  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  98701
dev_network_count:  330
learn step counter:  98751
dev_network_count:  330
learn step counter:  98801
dev_network_count:  330
learn step counter:  98851
dev_network_count:  330
learn step counter:  98901
dev_network_count:  330
learn step counter:  98951
dev_network_count:  330
EPOCH %d 102
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1793 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1794 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1795 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1796 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1797 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1798 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1799 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
331  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  99001
dev_network_count:  331
learn step counter:  99051
dev_network_count:  331
learn step counter:  99101
dev_network_count:  331
learn step counter:  99151
dev_network_count:  331
learn step counter:  99201
dev_network_count:  331
learn step counter:  99251
dev_network_count:  331

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
332  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  99301
dev_network_count:  332
learn step counter:  99351
dev_network_count:  332
learn step counter:  99401
dev_network_count:  332
learn step counter:  99451
dev_network_count:  332
learn step counter:  99501
dev_network_count:  332
learn step counter:  99551
dev_network_count:  332

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
333  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  99601
dev_network_count:  333
learn step counter:  99651
dev_network_count:  333
learn step counter:  99701
dev_network_count:  333
learn step counter:  99751
dev_network_count:  333
learn step counter:  99801
dev_network_count:  333
learn step counter:  99851
dev_network_count:  333

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
334  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  99901
dev_network_count:  334
learn step counter:  99951
dev_network_count:  334
EPOCH %d 103
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
4286 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4287 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4288 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4289 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4290 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4291 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4292 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  100001
dev_network_count:  334
learn step counter:  100051
dev_network_count:  334
learn step counter:  100101
dev_network_count:  334
learn step counter:  100151
dev_network_count:  334

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
335  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  100201
dev_network_count:  335
learn step counter:  100251
dev_network_count:  335
learn step counter:  100301
dev_network_count:  335
learn step counter:  100351
dev_network_count:  335
learn step counter:  100401
dev_network_count:  335
learn step counter:  100451
dev_network_count:  335

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
336  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  100501
dev_network_count:  336
learn step counter:  100551
dev_network_count:  336
learn step counter:  100601
dev_network_count:  336
learn step counter:  100651
dev_network_count:  336
learn step counter:  100701
dev_network_count:  336
learn step counter:  100751
dev_network_count:  336

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
337  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  100801
dev_network_count:  337
learn step counter:  100851
dev_network_count:  337
learn step counter:  100901
dev_network_count:  337
learn step counter:  100951
dev_network_count:  337
EPOCH %d 104
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1779 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1780 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1781 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1782 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1783 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1784 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1785 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  101001
dev_network_count:  337
learn step counter:  101051
dev_network_count:  337

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
338  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  101101
dev_network_count:  338
learn step counter:  101151
dev_network_count:  338
learn step counter:  101201
dev_network_count:  338
learn step counter:  101251
dev_network_count:  338
learn step counter:  101301
dev_network_count:  338
learn step counter:  101351
dev_network_count:  338

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
339  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  101401
dev_network_count:  339
learn step counter:  101451
dev_network_count:  339
learn step counter:  101501
dev_network_count:  339
learn step counter:  101551
dev_network_count:  339
learn step counter:  101601
dev_network_count:  339
learn step counter:  101651
dev_network_count:  339

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
340  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  101701
dev_network_count:  340
learn step counter:  101751
dev_network_count:  340
learn step counter:  101801
dev_network_count:  340
learn step counter:  101851
dev_network_count:  340
learn step counter:  101901
dev_network_count:  340
learn step counter:  101951
dev_network_count:  340
EPOCH %d 105
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
4272 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4273 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4274 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4275 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4276 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4277 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4278 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
341  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  102001
dev_network_count:  341
learn step counter:  102051
dev_network_count:  341
learn step counter:  102101
dev_network_count:  341
learn step counter:  102151
dev_network_count:  341
learn step counter:  102201
dev_network_count:  341
learn step counter:  102251
dev_network_count:  341

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
342  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  102301
dev_network_count:  342
learn step counter:  102351
dev_network_count:  342
learn step counter:  102401
dev_network_count:  342
learn step counter:  102451
dev_network_count:  342
learn step counter:  102501
dev_network_count:  342
learn step counter:  102551
dev_network_count:  342

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
343  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  102601
dev_network_count:  343
learn step counter:  102651
dev_network_count:  343
learn step counter:  102701
dev_network_count:  343
learn step counter:  102751
dev_network_count:  343
learn step counter:  102801
dev_network_count:  343
learn step counter:  102851
dev_network_count:  343

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
344  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  102901
dev_network_count:  344
learn step counter:  102951
dev_network_count:  344
EPOCH %d 106
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1765 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1766 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1767 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1768 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1769 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1770 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1771 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  103001
dev_network_count:  344
learn step counter:  103051
dev_network_count:  344
learn step counter:  103101
dev_network_count:  344
learn step counter:  103151
dev_network_count:  344

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
345  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  103201
dev_network_count:  345
learn step counter:  103251
dev_network_count:  345
learn step counter:  103301
dev_network_count:  345
learn step counter:  103351
dev_network_count:  345
learn step counter:  103401
dev_network_count:  345
learn step counter:  103451
dev_network_count:  345

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
346  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  103501
dev_network_count:  346
learn step counter:  103551
dev_network_count:  346
learn step counter:  103601
dev_network_count:  346
learn step counter:  103651
dev_network_count:  346
learn step counter:  103701
dev_network_count:  346
learn step counter:  103751
dev_network_count:  346

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
347  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  103801
dev_network_count:  347
learn step counter:  103851
dev_network_count:  347
learn step counter:  103901
dev_network_count:  347
learn step counter:  103951
dev_network_count:  347
EPOCH %d 107
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
4258 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4259 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4260 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4261 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4262 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4263 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4264 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  104001
dev_network_count:  347
learn step counter:  104051
dev_network_count:  347

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
348  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  104101
dev_network_count:  348
learn step counter:  104151
dev_network_count:  348
learn step counter:  104201
dev_network_count:  348
learn step counter:  104251
dev_network_count:  348
learn step counter:  104301
dev_network_count:  348
learn step counter:  104351
dev_network_count:  348

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
349  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  104401
dev_network_count:  349
learn step counter:  104451
dev_network_count:  349
learn step counter:  104501
dev_network_count:  349
learn step counter:  104551
dev_network_count:  349
learn step counter:  104601
dev_network_count:  349
learn step counter:  104651
dev_network_count:  349

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
350  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  104701
dev_network_count:  350
learn step counter:  104751
dev_network_count:  350
learn step counter:  104801
dev_network_count:  350
learn step counter:  104851
dev_network_count:  350
learn step counter:  104901
dev_network_count:  350
learn step counter:  104951
dev_network_count:  350
EPOCH %d 108
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1751 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1752 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1753 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1754 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1755 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1756 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1757 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
351  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  105001
dev_network_count:  351
learn step counter:  105051
dev_network_count:  351
learn step counter:  105101
dev_network_count:  351
learn step counter:  105151
dev_network_count:  351
learn step counter:  105201
dev_network_count:  351
learn step counter:  105251
dev_network_count:  351

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
352  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  105301
dev_network_count:  352
learn step counter:  105351
dev_network_count:  352
learn step counter:  105401
dev_network_count:  352
learn step counter:  105451
dev_network_count:  352
learn step counter:  105501
dev_network_count:  352
learn step counter:  105551
dev_network_count:  352

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
353  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  105601
dev_network_count:  353
learn step counter:  105651
dev_network_count:  353
learn step counter:  105701
dev_network_count:  353
learn step counter:  105751
dev_network_count:  353
learn step counter:  105801
dev_network_count:  353
learn step counter:  105851
dev_network_count:  353

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
354  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  105901
dev_network_count:  354
learn step counter:  105951
dev_network_count:  354
EPOCH %d 109
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
4244 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4245 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4246 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4247 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4248 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4249 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4250 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  106001
dev_network_count:  354
learn step counter:  106051
dev_network_count:  354
learn step counter:  106101
dev_network_count:  354
learn step counter:  106151
dev_network_count:  354

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
355  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  106201
dev_network_count:  355
learn step counter:  106251
dev_network_count:  355
learn step counter:  106301
dev_network_count:  355
learn step counter:  106351
dev_network_count:  355
learn step counter:  106401
dev_network_count:  355
learn step counter:  106451
dev_network_count:  355

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
356  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  106501
dev_network_count:  356
learn step counter:  106551
dev_network_count:  356
learn step counter:  106601
dev_network_count:  356
learn step counter:  106651
dev_network_count:  356
learn step counter:  106701
dev_network_count:  356
learn step counter:  106751
dev_network_count:  356

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
357  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  106801
dev_network_count:  357
learn step counter:  106851
dev_network_count:  357
learn step counter:  106901
dev_network_count:  357
learn step counter:  106951
dev_network_count:  357
EPOCH %d 110
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1737 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1738 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1739 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1740 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1741 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1742 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1743 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  107001
dev_network_count:  357
learn step counter:  107051
dev_network_count:  357

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
358  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  107101
dev_network_count:  358
learn step counter:  107151
dev_network_count:  358
learn step counter:  107201
dev_network_count:  358
learn step counter:  107251
dev_network_count:  358
learn step counter:  107301
dev_network_count:  358
learn step counter:  107351
dev_network_count:  358

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
359  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  107401
dev_network_count:  359
learn step counter:  107451
dev_network_count:  359
learn step counter:  107501
dev_network_count:  359
learn step counter:  107551
dev_network_count:  359
learn step counter:  107601
dev_network_count:  359
learn step counter:  107651
dev_network_count:  359

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
360  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  107701
dev_network_count:  360
learn step counter:  107751
dev_network_count:  360
learn step counter:  107801
dev_network_count:  360
learn step counter:  107851
dev_network_count:  360
learn step counter:  107901
dev_network_count:  360
learn step counter:  107951
dev_network_count:  360
EPOCH %d 111
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
4230 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4231 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4232 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4233 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4234 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4235 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4236 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
361  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  108001
dev_network_count:  361
learn step counter:  108051
dev_network_count:  361
learn step counter:  108101
dev_network_count:  361
learn step counter:  108151
dev_network_count:  361
learn step counter:  108201
dev_network_count:  361
learn step counter:  108251
dev_network_count:  361

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
362  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  108301
dev_network_count:  362
learn step counter:  108351
dev_network_count:  362
learn step counter:  108401
dev_network_count:  362
learn step counter:  108451
dev_network_count:  362
learn step counter:  108501
dev_network_count:  362
learn step counter:  108551
dev_network_count:  362

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
363  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  108601
dev_network_count:  363
learn step counter:  108651
dev_network_count:  363
learn step counter:  108701
dev_network_count:  363
learn step counter:  108751
dev_network_count:  363
learn step counter:  108801
dev_network_count:  363
learn step counter:  108851
dev_network_count:  363

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
364  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  108901
dev_network_count:  364
learn step counter:  108951
dev_network_count:  364
EPOCH %d 112
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1723 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1724 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1725 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1726 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1727 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1728 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1729 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  109001
dev_network_count:  364
learn step counter:  109051
dev_network_count:  364
learn step counter:  109101
dev_network_count:  364
learn step counter:  109151
dev_network_count:  364

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
365  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  109201
dev_network_count:  365
learn step counter:  109251
dev_network_count:  365
learn step counter:  109301
dev_network_count:  365
learn step counter:  109351
dev_network_count:  365
learn step counter:  109401
dev_network_count:  365
learn step counter:  109451
dev_network_count:  365

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
366  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  109501
dev_network_count:  366
learn step counter:  109551
dev_network_count:  366
learn step counter:  109601
dev_network_count:  366
learn step counter:  109651
dev_network_count:  366
learn step counter:  109701
dev_network_count:  366
learn step counter:  109751
dev_network_count:  366

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
367  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  109801
dev_network_count:  367
learn step counter:  109851
dev_network_count:  367
learn step counter:  109901
dev_network_count:  367
learn step counter:  109951
dev_network_count:  367
EPOCH %d 113
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
4216 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4217 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4218 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4219 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4220 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4221 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4222 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  110001
dev_network_count:  367
learn step counter:  110051
dev_network_count:  367

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
368  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  110101
dev_network_count:  368
learn step counter:  110151
dev_network_count:  368
learn step counter:  110201
dev_network_count:  368
learn step counter:  110251
dev_network_count:  368
learn step counter:  110301
dev_network_count:  368
learn step counter:  110351
dev_network_count:  368

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
369  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  110401
dev_network_count:  369
learn step counter:  110451
dev_network_count:  369
learn step counter:  110501
dev_network_count:  369
learn step counter:  110551
dev_network_count:  369
learn step counter:  110601
dev_network_count:  369
learn step counter:  110651
dev_network_count:  369

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
370  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  110701
dev_network_count:  370
learn step counter:  110751
dev_network_count:  370
learn step counter:  110801
dev_network_count:  370
learn step counter:  110851
dev_network_count:  370
learn step counter:  110901
dev_network_count:  370
learn step counter:  110951
dev_network_count:  370
EPOCH %d 114
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1709 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1710 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1711 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1712 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1713 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1714 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1715 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
371  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  111001
dev_network_count:  371
learn step counter:  111051
dev_network_count:  371
learn step counter:  111101
dev_network_count:  371
learn step counter:  111151
dev_network_count:  371
learn step counter:  111201
dev_network_count:  371
learn step counter:  111251
dev_network_count:  371

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
372  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  111301
dev_network_count:  372
learn step counter:  111351
dev_network_count:  372
learn step counter:  111401
dev_network_count:  372
learn step counter:  111451
dev_network_count:  372
learn step counter:  111501
dev_network_count:  372
learn step counter:  111551
dev_network_count:  372

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
373  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  111601
dev_network_count:  373
learn step counter:  111651
dev_network_count:  373
learn step counter:  111701
dev_network_count:  373
learn step counter:  111751
dev_network_count:  373
learn step counter:  111801
dev_network_count:  373
learn step counter:  111851
dev_network_count:  373

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
374  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  111901
dev_network_count:  374
learn step counter:  111951
dev_network_count:  374
EPOCH %d 115
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
4202 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4203 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4204 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4205 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4206 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4207 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4208 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  112001
dev_network_count:  374
learn step counter:  112051
dev_network_count:  374
learn step counter:  112101
dev_network_count:  374
learn step counter:  112151
dev_network_count:  374

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
375  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  112201
dev_network_count:  375
learn step counter:  112251
dev_network_count:  375
learn step counter:  112301
dev_network_count:  375
learn step counter:  112351
dev_network_count:  375
learn step counter:  112401
dev_network_count:  375
learn step counter:  112451
dev_network_count:  375

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
376  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  112501
dev_network_count:  376
learn step counter:  112551
dev_network_count:  376
learn step counter:  112601
dev_network_count:  376
learn step counter:  112651
dev_network_count:  376
learn step counter:  112701
dev_network_count:  376
learn step counter:  112751
dev_network_count:  376

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
377  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  112801
dev_network_count:  377
learn step counter:  112851
dev_network_count:  377
learn step counter:  112901
dev_network_count:  377
learn step counter:  112951
dev_network_count:  377
EPOCH %d 116
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1695 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1696 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1697 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1698 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1699 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1700 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1701 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  113001
dev_network_count:  377
learn step counter:  113051
dev_network_count:  377

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
378  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  113101
dev_network_count:  378
learn step counter:  113151
dev_network_count:  378
learn step counter:  113201
dev_network_count:  378
learn step counter:  113251
dev_network_count:  378
learn step counter:  113301
dev_network_count:  378
learn step counter:  113351
dev_network_count:  378

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
379  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  113401
dev_network_count:  379
learn step counter:  113451
dev_network_count:  379
learn step counter:  113501
dev_network_count:  379
learn step counter:  113551
dev_network_count:  379
learn step counter:  113601
dev_network_count:  379
learn step counter:  113651
dev_network_count:  379

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
380  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  113701
dev_network_count:  380
learn step counter:  113751
dev_network_count:  380
learn step counter:  113801
dev_network_count:  380
learn step counter:  113851
dev_network_count:  380
learn step counter:  113901
dev_network_count:  380
learn step counter:  113951
dev_network_count:  380
EPOCH %d 117
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
4188 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4189 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4190 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4191 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4192 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4193 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4194 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
381  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  114001
dev_network_count:  381
learn step counter:  114051
dev_network_count:  381
learn step counter:  114101
dev_network_count:  381
learn step counter:  114151
dev_network_count:  381
learn step counter:  114201
dev_network_count:  381
learn step counter:  114251
dev_network_count:  381

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
382  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  114301
dev_network_count:  382
learn step counter:  114351
dev_network_count:  382
learn step counter:  114401
dev_network_count:  382
learn step counter:  114451
dev_network_count:  382
learn step counter:  114501
dev_network_count:  382
learn step counter:  114551
dev_network_count:  382

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
383  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  114601
dev_network_count:  383
learn step counter:  114651
dev_network_count:  383
learn step counter:  114701
dev_network_count:  383
learn step counter:  114751
dev_network_count:  383
learn step counter:  114801
dev_network_count:  383
learn step counter:  114851
dev_network_count:  383

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
384  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  114901
dev_network_count:  384
learn step counter:  114951
dev_network_count:  384
EPOCH %d 118
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1681 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1682 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1683 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1684 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1685 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1686 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1687 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  115001
dev_network_count:  384
learn step counter:  115051
dev_network_count:  384
learn step counter:  115101
dev_network_count:  384
learn step counter:  115151
dev_network_count:  384

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
385  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  115201
dev_network_count:  385
learn step counter:  115251
dev_network_count:  385
learn step counter:  115301
dev_network_count:  385
learn step counter:  115351
dev_network_count:  385
learn step counter:  115401
dev_network_count:  385
learn step counter:  115451
dev_network_count:  385

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
386  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  115501
dev_network_count:  386
learn step counter:  115551
dev_network_count:  386
learn step counter:  115601
dev_network_count:  386
learn step counter:  115651
dev_network_count:  386
learn step counter:  115701
dev_network_count:  386
learn step counter:  115751
dev_network_count:  386

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
387  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  115801
dev_network_count:  387
learn step counter:  115851
dev_network_count:  387
learn step counter:  115901
dev_network_count:  387
learn step counter:  115951
dev_network_count:  387
EPOCH %d 119
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
4174 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4175 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4176 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4177 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4178 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4179 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4180 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  116001
dev_network_count:  387
learn step counter:  116051
dev_network_count:  387

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
388  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  116101
dev_network_count:  388
learn step counter:  116151
dev_network_count:  388
learn step counter:  116201
dev_network_count:  388
learn step counter:  116251
dev_network_count:  388
learn step counter:  116301
dev_network_count:  388
learn step counter:  116351
dev_network_count:  388

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
389  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  116401
dev_network_count:  389
learn step counter:  116451
dev_network_count:  389
learn step counter:  116501
dev_network_count:  389
learn step counter:  116551
dev_network_count:  389
learn step counter:  116601
dev_network_count:  389
learn step counter:  116651
dev_network_count:  389

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
390  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  116701
dev_network_count:  390
learn step counter:  116751
dev_network_count:  390
learn step counter:  116801
dev_network_count:  390
learn step counter:  116851
dev_network_count:  390
learn step counter:  116901
dev_network_count:  390
learn step counter:  116951
dev_network_count:  390
EPOCH %d 120
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1667 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1668 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1669 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1670 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1671 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1672 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1673 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
391  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  117001
dev_network_count:  391
learn step counter:  117051
dev_network_count:  391
learn step counter:  117101
dev_network_count:  391
learn step counter:  117151
dev_network_count:  391
learn step counter:  117201
dev_network_count:  391
learn step counter:  117251
dev_network_count:  391

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
392  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  117301
dev_network_count:  392
learn step counter:  117351
dev_network_count:  392
learn step counter:  117401
dev_network_count:  392
learn step counter:  117451
dev_network_count:  392
learn step counter:  117501
dev_network_count:  392
learn step counter:  117551
dev_network_count:  392

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
393  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  117601
dev_network_count:  393
learn step counter:  117651
dev_network_count:  393
learn step counter:  117701
dev_network_count:  393
learn step counter:  117751
dev_network_count:  393
learn step counter:  117801
dev_network_count:  393
learn step counter:  117851
dev_network_count:  393

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
394  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  117901
dev_network_count:  394
learn step counter:  117951
dev_network_count:  394
EPOCH %d 121
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
4160 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4161 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4162 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4163 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4164 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4165 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4166 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  118001
dev_network_count:  394
learn step counter:  118051
dev_network_count:  394
learn step counter:  118101
dev_network_count:  394
learn step counter:  118151
dev_network_count:  394

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
395  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  118201
dev_network_count:  395
learn step counter:  118251
dev_network_count:  395
learn step counter:  118301
dev_network_count:  395
learn step counter:  118351
dev_network_count:  395
learn step counter:  118401
dev_network_count:  395
learn step counter:  118451
dev_network_count:  395

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
396  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  118501
dev_network_count:  396
learn step counter:  118551
dev_network_count:  396
learn step counter:  118601
dev_network_count:  396
learn step counter:  118651
dev_network_count:  396
learn step counter:  118701
dev_network_count:  396
learn step counter:  118751
dev_network_count:  396

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
397  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  118801
dev_network_count:  397
learn step counter:  118851
dev_network_count:  397
learn step counter:  118901
dev_network_count:  397
learn step counter:  118951
dev_network_count:  397
EPOCH %d 122
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1653 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1654 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1655 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1656 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1657 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1658 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1659 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  119001
dev_network_count:  397
learn step counter:  119051
dev_network_count:  397

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
398  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  119101
dev_network_count:  398
learn step counter:  119151
dev_network_count:  398
learn step counter:  119201
dev_network_count:  398
learn step counter:  119251
dev_network_count:  398
learn step counter:  119301
dev_network_count:  398
learn step counter:  119351
dev_network_count:  398

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
399  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  119401
dev_network_count:  399
learn step counter:  119451
dev_network_count:  399
learn step counter:  119501
dev_network_count:  399
learn step counter:  119551
dev_network_count:  399
learn step counter:  119601
dev_network_count:  399
learn step counter:  119651
dev_network_count:  399

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
400  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  119701
dev_network_count:  400
learn step counter:  119751
dev_network_count:  400
learn step counter:  119801
dev_network_count:  400
learn step counter:  119851
dev_network_count:  400
learn step counter:  119901
dev_network_count:  400
learn step counter:  119951
dev_network_count:  400
EPOCH %d 123
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
4146 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4147 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4148 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4149 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4150 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4151 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4152 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
401  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  120001
dev_network_count:  401
learn step counter:  120051
dev_network_count:  401
learn step counter:  120101
dev_network_count:  401
learn step counter:  120151
dev_network_count:  401
learn step counter:  120201
dev_network_count:  401
learn step counter:  120251
dev_network_count:  401

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
402  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  120301
dev_network_count:  402
learn step counter:  120351
dev_network_count:  402
learn step counter:  120401
dev_network_count:  402
learn step counter:  120451
dev_network_count:  402
learn step counter:  120501
dev_network_count:  402
learn step counter:  120551
dev_network_count:  402

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
403  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  120601
dev_network_count:  403
learn step counter:  120651
dev_network_count:  403
learn step counter:  120701
dev_network_count:  403
learn step counter:  120751
dev_network_count:  403
learn step counter:  120801
dev_network_count:  403
learn step counter:  120851
dev_network_count:  403

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
404  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  120901
dev_network_count:  404
learn step counter:  120951
dev_network_count:  404
EPOCH %d 124
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1639 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1640 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1641 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1642 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1643 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1644 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1645 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  121001
dev_network_count:  404
learn step counter:  121051
dev_network_count:  404
learn step counter:  121101
dev_network_count:  404
learn step counter:  121151
dev_network_count:  404

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
405  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  121201
dev_network_count:  405
learn step counter:  121251
dev_network_count:  405
learn step counter:  121301
dev_network_count:  405
learn step counter:  121351
dev_network_count:  405
learn step counter:  121401
dev_network_count:  405
learn step counter:  121451
dev_network_count:  405

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
406  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  121501
dev_network_count:  406
learn step counter:  121551
dev_network_count:  406
learn step counter:  121601
dev_network_count:  406
learn step counter:  121651
dev_network_count:  406
learn step counter:  121701
dev_network_count:  406
learn step counter:  121751
dev_network_count:  406

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
407  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  121801
dev_network_count:  407
learn step counter:  121851
dev_network_count:  407
learn step counter:  121901
dev_network_count:  407
learn step counter:  121951
dev_network_count:  407
EPOCH %d 125
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
4132 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4133 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4134 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4135 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4136 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4137 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4138 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  122001
dev_network_count:  407
learn step counter:  122051
dev_network_count:  407

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
408  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  122101
dev_network_count:  408
learn step counter:  122151
dev_network_count:  408
learn step counter:  122201
dev_network_count:  408
learn step counter:  122251
dev_network_count:  408
learn step counter:  122301
dev_network_count:  408
learn step counter:  122351
dev_network_count:  408

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
409  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  122401
dev_network_count:  409
learn step counter:  122451
dev_network_count:  409
learn step counter:  122501
dev_network_count:  409
learn step counter:  122551
dev_network_count:  409
learn step counter:  122601
dev_network_count:  409
learn step counter:  122651
dev_network_count:  409

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
410  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  122701
dev_network_count:  410
learn step counter:  122751
dev_network_count:  410
learn step counter:  122801
dev_network_count:  410
learn step counter:  122851
dev_network_count:  410
learn step counter:  122901
dev_network_count:  410
learn step counter:  122951
dev_network_count:  410
EPOCH %d 126
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1625 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1626 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1627 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1628 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1629 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1630 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1631 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
411  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  123001
dev_network_count:  411
learn step counter:  123051
dev_network_count:  411
learn step counter:  123101
dev_network_count:  411
learn step counter:  123151
dev_network_count:  411
learn step counter:  123201
dev_network_count:  411
learn step counter:  123251
dev_network_count:  411

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
412  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  123301
dev_network_count:  412
learn step counter:  123351
dev_network_count:  412
learn step counter:  123401
dev_network_count:  412
learn step counter:  123451
dev_network_count:  412
learn step counter:  123501
dev_network_count:  412
learn step counter:  123551
dev_network_count:  412

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
413  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  123601
dev_network_count:  413
learn step counter:  123651
dev_network_count:  413
learn step counter:  123701
dev_network_count:  413
learn step counter:  123751
dev_network_count:  413
learn step counter:  123801
dev_network_count:  413
learn step counter:  123851
dev_network_count:  413

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
414  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  123901
dev_network_count:  414
learn step counter:  123951
dev_network_count:  414
EPOCH %d 127
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
4118 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4119 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4120 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4121 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4122 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4123 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4124 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  124001
dev_network_count:  414
learn step counter:  124051
dev_network_count:  414
learn step counter:  124101
dev_network_count:  414
learn step counter:  124151
dev_network_count:  414

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
415  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  124201
dev_network_count:  415
learn step counter:  124251
dev_network_count:  415
learn step counter:  124301
dev_network_count:  415
learn step counter:  124351
dev_network_count:  415
learn step counter:  124401
dev_network_count:  415
learn step counter:  124451
dev_network_count:  415

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
416  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  124501
dev_network_count:  416
learn step counter:  124551
dev_network_count:  416
learn step counter:  124601
dev_network_count:  416
learn step counter:  124651
dev_network_count:  416
learn step counter:  124701
dev_network_count:  416
learn step counter:  124751
dev_network_count:  416

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
417  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  124801
dev_network_count:  417
learn step counter:  124851
dev_network_count:  417
learn step counter:  124901
dev_network_count:  417
learn step counter:  124951
dev_network_count:  417
EPOCH %d 128
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1611 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1612 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1613 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1614 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1615 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1616 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1617 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  125001
dev_network_count:  417
learn step counter:  125051
dev_network_count:  417

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
418  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  125101
dev_network_count:  418
learn step counter:  125151
dev_network_count:  418
learn step counter:  125201
dev_network_count:  418
learn step counter:  125251
dev_network_count:  418
learn step counter:  125301
dev_network_count:  418
learn step counter:  125351
dev_network_count:  418

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
419  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  125401
dev_network_count:  419
learn step counter:  125451
dev_network_count:  419
learn step counter:  125501
dev_network_count:  419
learn step counter:  125551
dev_network_count:  419
learn step counter:  125601
dev_network_count:  419
learn step counter:  125651
dev_network_count:  419

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
420  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  125701
dev_network_count:  420
learn step counter:  125751
dev_network_count:  420
learn step counter:  125801
dev_network_count:  420
learn step counter:  125851
dev_network_count:  420
learn step counter:  125901
dev_network_count:  420
learn step counter:  125951
dev_network_count:  420
EPOCH %d 129
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
4104 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4105 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4106 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4107 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4108 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4109 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4110 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
421  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  126001
dev_network_count:  421
learn step counter:  126051
dev_network_count:  421
learn step counter:  126101
dev_network_count:  421
learn step counter:  126151
dev_network_count:  421
learn step counter:  126201
dev_network_count:  421
learn step counter:  126251
dev_network_count:  421

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
422  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  126301
dev_network_count:  422
learn step counter:  126351
dev_network_count:  422
learn step counter:  126401
dev_network_count:  422
learn step counter:  126451
dev_network_count:  422
learn step counter:  126501
dev_network_count:  422
learn step counter:  126551
dev_network_count:  422

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
423  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  126601
dev_network_count:  423
learn step counter:  126651
dev_network_count:  423
learn step counter:  126701
dev_network_count:  423
learn step counter:  126751
dev_network_count:  423
learn step counter:  126801
dev_network_count:  423
learn step counter:  126851
dev_network_count:  423

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
424  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  126901
dev_network_count:  424
learn step counter:  126951
dev_network_count:  424
EPOCH %d 130
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1597 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1598 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1599 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1600 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1601 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1602 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1603 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  127001
dev_network_count:  424
learn step counter:  127051
dev_network_count:  424
learn step counter:  127101
dev_network_count:  424
learn step counter:  127151
dev_network_count:  424

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
425  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  127201
dev_network_count:  425
learn step counter:  127251
dev_network_count:  425
learn step counter:  127301
dev_network_count:  425
learn step counter:  127351
dev_network_count:  425
learn step counter:  127401
dev_network_count:  425
learn step counter:  127451
dev_network_count:  425

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
426  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  127501
dev_network_count:  426
learn step counter:  127551
dev_network_count:  426
learn step counter:  127601
dev_network_count:  426
learn step counter:  127651
dev_network_count:  426
learn step counter:  127701
dev_network_count:  426
learn step counter:  127751
dev_network_count:  426

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
427  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  127801
dev_network_count:  427
learn step counter:  127851
dev_network_count:  427
learn step counter:  127901
dev_network_count:  427
learn step counter:  127951
dev_network_count:  427
EPOCH %d 131
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
4090 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4091 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4092 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4093 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4094 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4095 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4096 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  128001
dev_network_count:  427
learn step counter:  128051
dev_network_count:  427

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
428  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  128101
dev_network_count:  428
learn step counter:  128151
dev_network_count:  428
learn step counter:  128201
dev_network_count:  428
learn step counter:  128251
dev_network_count:  428
learn step counter:  128301
dev_network_count:  428
learn step counter:  128351
dev_network_count:  428

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
429  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  128401
dev_network_count:  429
learn step counter:  128451
dev_network_count:  429
learn step counter:  128501
dev_network_count:  429
learn step counter:  128551
dev_network_count:  429
learn step counter:  128601
dev_network_count:  429
learn step counter:  128651
dev_network_count:  429

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
430  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  128701
dev_network_count:  430
learn step counter:  128751
dev_network_count:  430
learn step counter:  128801
dev_network_count:  430
learn step counter:  128851
dev_network_count:  430
learn step counter:  128901
dev_network_count:  430
learn step counter:  128951
dev_network_count:  430
EPOCH %d 132
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1583 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1584 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1585 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1586 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1587 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1588 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1589 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
431  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  129001
dev_network_count:  431
learn step counter:  129051
dev_network_count:  431
learn step counter:  129101
dev_network_count:  431
learn step counter:  129151
dev_network_count:  431
learn step counter:  129201
dev_network_count:  431
learn step counter:  129251
dev_network_count:  431

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
432  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  129301
dev_network_count:  432
learn step counter:  129351
dev_network_count:  432
learn step counter:  129401
dev_network_count:  432
learn step counter:  129451
dev_network_count:  432
learn step counter:  129501
dev_network_count:  432
learn step counter:  129551
dev_network_count:  432

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
433  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  129601
dev_network_count:  433
learn step counter:  129651
dev_network_count:  433
learn step counter:  129701
dev_network_count:  433
learn step counter:  129751
dev_network_count:  433
learn step counter:  129801
dev_network_count:  433
learn step counter:  129851
dev_network_count:  433

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
434  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  129901
dev_network_count:  434
learn step counter:  129951
dev_network_count:  434
EPOCH %d 133
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
4076 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4077 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4078 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4079 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4080 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4081 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4082 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  130001
dev_network_count:  434
learn step counter:  130051
dev_network_count:  434
learn step counter:  130101
dev_network_count:  434
learn step counter:  130151
dev_network_count:  434

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
435  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  130201
dev_network_count:  435
learn step counter:  130251
dev_network_count:  435
learn step counter:  130301
dev_network_count:  435
learn step counter:  130351
dev_network_count:  435
learn step counter:  130401
dev_network_count:  435
learn step counter:  130451
dev_network_count:  435

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
436  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  130501
dev_network_count:  436
learn step counter:  130551
dev_network_count:  436
learn step counter:  130601
dev_network_count:  436
learn step counter:  130651
dev_network_count:  436
learn step counter:  130701
dev_network_count:  436
learn step counter:  130751
dev_network_count:  436

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
437  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  130801
dev_network_count:  437
learn step counter:  130851
dev_network_count:  437
learn step counter:  130901
dev_network_count:  437
learn step counter:  130951
dev_network_count:  437
EPOCH %d 134
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1569 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1570 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1571 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1572 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1573 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1574 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1575 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  131001
dev_network_count:  437
learn step counter:  131051
dev_network_count:  437

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
438  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  131101
dev_network_count:  438
learn step counter:  131151
dev_network_count:  438
learn step counter:  131201
dev_network_count:  438
learn step counter:  131251
dev_network_count:  438
learn step counter:  131301
dev_network_count:  438
learn step counter:  131351
dev_network_count:  438

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
439  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  131401
dev_network_count:  439
learn step counter:  131451
dev_network_count:  439
learn step counter:  131501
dev_network_count:  439
learn step counter:  131551
dev_network_count:  439
learn step counter:  131601
dev_network_count:  439
learn step counter:  131651
dev_network_count:  439

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
440  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  131701
dev_network_count:  440
learn step counter:  131751
dev_network_count:  440
learn step counter:  131801
dev_network_count:  440
learn step counter:  131851
dev_network_count:  440
learn step counter:  131901
dev_network_count:  440
learn step counter:  131951
dev_network_count:  440
EPOCH %d 135
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
4062 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4063 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4064 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4065 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4066 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4067 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4068 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
441  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  132001
dev_network_count:  441
learn step counter:  132051
dev_network_count:  441
learn step counter:  132101
dev_network_count:  441
learn step counter:  132151
dev_network_count:  441
learn step counter:  132201
dev_network_count:  441
learn step counter:  132251
dev_network_count:  441

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
442  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  132301
dev_network_count:  442
learn step counter:  132351
dev_network_count:  442
learn step counter:  132401
dev_network_count:  442
learn step counter:  132451
dev_network_count:  442
learn step counter:  132501
dev_network_count:  442
learn step counter:  132551
dev_network_count:  442

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
443  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  132601
dev_network_count:  443
learn step counter:  132651
dev_network_count:  443
learn step counter:  132701
dev_network_count:  443
learn step counter:  132751
dev_network_count:  443
learn step counter:  132801
dev_network_count:  443
learn step counter:  132851
dev_network_count:  443

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
444  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  132901
dev_network_count:  444
learn step counter:  132951
dev_network_count:  444
EPOCH %d 136
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1555 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1556 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1557 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1558 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1559 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1560 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1561 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  133001
dev_network_count:  444
learn step counter:  133051
dev_network_count:  444
learn step counter:  133101
dev_network_count:  444
learn step counter:  133151
dev_network_count:  444

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
445  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  133201
dev_network_count:  445
learn step counter:  133251
dev_network_count:  445
learn step counter:  133301
dev_network_count:  445
learn step counter:  133351
dev_network_count:  445
learn step counter:  133401
dev_network_count:  445
learn step counter:  133451
dev_network_count:  445

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
446  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  133501
dev_network_count:  446
learn step counter:  133551
dev_network_count:  446
learn step counter:  133601
dev_network_count:  446
learn step counter:  133651
dev_network_count:  446
learn step counter:  133701
dev_network_count:  446
learn step counter:  133751
dev_network_count:  446

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
447  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  133801
dev_network_count:  447
learn step counter:  133851
dev_network_count:  447
learn step counter:  133901
dev_network_count:  447
learn step counter:  133951
dev_network_count:  447
EPOCH %d 137
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
4048 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4049 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4050 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4051 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4052 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4053 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4054 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  134001
dev_network_count:  447
learn step counter:  134051
dev_network_count:  447

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
448  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  134101
dev_network_count:  448
learn step counter:  134151
dev_network_count:  448
learn step counter:  134201
dev_network_count:  448
learn step counter:  134251
dev_network_count:  448
learn step counter:  134301
dev_network_count:  448
learn step counter:  134351
dev_network_count:  448

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
449  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  134401
dev_network_count:  449
learn step counter:  134451
dev_network_count:  449
learn step counter:  134501
dev_network_count:  449
learn step counter:  134551
dev_network_count:  449
learn step counter:  134601
dev_network_count:  449
learn step counter:  134651
dev_network_count:  449

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
450  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  134701
dev_network_count:  450
learn step counter:  134751
dev_network_count:  450
learn step counter:  134801
dev_network_count:  450
learn step counter:  134851
dev_network_count:  450
learn step counter:  134901
dev_network_count:  450
learn step counter:  134951
dev_network_count:  450
EPOCH %d 138
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1541 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1542 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1543 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1544 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1545 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1546 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1547 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
451  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  135001
dev_network_count:  451
learn step counter:  135051
dev_network_count:  451
learn step counter:  135101
dev_network_count:  451
learn step counter:  135151
dev_network_count:  451
learn step counter:  135201
dev_network_count:  451
learn step counter:  135251
dev_network_count:  451

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
452  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  135301
dev_network_count:  452
learn step counter:  135351
dev_network_count:  452
learn step counter:  135401
dev_network_count:  452
learn step counter:  135451
dev_network_count:  452
learn step counter:  135501
dev_network_count:  452
learn step counter:  135551
dev_network_count:  452

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
453  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  135601
dev_network_count:  453
learn step counter:  135651
dev_network_count:  453
learn step counter:  135701
dev_network_count:  453
learn step counter:  135751
dev_network_count:  453
learn step counter:  135801
dev_network_count:  453
learn step counter:  135851
dev_network_count:  453

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
454  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  135901
dev_network_count:  454
learn step counter:  135951
dev_network_count:  454
EPOCH %d 139
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
4034 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4035 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4036 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4037 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4038 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4039 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4040 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  136001
dev_network_count:  454
learn step counter:  136051
dev_network_count:  454
learn step counter:  136101
dev_network_count:  454
learn step counter:  136151
dev_network_count:  454

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
455  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  136201
dev_network_count:  455
learn step counter:  136251
dev_network_count:  455
learn step counter:  136301
dev_network_count:  455
learn step counter:  136351
dev_network_count:  455
learn step counter:  136401
dev_network_count:  455
learn step counter:  136451
dev_network_count:  455

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
456  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  136501
dev_network_count:  456
learn step counter:  136551
dev_network_count:  456
learn step counter:  136601
dev_network_count:  456
learn step counter:  136651
dev_network_count:  456
learn step counter:  136701
dev_network_count:  456
learn step counter:  136751
dev_network_count:  456

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
457  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  136801
dev_network_count:  457
learn step counter:  136851
dev_network_count:  457
learn step counter:  136901
dev_network_count:  457
learn step counter:  136951
dev_network_count:  457
EPOCH %d 140
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1527 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1528 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1529 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1530 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1531 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1532 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1533 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  137001
dev_network_count:  457
learn step counter:  137051
dev_network_count:  457

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
458  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  137101
dev_network_count:  458
learn step counter:  137151
dev_network_count:  458
learn step counter:  137201
dev_network_count:  458
learn step counter:  137251
dev_network_count:  458
learn step counter:  137301
dev_network_count:  458
learn step counter:  137351
dev_network_count:  458

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
459  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  137401
dev_network_count:  459
learn step counter:  137451
dev_network_count:  459
learn step counter:  137501
dev_network_count:  459
learn step counter:  137551
dev_network_count:  459
learn step counter:  137601
dev_network_count:  459
learn step counter:  137651
dev_network_count:  459

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
460  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  137701
dev_network_count:  460
learn step counter:  137751
dev_network_count:  460
learn step counter:  137801
dev_network_count:  460
learn step counter:  137851
dev_network_count:  460
learn step counter:  137901
dev_network_count:  460
learn step counter:  137951
dev_network_count:  460
EPOCH %d 141
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
4020 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4021 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4022 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4023 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4024 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4025 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4026 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
461  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  138001
dev_network_count:  461
learn step counter:  138051
dev_network_count:  461
learn step counter:  138101
dev_network_count:  461
learn step counter:  138151
dev_network_count:  461
learn step counter:  138201
dev_network_count:  461
learn step counter:  138251
dev_network_count:  461

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
462  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  138301
dev_network_count:  462
learn step counter:  138351
dev_network_count:  462
learn step counter:  138401
dev_network_count:  462
learn step counter:  138451
dev_network_count:  462
learn step counter:  138501
dev_network_count:  462
learn step counter:  138551
dev_network_count:  462

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
463  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  138601
dev_network_count:  463
learn step counter:  138651
dev_network_count:  463
learn step counter:  138701
dev_network_count:  463
learn step counter:  138751
dev_network_count:  463
learn step counter:  138801
dev_network_count:  463
learn step counter:  138851
dev_network_count:  463

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
464  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  138901
dev_network_count:  464
learn step counter:  138951
dev_network_count:  464
EPOCH %d 142
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1513 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1514 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1515 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1516 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1517 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1518 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1519 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  139001
dev_network_count:  464
learn step counter:  139051
dev_network_count:  464
learn step counter:  139101
dev_network_count:  464
learn step counter:  139151
dev_network_count:  464

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
465  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  139201
dev_network_count:  465
learn step counter:  139251
dev_network_count:  465
learn step counter:  139301
dev_network_count:  465
learn step counter:  139351
dev_network_count:  465
learn step counter:  139401
dev_network_count:  465
learn step counter:  139451
dev_network_count:  465

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
466  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  139501
dev_network_count:  466
learn step counter:  139551
dev_network_count:  466
learn step counter:  139601
dev_network_count:  466
learn step counter:  139651
dev_network_count:  466
learn step counter:  139701
dev_network_count:  466
learn step counter:  139751
dev_network_count:  466

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
467  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  139801
dev_network_count:  467
learn step counter:  139851
dev_network_count:  467
learn step counter:  139901
dev_network_count:  467
learn step counter:  139951
dev_network_count:  467
EPOCH %d 143
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
4006 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
4007 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
4008 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
4009 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
4010 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
4011 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
4012 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  140001
dev_network_count:  467
learn step counter:  140051
dev_network_count:  467

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
468  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  140101
dev_network_count:  468
learn step counter:  140151
dev_network_count:  468
learn step counter:  140201
dev_network_count:  468
learn step counter:  140251
dev_network_count:  468
learn step counter:  140301
dev_network_count:  468
learn step counter:  140351
dev_network_count:  468

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
469  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  140401
dev_network_count:  469
learn step counter:  140451
dev_network_count:  469
learn step counter:  140501
dev_network_count:  469
learn step counter:  140551
dev_network_count:  469
learn step counter:  140601
dev_network_count:  469
learn step counter:  140651
dev_network_count:  469

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
470  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  140701
dev_network_count:  470
learn step counter:  140751
dev_network_count:  470
learn step counter:  140801
dev_network_count:  470
learn step counter:  140851
dev_network_count:  470
learn step counter:  140901
dev_network_count:  470
learn step counter:  140951
dev_network_count:  470
EPOCH %d 144
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1499 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1500 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1501 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1502 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1503 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1504 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1505 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
471  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  141001
dev_network_count:  471
learn step counter:  141051
dev_network_count:  471
learn step counter:  141101
dev_network_count:  471
learn step counter:  141151
dev_network_count:  471
learn step counter:  141201
dev_network_count:  471
learn step counter:  141251
dev_network_count:  471

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
472  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  141301
dev_network_count:  472
learn step counter:  141351
dev_network_count:  472
learn step counter:  141401
dev_network_count:  472
learn step counter:  141451
dev_network_count:  472
learn step counter:  141501
dev_network_count:  472
learn step counter:  141551
dev_network_count:  472

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
473  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  141601
dev_network_count:  473
learn step counter:  141651
dev_network_count:  473
learn step counter:  141701
dev_network_count:  473
learn step counter:  141751
dev_network_count:  473
learn step counter:  141801
dev_network_count:  473
learn step counter:  141851
dev_network_count:  473

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
474  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  141901
dev_network_count:  474
learn step counter:  141951
dev_network_count:  474
EPOCH %d 145
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
3992 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
3993 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
3994 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
3995 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
3996 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
3997 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
3998 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  142001
dev_network_count:  474
learn step counter:  142051
dev_network_count:  474
learn step counter:  142101
dev_network_count:  474
learn step counter:  142151
dev_network_count:  474

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
475  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  142201
dev_network_count:  475
learn step counter:  142251
dev_network_count:  475
learn step counter:  142301
dev_network_count:  475
learn step counter:  142351
dev_network_count:  475
learn step counter:  142401
dev_network_count:  475
learn step counter:  142451
dev_network_count:  475

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
476  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  142501
dev_network_count:  476
learn step counter:  142551
dev_network_count:  476
learn step counter:  142601
dev_network_count:  476
learn step counter:  142651
dev_network_count:  476
learn step counter:  142701
dev_network_count:  476
learn step counter:  142751
dev_network_count:  476

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
477  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  142801
dev_network_count:  477
learn step counter:  142851
dev_network_count:  477
learn step counter:  142901
dev_network_count:  477
learn step counter:  142951
dev_network_count:  477
EPOCH %d 146
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1485 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1486 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1487 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1488 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1489 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1490 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1491 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  143001
dev_network_count:  477
learn step counter:  143051
dev_network_count:  477

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
478  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  143101
dev_network_count:  478
learn step counter:  143151
dev_network_count:  478
learn step counter:  143201
dev_network_count:  478
learn step counter:  143251
dev_network_count:  478
learn step counter:  143301
dev_network_count:  478
learn step counter:  143351
dev_network_count:  478

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
479  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  143401
dev_network_count:  479
learn step counter:  143451
dev_network_count:  479
learn step counter:  143501
dev_network_count:  479
learn step counter:  143551
dev_network_count:  479
learn step counter:  143601
dev_network_count:  479
learn step counter:  143651
dev_network_count:  479

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
480  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  143701
dev_network_count:  480
learn step counter:  143751
dev_network_count:  480
learn step counter:  143801
dev_network_count:  480
learn step counter:  143851
dev_network_count:  480
learn step counter:  143901
dev_network_count:  480
learn step counter:  143951
dev_network_count:  480
EPOCH %d 147
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
3978 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
3979 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
3980 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
3981 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
3982 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
3983 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
3984 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
481  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  144001
dev_network_count:  481
learn step counter:  144051
dev_network_count:  481
learn step counter:  144101
dev_network_count:  481
learn step counter:  144151
dev_network_count:  481
learn step counter:  144201
dev_network_count:  481
learn step counter:  144251
dev_network_count:  481

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
482  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  144301
dev_network_count:  482
learn step counter:  144351
dev_network_count:  482
learn step counter:  144401
dev_network_count:  482
learn step counter:  144451
dev_network_count:  482
learn step counter:  144501
dev_network_count:  482
learn step counter:  144551
dev_network_count:  482

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
483  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  144601
dev_network_count:  483
learn step counter:  144651
dev_network_count:  483
learn step counter:  144701
dev_network_count:  483
learn step counter:  144751
dev_network_count:  483
learn step counter:  144801
dev_network_count:  483
learn step counter:  144851
dev_network_count:  483

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
484  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  144901
dev_network_count:  484
learn step counter:  144951
dev_network_count:  484
EPOCH %d 148
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1471 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1472 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1473 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1474 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1475 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1476 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1477 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  145001
dev_network_count:  484
learn step counter:  145051
dev_network_count:  484
learn step counter:  145101
dev_network_count:  484
learn step counter:  145151
dev_network_count:  484

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
485  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  145201
dev_network_count:  485
learn step counter:  145251
dev_network_count:  485
learn step counter:  145301
dev_network_count:  485
learn step counter:  145351
dev_network_count:  485
learn step counter:  145401
dev_network_count:  485
learn step counter:  145451
dev_network_count:  485

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
486  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  145501
dev_network_count:  486
learn step counter:  145551
dev_network_count:  486
learn step counter:  145601
dev_network_count:  486
learn step counter:  145651
dev_network_count:  486
learn step counter:  145701
dev_network_count:  486
learn step counter:  145751
dev_network_count:  486

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
487  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  145801
dev_network_count:  487
learn step counter:  145851
dev_network_count:  487
learn step counter:  145901
dev_network_count:  487
learn step counter:  145951
dev_network_count:  487
EPOCH %d 149
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
3964 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
3965 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
3966 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
3967 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
3968 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
3969 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
3970 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  146001
dev_network_count:  487
learn step counter:  146051
dev_network_count:  487

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
488  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  146101
dev_network_count:  488
learn step counter:  146151
dev_network_count:  488
learn step counter:  146201
dev_network_count:  488
learn step counter:  146251
dev_network_count:  488
learn step counter:  146301
dev_network_count:  488
learn step counter:  146351
dev_network_count:  488

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
489  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  146401
dev_network_count:  489
learn step counter:  146451
dev_network_count:  489
learn step counter:  146501
dev_network_count:  489
learn step counter:  146551
dev_network_count:  489
learn step counter:  146601
dev_network_count:  489
learn step counter:  146651
dev_network_count:  489

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
490  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  146701
dev_network_count:  490
learn step counter:  146751
dev_network_count:  490
learn step counter:  146801
dev_network_count:  490
learn step counter:  146851
dev_network_count:  490
learn step counter:  146901
dev_network_count:  490
learn step counter:  146951
dev_network_count:  490
EPOCH %d 150
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1457 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1458 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1459 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1460 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1461 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1462 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1463 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
491  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  147001
dev_network_count:  491
learn step counter:  147051
dev_network_count:  491
learn step counter:  147101
dev_network_count:  491
learn step counter:  147151
dev_network_count:  491
learn step counter:  147201
dev_network_count:  491
learn step counter:  147251
dev_network_count:  491

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
492  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  147301
dev_network_count:  492
learn step counter:  147351
dev_network_count:  492
learn step counter:  147401
dev_network_count:  492
learn step counter:  147451
dev_network_count:  492
learn step counter:  147501
dev_network_count:  492
learn step counter:  147551
dev_network_count:  492

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
493  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  147601
dev_network_count:  493
learn step counter:  147651
dev_network_count:  493
learn step counter:  147701
dev_network_count:  493
learn step counter:  147751
dev_network_count:  493
learn step counter:  147801
dev_network_count:  493
learn step counter:  147851
dev_network_count:  493

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
494  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  147901
dev_network_count:  494
learn step counter:  147951
dev_network_count:  494
EPOCH %d 151
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
3950 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
3951 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
3952 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
3953 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
3954 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
3955 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
3956 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  148001
dev_network_count:  494
learn step counter:  148051
dev_network_count:  494
learn step counter:  148101
dev_network_count:  494
learn step counter:  148151
dev_network_count:  494

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
495  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  148201
dev_network_count:  495
learn step counter:  148251
dev_network_count:  495
learn step counter:  148301
dev_network_count:  495
learn step counter:  148351
dev_network_count:  495
learn step counter:  148401
dev_network_count:  495
learn step counter:  148451
dev_network_count:  495

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
496  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  148501
dev_network_count:  496
learn step counter:  148551
dev_network_count:  496
learn step counter:  148601
dev_network_count:  496
learn step counter:  148651
dev_network_count:  496
learn step counter:  148701
dev_network_count:  496
learn step counter:  148751
dev_network_count:  496

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
497  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  148801
dev_network_count:  497
learn step counter:  148851
dev_network_count:  497
learn step counter:  148901
dev_network_count:  497
learn step counter:  148951
dev_network_count:  497
EPOCH %d 152
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1443 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1444 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1445 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1446 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1447 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1448 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1449 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  149001
dev_network_count:  497
learn step counter:  149051
dev_network_count:  497

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
498  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  149101
dev_network_count:  498
learn step counter:  149151
dev_network_count:  498
learn step counter:  149201
dev_network_count:  498
learn step counter:  149251
dev_network_count:  498
learn step counter:  149301
dev_network_count:  498
learn step counter:  149351
dev_network_count:  498

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
499  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  149401
dev_network_count:  499
learn step counter:  149451
dev_network_count:  499
learn step counter:  149501
dev_network_count:  499
learn step counter:  149551
dev_network_count:  499
learn step counter:  149601
dev_network_count:  499
learn step counter:  149651
dev_network_count:  499

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
500  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  149701
dev_network_count:  500
learn step counter:  149751
dev_network_count:  500
learn step counter:  149801
dev_network_count:  500
learn step counter:  149851
dev_network_count:  500
learn step counter:  149901
dev_network_count:  500
learn step counter:  149951
dev_network_count:  500
EPOCH %d 153
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
3936 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
3937 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
3938 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
3939 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
3940 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
3941 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
3942 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
501  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  150001
dev_network_count:  501
learn step counter:  150051
dev_network_count:  501
learn step counter:  150101
dev_network_count:  501
learn step counter:  150151
dev_network_count:  501
learn step counter:  150201
dev_network_count:  501
learn step counter:  150251
dev_network_count:  501

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
502  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  150301
dev_network_count:  502
learn step counter:  150351
dev_network_count:  502
learn step counter:  150401
dev_network_count:  502
learn step counter:  150451
dev_network_count:  502
learn step counter:  150501
dev_network_count:  502
learn step counter:  150551
dev_network_count:  502

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
503  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  150601
dev_network_count:  503
learn step counter:  150651
dev_network_count:  503
learn step counter:  150701
dev_network_count:  503
learn step counter:  150751
dev_network_count:  503
learn step counter:  150801
dev_network_count:  503
learn step counter:  150851
dev_network_count:  503

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
504  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  150901
dev_network_count:  504
learn step counter:  150951
dev_network_count:  504
EPOCH %d 154
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1429 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1430 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1431 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1432 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1433 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1434 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1435 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  151001
dev_network_count:  504
learn step counter:  151051
dev_network_count:  504
learn step counter:  151101
dev_network_count:  504
learn step counter:  151151
dev_network_count:  504

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
505  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  151201
dev_network_count:  505
learn step counter:  151251
dev_network_count:  505
learn step counter:  151301
dev_network_count:  505
learn step counter:  151351
dev_network_count:  505
learn step counter:  151401
dev_network_count:  505
learn step counter:  151451
dev_network_count:  505

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
506  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  151501
dev_network_count:  506
learn step counter:  151551
dev_network_count:  506
learn step counter:  151601
dev_network_count:  506
learn step counter:  151651
dev_network_count:  506
learn step counter:  151701
dev_network_count:  506
learn step counter:  151751
dev_network_count:  506

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
507  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  151801
dev_network_count:  507
learn step counter:  151851
dev_network_count:  507
learn step counter:  151901
dev_network_count:  507
learn step counter:  151951
dev_network_count:  507
EPOCH %d 155
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
3922 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
3923 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
3924 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
3925 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
3926 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
3927 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
3928 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]
learn step counter:  152001
dev_network_count:  507
learn step counter:  152051
dev_network_count:  507

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
508  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  152101
dev_network_count:  508
learn step counter:  152151
dev_network_count:  508
learn step counter:  152201
dev_network_count:  508
learn step counter:  152251
dev_network_count:  508
learn step counter:  152301
dev_network_count:  508
learn step counter:  152351
dev_network_count:  508

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
509  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  152401
dev_network_count:  509
learn step counter:  152451
dev_network_count:  509
learn step counter:  152501
dev_network_count:  509
learn step counter:  152551
dev_network_count:  509
learn step counter:  152601
dev_network_count:  509
learn step counter:  152651
dev_network_count:  509

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
510  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  152701
dev_network_count:  510
learn step counter:  152751
dev_network_count:  510
learn step counter:  152801
dev_network_count:  510
learn step counter:  152851
dev_network_count:  510
learn step counter:  152901
dev_network_count:  510
learn step counter:  152951
dev_network_count:  510
EPOCH %d 156
On the pretrain of the Q target network. The beam_dqn =1.
 beam_dqn, egreed, gamma:  1 0.001 0.3
1415 7 1.0 3 1  ... s[:3]:  [-0.71668434  0.06666937  0.34099293]  ... s_[:5]:  [-0.9274273   0.6027868  -0.64979553]
1416 3 0.8 0 0  ... s[:3]:  [-0.9274273   0.6027868  -0.64979553]  ... s_[:5]:  [0. 0. 0.]
1417 8 1.0 3 1  ... s[:3]:  [-0.75541776  0.83350456 -0.4593433 ]  ... s_[:5]:  [-0.9849706   0.89682406 -0.9233895 ]
1418 3 0.8 0 0  ... s[:3]:  [-0.9849706   0.89682406 -0.9233895 ]  ... s_[:5]:  [0. 0. 0.]
1419 4 1.0 5 1  ... s[:3]:  [0.29544878 0.91327965 0.91813076]  ... s_[:5]:  [-0.5008849   0.9886308   0.37956738]
1420 5 0.8 3 1  ... s[:3]:  [-0.5008849   0.9886308   0.37956738]  ... s_[:5]:  [-0.97250515  0.9893948  -0.73787904]
1421 3 0.6 0 0  ... s[:3]:  [-0.97250515  0.9893948  -0.73787904]  ... s_[:5]:  [0. 0. 0.]

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
511  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  153001
dev_network_count:  511
learn step counter:  153051
dev_network_count:  511
learn step counter:  153101
dev_network_count:  511
learn step counter:  153151
dev_network_count:  511
learn step counter:  153201
dev_network_count:  511
learn step counter:  153251
dev_network_count:  511

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
512  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  153301
dev_network_count:  512
learn step counter:  153351
dev_network_count:  512
learn step counter:  153401
dev_network_count:  512
learn step counter:  153451
dev_network_count:  512
learn step counter:  153501
dev_network_count:  512
learn step counter:  153551
dev_network_count:  512

 Lets copy the Q-value Net in to Q-target net!. And test the performace on the dev data: 
So far:  [array([2])]  the state[:3] is:  tensor([-0.7167,  0.0667,  0.3410])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.9057,  0.6030, -0.6316])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

So far:  [array([2])]  the state[:3] is:  tensor([-0.7554,  0.8335, -0.4593])
So far:  [array([2]), array([8])]  the state[:3] is:  tensor([-0.9850,  0.8968, -0.9234])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 

So far:  [array([2])]  the state[:3] is:  tensor([0.2954, 0.9133, 0.9181])
So far:  [array([2]), array([6])]  the state[:3] is:  tensor([-0.1827,  0.9795,  0.6311])
So far:  [array([2]), array([6]), array([8])]  the state[:3] is:  tensor([-0.9242,  0.9921, -0.5906])

 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 8 3]
Eval  :  [[2 8 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 5 3]
Eval  :  [[2 8 3]]
Reward:  [-1.   2.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 6 3]
Eval  :  [[2 6 3]]
Reward:  [1.  0.8] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 4 6 3]
Eval  :  [[2 6 6 3]]
Reward:  [-1.   2.8  0.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 5 3]
Eval  :  [[2 6 8 3]]
Reward:  [-1.  -0.2  3.6] 


 Sample-------------Target vs Eval_net prediction:--Raw---and---Decoded-----
Target:  [2 7 3]
Eval  :  [[2 6 3]]
Reward:  [-1.   2.8] 

valid_references:  ['3', '2', '1 0', '3 4', '2', '0', '4', '1 4', '3 0', '3']
valid_hypotheses:  ['4', '2', '4 2', '4 4', '2', '2', '4', '4 4', '4 2', '4']
513  r_total and score:  302.00000000000034 26.732755161246782
Current Bleu score is:  26.732755161246782
learn step counter:  153601
dev_network_count:  513
learn step counter:  153651
dev_network_count:  513
learn step counter:  153701
dev_network_count:  513
